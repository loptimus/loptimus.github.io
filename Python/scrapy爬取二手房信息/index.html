<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0"><link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.1.0",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!1,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Scrapy爬取二手房信息 Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。这里使用Scrapy对链家网网页中的广州二手房信息进行爬取。"><meta name="keywords" content="爬虫"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy爬取二手房信息"><meta property="og:url" content="http://yoursite.com/Python/scrapy爬取二手房信息/index.html"><meta property="og:site_name" content="QuickNotes"><meta property="og:description" content="Scrapy爬取二手房信息 Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。这里使用Scrapy对链家网网页中的广州二手房信息进行爬取。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/ershoufang.jpg"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/scrapy_help.jpg"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/region.jpg"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/region_xpath.jpg"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/region_page.jpg"><meta property="og:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/csv.jpg"><meta property="og:updated_time" content="2019-07-01T10:34:47.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy爬取二手房信息"><meta name="twitter:description" content="Scrapy爬取二手房信息 Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。这里使用Scrapy对链家网网页中的广州二手房信息进行爬取。"><meta name="twitter:image" content="http://yoursite.com/Python/scrapy爬取二手房信息/ershoufang.jpg"><link rel="canonical" href="http://yoursite.com/Python/scrapy爬取二手房信息/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>Scrapy爬取二手房信息 | QuickNotes</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">QuickNotes</span><span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"> <button aria-label="切换导航栏"><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益 404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header> <a href="https://github.com/loptimus" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"/></svg></a><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/Python/scrapy爬取二手房信息/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lwl"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="QuickNotes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy爬取二手房信息</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-02-26 18:34:47" itemprop="dateCreated datePublished" datetime="2019-02-26T18:34:47+08:00">2019-02-26</time> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-07-01 18:34:47" itemprop="dateModified" datetime="2019-07-01T18:34:47+08:00">2019-07-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数：<span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span title="本文字数">8.7k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">8 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><h1 id="scrapy爬取二手房信息">Scrapy爬取二手房信息</h1><p>Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。这里使用Scrapy对链家网网页中的广州二手房信息进行爬取。</p><a id="more"></a><h2 id="网页结构分析">网页结构分析</h2><p>以链家网网页信息作为信息来源，打开二手房信息页面。</p> <img src="/Python/scrapy爬取二手房信息/ershoufang.jpg" title="二手房"><p>可以看到网页中，位置筛选条件分为“区域”和“地铁”两个选项。这里将采用对区域分别进行爬取，步骤如下：</p><ol type="1"><li>按照区域划分，先选择一个区域，如“天河”，筛选出该区域的所有二手房信息；</li><li>按当前所选区域的所有二手房信息进行逐页爬取，提取该区域的所有二手房信息；</li><li>完成当前区域的所有二手房信息提取后，返回到第1步，继续选择下一个区域，直至所有区域二手房信息都被提取完成；</li><li>合并所有区域的二手房信息，然后输出称CSV文件。</li></ol><h2 id="scrapy实现爬取">Scrapy实现爬取</h2><h3 id="安装">安装</h3><ul><li><p>运行环境</p><ul><li>操作系统：macOS Mojave</li><li>Python：3.7</li><li>pip：19.1.1</li></ul><blockquote><p>pip是一个安装python包的工具。安装方法：<a href="https://pypi.org/project/pip/" target="_blank" rel="noopener">pip</a></p></blockquote></li><li><p>安装</p></li></ul><p>使用pip安装Scrapy，在终端输入以下命令：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install Scrapy</span><br></pre></td></tr></table></figure><p>待上面命令执行完后，输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>提示类似以下信息，表示已经安装成功。</p> <img src="/Python/scrapy爬取二手房信息/scrapy_help.jpg" title="Scrapy"><h3 id="创建爬虫">创建爬虫</h3><p>在开始爬取之前，必须先创建一个爬虫项目：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject mySpider</span><br></pre></td></tr></table></figure><p>其中，mySpider为项目名称。该命令执行完成后，会创建一个mySpider文件夹，目录结构大致如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">├── mySpider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       └──  house.py</span><br><span class="line">└── scrapy.cfg</span><br><span class="line">2 directories, 8 files</span><br></pre></td></tr></table></figure><p>这些文件分别是：</p><ul><li>scrapy.cfg： 项目的配置文件。</li><li>mySpider/items.py：项目中的item文件，用于定义提取内容的数据结构。</li><li>mySpider/spiders/：放置spider代码的目录，同一个项目可以有多个spider，都放在这个目录下。</li><li>mySpider/pipelines.py：项目中的pipelines文件，用于对提取内容进行结构化处理。</li><li>mySpider/settings.py：项目的设置文件，例如：可用于限制访问频率等。</li></ul><h3 id="数据结构定义">数据结构定义</h3><p>根据前面的网页结构分析，我们要提取网页中二手房的相关信息：房子标题、房子名称、户型、面积、楼层、行政区、行政街道、关注、发布日期、总价、单价。</p><p>在mySpider/items.py文件中创建一个HouseItem类，该类继承于scrapy.Item类，在HouseItem类中添加以上信息字段，并定义类型为scrapy.Field的类属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HouseItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># 标题</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    <span class="comment"># 名称</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    <span class="comment"># 户型</span></span><br><span class="line">    room = scrapy.Field()</span><br><span class="line">    <span class="comment"># 面积</span></span><br><span class="line">    area = scrapy.Field()</span><br><span class="line">    <span class="comment"># 楼层</span></span><br><span class="line">    floor = scrapy.Field()</span><br><span class="line">    <span class="comment"># 行政区</span></span><br><span class="line">    region = scrapy.Field()</span><br><span class="line">    <span class="comment"># 行政街道</span></span><br><span class="line">    street = scrapy.Field()</span><br><span class="line">    <span class="comment"># 关注</span></span><br><span class="line">    follow = scrapy.Field()</span><br><span class="line">    <span class="comment"># 发布日期</span></span><br><span class="line">    publish = scrapy.Field()</span><br><span class="line">    <span class="comment"># 总价，单位：万</span></span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    <span class="comment"># 单价，单位：平方米</span></span><br><span class="line">    unit_price = scrapy.Field()</span><br></pre></td></tr></table></figure><h3 id="编写爬虫">编写爬虫</h3><p>爬虫爬取网页的基本流程可以分为三个步骤：</p><ol type="1"><li>模拟浏览器向指定的网页发起请求；</li><li>接收网站返回的响应数据；</li><li>根据网站返回的数据，提取所需信息。若要进行下一次爬取，则重复以上步骤。</li></ol><p>在mySpider目录下，使用scrapy的genspider命令，可以创建包含以上流程的基本框架代码。输入命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider house <span class="string">"lianjia.com"</span></span><br></pre></td></tr></table></figure><p>将创建名为house的爬虫，且基本框架代码保存于spiders目录下的house.py文件中。</p><p>查看house.py文件内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HouseSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'house'</span></span><br><span class="line">    allowed_domains = [<span class="string">'lianjia.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://lianjia.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>可以看到，scrapy的genspider命令为爬虫创建一个名为HouseSpider的类，其继承了scrapy.Spider类，并创建了三个默认属性和一个默认方法。</p><ul><li>name：爬虫名称，用于识别不同的爬虫。</li><li>allowed_domains：表示搜索的域名范围，规定了爬虫只爬取这个域名下的网页。</li><li>start_urls：表示爬虫初始请求的URL列表。</li><li>parse(self, response)：用于负责接收和解析返回的网页数据(response.body)，提取所需的网页数据。若有需要，生成下一页的URL请求。每个初始URL的网站返回响应数据后都将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数。</li></ul><p>接下来，将要对house.py文件中的基本框架代码进行修改，以满足我们的爬取数据的需求。</p><ol type="1"><li><p>发送获取二手房信息的初始请求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HouseSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'house'</span></span><br><span class="line">    allowed_domains = [<span class="string">'lianjia.com'</span>]</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>:</span><br><span class="line">        <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 起始请求地址</span></span><br><span class="line">    start_url = <span class="string">'http://gz.lianjia.com/ershoufang'</span></span><br><span class="line">    <span class="comment"># 限制所有行政区域的爬取页数</span></span><br><span class="line">    limit_page = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.start_url, callback=self.parse_region, headers=self.headers)</span><br></pre></td></tr></table></figure><p>上述代码中，将start_urls属性改为start_url，其值为起始请求地址；增加limit_page属性，用于限制所有行政区域的爬取页数；增加headers属性，用于模拟浏览器请求。</p><p>新增方法start_request(self)用于向scrapy.Request函数传递指定参数，重定义起始请求的行为。callback参数意思是使用新增的方法self.parse_region解析此次请求的返回的网站数据，这里是用于解析行政区域信息。</p></li><li><p>提取二手房所有区域信息，并对每个区域逐个发送请求，获取对应区域的二手房信息。</p><p>通过在浏览器中选择页面中要提取的内容，右键点击检查元素，定位到相应的页面源码中元素的的位置</p> <img src="/Python/scrapy爬取二手房信息/region.jpg" title="检查行政区域元素"><p>拷贝出该元素的xpath</p><blockquote><p><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">xpath语法</a></p></blockquote> <img src="/Python/scrapy爬取二手房信息/region_xpath.jpg" title="拷贝行政区域xpath"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_region</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># 提取所有行政区域</span></span><br><span class="line">    region_list = response.xpath(<span class="string">'/html/body/div[3]/div/div[1]/dl[2]/dd/div[1]/div[1]/a'</span>)</span><br><span class="line">    <span class="keyword">for</span> region <span class="keyword">in</span> region_list:</span><br><span class="line">        region_data = region.xpath(<span class="string">'text()'</span>).extract()  <span class="comment"># 区域名称</span></span><br><span class="line">        region_href = region.xpath(<span class="string">'@href'</span>).extract()   <span class="comment"># 区域链接</span></span><br><span class="line">        region_uri = region_href[<span class="number">0</span>].split(<span class="string">'/'</span>)[<span class="number">2</span>]</span><br><span class="line">        region_url = <span class="string">'/'</span>.join([self.start_url, <span class="string">'&#123;&#125;/'</span>]).format(region_uri)</span><br><span class="line">        meta_data = &#123;<span class="string">"region"</span>:region_data, <span class="string">"region_uri"</span>:region_uri&#125;</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=region_url, headers=self.headers, callback=self.parse, meta=meta_data)</span><br></pre></td></tr></table></figure><p>提取所有区域名称和链接，生成每个行政区域的链接地址，然后模拟浏览器发送请求。需要注意的是，对于每个行政区域请求的返回数据，callback的参数使用的是方法parse进行处理。参数meta用于向callback参数所使用的方法中的response参数传递数据。</p></li><li><p>提取页面中每个行政区域分页的二手房信息。</p><p>对于每个行政区域，其房产信息可能会分页展示，因此需要逐页对每个分页的二手房信息进行提取</p> <img src="/Python/scrapy爬取二手房信息/region_page.jpg" title="分页"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    info_list = response.xpath(<span class="string">'//*[@id="content"]/div[1]/ul/li/div[@class="info clear"]'</span>)</span><br><span class="line">    <span class="keyword">for</span> info <span class="keyword">in</span> info_list:</span><br><span class="line">        item = self.make_item(info, response)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">    <span class="comment"># 提取分页信息</span></span><br><span class="line">    page_data = response.xpath(<span class="string">'//*[@id="content"]/div[1]/div[8]/div[2]/div/@page-data'</span>).extract()</span><br><span class="line">    cur_page = <span class="number">1</span></span><br><span class="line">    total_page = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> len(page_data):</span><br><span class="line">        page_data = json.loads(page_data[<span class="number">0</span>])</span><br><span class="line">        cur_page = page_data[<span class="string">'curPage'</span>]</span><br><span class="line">        total_page = page_data[<span class="string">'totalPage'</span>]</span><br><span class="line">    next_page = cur_page + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 判断是否需要爬取下一分页的条件，若满足，则构建新的url，并发送请求</span></span><br><span class="line">    <span class="keyword">if</span> (next_page &lt;= total_page <span class="keyword">and</span> next_page &lt;= self.limit_page):</span><br><span class="line">        uri_list = [self.start_url, response.meta[<span class="string">'region_uri'</span>], <span class="string">"pg&#123;&#125;/"</span>]</span><br><span class="line">        next_url = <span class="string">'/'</span>.join(uri_list).format(next_page)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse, headers=self.headers, meta=response.meta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取二手房信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_item</span><span class="params">(self, info, response)</span>:</span></span><br><span class="line">    item = HouseItem()</span><br><span class="line">    <span class="comment"># 提取房子相关信息</span></span><br><span class="line">    title = info.xpath(<span class="string">'div[@class="title"]/a/text()'</span>).extract()</span><br><span class="line">    name = info.xpath(<span class="string">'div[@class="address"]/div/a/text()'</span>).extract()</span><br><span class="line">    desc = info.xpath(<span class="string">'div[@class="address"]/div/text()'</span>).extract()</span><br><span class="line">    floor = info.xpath(<span class="string">'div[@class="flood"]/div/text()'</span>).extract()</span><br><span class="line">    street = info.xpath(<span class="string">'div[@class="flood"]/div/a/text()'</span>).extract()</span><br><span class="line">    follow = info.xpath(<span class="string">'div[@class="followInfo"]/text()'</span>).extract()      </span><br><span class="line">    price = info.xpath(<span class="string">'div[@class="priceInfo"]/div[@class="totalPrice"]/span/text()'</span>).extract()</span><br><span class="line">    unit_price = info.xpath(<span class="string">'div[@class="priceInfo"]/div[@class="unitPrice"]/@data-price'</span>).extract()</span><br><span class="line"></span><br><span class="line">    item[<span class="string">'region'</span>] = response.meta[<span class="string">'region'</span>]  <span class="comment"># 行政区域</span></span><br><span class="line">    item[<span class="string">'title'</span>] = title[<span class="number">0</span>]  <span class="comment"># 标题</span></span><br><span class="line">    item[<span class="string">'name'</span>] = name[<span class="number">0</span>]  <span class="comment"># 名称</span></span><br><span class="line">    </span><br><span class="line">    desc = [str.strip(x) <span class="keyword">for</span> x <span class="keyword">in</span> desc[<span class="number">0</span>].split(<span class="string">'|'</span>)]</span><br><span class="line">    item[<span class="string">'room'</span>] = desc[<span class="number">1</span>]  <span class="comment"># 户型</span></span><br><span class="line">    item[<span class="string">'area'</span>] = desc[<span class="number">2</span>]  <span class="comment"># 面积</span></span><br><span class="line">    </span><br><span class="line">    floor = floor[<span class="number">0</span>].split()</span><br><span class="line">    item[<span class="string">'floor'</span>] = floor[<span class="number">0</span>]  <span class="comment"># 楼层</span></span><br><span class="line">    item[<span class="string">'street'</span>] = street[<span class="number">0</span>]  <span class="comment"># 街道</span></span><br><span class="line"></span><br><span class="line">    follow = [str.strip(x) <span class="keyword">for</span> x <span class="keyword">in</span> follow[<span class="number">0</span>].split(<span class="string">'/'</span>)]</span><br><span class="line">    item[<span class="string">'follow'</span>] = follow[<span class="number">0</span>]  <span class="comment"># 关注</span></span><br><span class="line">    item[<span class="string">'publish'</span>] = follow[<span class="number">1</span>]  <span class="comment"># 发布时间</span></span><br><span class="line"></span><br><span class="line">    item[<span class="string">'price'</span>] = float(price[<span class="number">0</span>]) * <span class="number">10000</span>  <span class="comment"># 总价：万</span></span><br><span class="line">    item[<span class="string">'unit_price'</span>] = unit_price[<span class="number">0</span>]  <span class="comment"># 单价：平方米</span></span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>与提取所有区域内容一样，通过浏览器的检查元素，获取xpath，然后分别对所需内容进行提取。</p></li></ol><h3 id="爬虫配置">爬虫配置</h3><p>爬虫的逻辑代码完成后，需要对爬虫进行相应配置。配置文件为mySpider目录下settings.py文件。scrapy运行时会根据该配置文件内容设置爬取规则。打开settings.py文件，设置以下内容：</p><ul><li><p>遵守爬虫规则文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遵守robots.txt规则文件</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">True</span></span><br></pre></td></tr></table></figure></li><li><p>禁用COOKIES</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁用COOKIES，默认是开启的</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br></pre></td></tr></table></figure></li><li><p>限制访问速率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启访问频率限制</span></span><br><span class="line">AUTOTHROTTLE ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 设置访问开始的延迟</span></span><br><span class="line">AUTOTHROTTLE START DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment"># 设置访问之间的最大延迟</span></span><br><span class="line">AUTOTHROTTLE MAX DELAY = <span class="number">60</span></span><br><span class="line"><span class="comment"># 设置Scrapy 并行发给每台远程服务器的请求数量</span></span><br><span class="line">AUTOTHROTTLE TARGET CONCURRENCY= <span class="number">1.0</span></span><br><span class="line"><span class="comment"># 设置下裁之后的自动延迟</span></span><br><span class="line">DOWNLOAD DELAY = <span class="number">3</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="运行爬虫">运行爬虫</h3><p>在mySpider目录输入命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl house</span><br></pre></td></tr></table></figure><p>即可开始执行脚本。现要将爬取的内容保存为CSV文件，则只需在该命令后添加参数“-o”，用于指定输出格式文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl house -o house.csv</span><br></pre></td></tr></table></figure><p>命令执行完后，Scarpy在日志等级为DEBUG的情况下，会打印爬取操作过程中的状态信息及Item数据信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">2019-07-01 20:32:35 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">2019-07-01 20:32:35 [scrapy.extensions.feedexport] INFO: Stored csv feed (723 items) in: house.csv</span><br><span class="line">2019-07-01 20:32:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;&apos;downloader/request_bytes&apos;: 17393,</span><br><span class="line"> &apos;downloader/request_count&apos;: 55,</span><br><span class="line"> &apos;downloader/request_method_count/GET&apos;: 55,</span><br><span class="line"> &apos;downloader/response_bytes&apos;: 904411,</span><br><span class="line"> &apos;downloader/response_count&apos;: 55,</span><br><span class="line"> &apos;downloader/response_status_count/200&apos;: 27,</span><br><span class="line"> &apos;downloader/response_status_count/301&apos;: 28,</span><br><span class="line"> &apos;finish_reason&apos;: &apos;finished&apos;,</span><br><span class="line"> &apos;finish_time&apos;: datetime.datetime(2019, 7, 1, 12, 32, 35, 139438),</span><br><span class="line"> &apos;item_scraped_count&apos;: 723,</span><br><span class="line"> &apos;log_count/DEBUG&apos;: 778,</span><br><span class="line"> &apos;log_count/INFO&apos;: 13,</span><br><span class="line"> &apos;memusage/max&apos;: 76783616,</span><br><span class="line"> &apos;memusage/startup&apos;: 51191808,</span><br><span class="line"> &apos;request_depth_max&apos;: 2,</span><br><span class="line"> &apos;response_received_count&apos;: 27,</span><br><span class="line"> &apos;robotstxt/request_count&apos;: 1,</span><br><span class="line"> &apos;robotstxt/response_count&apos;: 1,</span><br><span class="line"> &apos;robotstxt/response_status_count/200&apos;: 1,</span><br><span class="line"> &apos;scheduler/dequeued&apos;: 53,</span><br><span class="line"> &apos;scheduler/dequeued/memory&apos;: 53,</span><br><span class="line"> &apos;scheduler/enqueued&apos;: 53,</span><br><span class="line"> &apos;scheduler/enqueued/memory&apos;: 53,</span><br><span class="line"> &apos;start_time&apos;: datetime.datetime(2019, 7, 1, 12, 29, 8, 618745)&#125;</span><br><span class="line">2019-07-01 20:32:35 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure><p>爬取结果：</p> <img src="/Python/scrapy爬取二手房信息/csv.jpg" title="二手房信息"></div><footer class="post-footer"><div class="post-tags"><a href="/tags/爬虫/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/SQL/关系代数/" rel="next" title="关系代数"><i class="fa fa-chevron-left"></i> 关系代数</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/概率论与数理统计/统计量及其分布/" rel="prev" title="统计量及其分布">统计量及其分布<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">lwl</p><div class="site-description motion-element" itemprop="description"></div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">23</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy爬取二手房信息"><span class="nav-number">1.</span> <span class="nav-text">Scrapy爬取二手房信息</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#网页结构分析"><span class="nav-number">1.1.</span> <span class="nav-text">网页结构分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy实现爬取"><span class="nav-number">1.2.</span> <span class="nav-text">Scrapy实现爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装"><span class="nav-number">1.2.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建爬虫"><span class="nav-number">1.2.2.</span> <span class="nav-text">创建爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据结构定义"><span class="nav-number">1.2.3.</span> <span class="nav-text">数据结构定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写爬虫"><span class="nav-number">1.2.4.</span> <span class="nav-text">编写爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫配置"><span class="nav-number">1.2.5.</span> <span class="nav-text">爬虫配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#运行爬虫"><span class="nav-number">1.2.6.</span> <span class="nav-text">运行爬虫</span></a></li></ol></li></ol></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span><span class="with-love" id="animate"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">lwl</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span title="站点总字数">164k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">2:29</span></div><div class="powered-by"><i class="fa fa-user-md"></i> <span id="busuanzi_container_site_uv">本站访客数:<span id="busuanzi_value_site_uv">&nbsp;</span></span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/utils.js?v=7.1.0"></script><script src="/js/motion.js?v=7.1.0"></script><script src="/js/affix.js?v=7.1.0"></script><script src="/js/schemes/pisces.js?v=7.1.0"></script><script src="/js/scrollspy.js?v=7.1.0"></script><script src="/js/post-details.js?v=7.1.0"></script><script src="/js/next-boot.js?v=7.1.0"></script><script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html>