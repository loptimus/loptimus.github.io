<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[收入分析—RFM模型]]></title>
    <url>%2F%E6%B8%B8%E6%88%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F%E6%94%B6%E5%85%A5%E5%88%86%E6%9E%90%E2%80%94rfm%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[收入分析—RFM模型 RFM模型是基于客户的消费行为，将用户在三个维度上进行分类，为公司营销决策提供依据。RFM模型通过客户的最近付费时间间隔（R，Recency） 、付费总次数（F，Frequency）和付费总金额（M，Monetary）三项指标衡量客户价值。 R表示距离客户最近一次付费日期有多久，R值越低，客户价值越高； F表示客户的购买频率，即一段时间内客户发生交易的次数，F值越高，客户的价值越高； M表示客户在公司的消费金额，是一段时间内利润的度量，M值越高，客户的价值越高。 R的计算可以用天数表示，即R = 统计截止日期 - 每个用户最后一次消费的日期，其中可以使用统计期间内所有用户中最后一次交易的时间作为统计的截止日期。 通过这三个指标对客户的价值进行划分： 客户类型 客户特征 重要价值客户 消费金额大、消费频率高、最近一次消费时间间隔短，优质客户，需要保持 重要发展客户 消费金额大，消费频率低，最近一次消费时间间隔短，深耕客户，需要重点识别 重要挽留客户 消费金额大，消费频率高，但最近无消费，有价值客户，需要挽留 一般挽留客户 消费金额大，消费频率低，最近无消费，潜在的有价值客户，需要挽留 一般保持客户 消费金额小，消费频率高，最近一次消费时间间隔短，潜力客户，需要挖掘 新客户 消费金额小，消费频率低，最近一次消费时间间隔短，新客户，有推广价值 一般价值客户 消费金额小，消费频率高，最近无消费，一般维持客户 低价值客户 消费金额小，消费频率低，最近无消费，低价值客户 付费用户RFM模型分析 现有某一款游戏最近一周的付费类数据，其中包括本周最后一次付费日期、付费次数、付费金额等。利用该数据进行付费用户RFM模型分析。 载入分析所需的库 12345import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.cm as cmplt.rcParams['font.sans-serif'] = ['SimHei'] # 显示图形中的中文标签 导入数据 12df = pd.read_csv("用户消费数据.csv")df.head() player_id last_date pay_cnt pay_mnt 0 1 20161002 2 48.0 1 2 20161002 2 7.0 2 3 20161002 1 6.0 3 4 20160927 2 31.0 4 5 20160928 1 6.0 查看数据结果明细 123456789df.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 3782 entries, 0 to 3781Data columns (total 4 columns):player_id 3782 non-null int64last_date 3782 non-null int64pay_cnt 3782 non-null int64pay_mnt 3782 non-null float64dtypes: float64(1), int64(3) 数据说明： player_id表示玩家ID； last_date表示最后一次付费日期； pay_cnt表示付费次数； pay_mnt表示付费金额。 计算用户最近一次付费时间间隔 1234# 将last_date转换成日期格式df['last_date'] = pd.to_datetime(df['last_date'].astype(str))# 计算RFM中的R，即最后一次消费日期的时间间隔，用天数表示df['time_interval'] = (max(df['last_date']) - df['last_date']).dt.days 查看付费时间间隔（R）、付费次数（F）及付费金额（M）的分位数情况 1df[["time_interval", "pay_cnt", "pay_mnt"]].quantile(np.arange(0, 1.1, 0.1)) time_interval pay_cnt pay_mnt 0.0 0.0 1.0 0.03 0.1 0.0 1.0 1.00 0.2 0.0 1.0 1.00 0.3 0.0 1.0 6.00 0.4 0.0 1.0 6.00 0.5 0.0 1.0 6.00 0.6 0.0 1.0 7.00 0.7 0.0 2.0 18.00 0.8 1.0 2.0 30.00 0.9 2.0 3.0 57.00 1.0 6.0 42.0 11206.00 从分位数的结果来看，付费时间间隔（R）、付费次数（F）及付费金额（M）都存在明显的左偏现象，所以不宜使用平均数的方法选择基准值。根据分位数的结果，R的基准值选择70%分位数0天，F的基准值选择60%分位数的1次，M的基准值选择50%分位数的6元。选定基准值后，可以使用者这三个基准值对用户进行分类。为了便于处理，可以引入3个衍生变量：tagR、tagF、tagM。当值大于基准时，tagR为0，tagF、tagM为1，当值小于等于基准值时，tagR为1，tagF、tagM为0。 123df['tagR'] = np.where(df['time_interval'] &lt;= 0, 1, 0)df['tagF'] = np.where(df['pay_cnt'] &lt;= 1, 0, 1)df['tagM'] = np.where(df['pay_mnt'] &lt;= 6, 0, 1) 根据RFM模型进行分类，给每位用户打上相应的类型标签，统计不同RFM类型的用户人数和付费金额，并绘制柱形图。 1234567891011121314151617181920212223242526272829# 为所有用户打上标签df['type'] = "一般挽留客户"df.loc[(df['tagR'] == 1) &amp; (df['tagF'] == 1) &amp; (df['tagM'] == 1), "type"] = "重要价值客户"df.loc[(df['tagR'] == 1) &amp; (df['tagF'] == 0) &amp; (df['tagM'] == 1), "type"] = "重要发展客户"df.loc[(df['tagR'] == 0) &amp; (df['tagF'] == 1) &amp; (df['tagM'] == 1), "type"] = "重要挽留客户"df.loc[(df['tagR'] == 1) &amp; (df['tagF'] == 1) &amp; (df['tagM'] == 0), "type"] = "一般保持客户"df.loc[(df['tagR'] == 1) &amp; (df['tagF'] == 0) &amp; (df['tagM'] == 0), "type"] = "新客户"df.loc[(df['tagR'] == 0) &amp; (df['tagF'] == 1) &amp; (df['tagM'] == 0), "type"] = "一般价值客户"df.loc[(df['tagR'] == 0) &amp; (df['tagF'] == 0) &amp; (df['tagM'] == 0), "type"] = "低价值客户"# 按用户类型分组grouped = df.groupby('type')# 对分组后的各组的play_id去重后计数，并重命名usr_cnt = grouped.player_id.nunique().rename("usr_cnt")# 对分组后的各组的pay_mnt求和，并重命名pay_mnt_sum = grouped.pay_mnt.sum().rename("pay_mnt_sum")# 合并统计数据usr_cnt，pay_mnt_sum到同一个DataFrame中df_stats = pd.concat([usr_cnt, pay_mnt_sum], axis=1).reset_index()# 按照付费金额进行降序排列df_stats = df_stats.sort_values(by="pay_mnt_sum", ascending=False)# 计算各分组人数所占总体人数的比例df_stats["usr_cnt_rate"] = round( df_stats["usr_cnt"] / np.sum(df_stats["usr_cnt"]), 2)# 计算各分组消费金额占总体消费金额的比例df_stats["pay_mnt_sum_rate"] = round( df_stats["pay_mnt_sum"] / np.sum(df_stats["pay_mnt_sum"]), 2) 查看上述的统计数据 type usr_cnt pay_mnt_sum usr_cnt_rate pay_mnt_sum_rate 5 重要价值客户 1098 143775.00 0.2903 0.8124 6 重要发展客户 378 16717.00 0.0999 0.0945 7 重要挽留客户 231 7567.00 0.0611 0.0428 4 新客户 1259 4609.00 0.3329 0.0260 2 一般挽留客户 41 2072.00 0.0108 0.0117 3 低价值客户 601 1876.00 0.1589 0.0106 1 一般保持客户 120 243.00 0.0317 0.0014 0 一般价值客户 54 106.03 0.0143 0.0006 绘制付费金额统计、付费人数统计柱形图 1234567891011121314fig = plt.figure(dpi=100, figsize=(10,8))# 选择柱形图颜色type_len = len(df_stats["type"].unique())colors = cm.rainbow(np.linspace(0, 1, type_len))ax_pay = fig.add_subplot(211)ax_pay.bar(df_stats["type"], df_stats["pay_mnt_sum_rate"], color=colors)ax_pay.set_title("RFM付费金额比例")ax_usr = fig.add_subplot(212)ax_usr.bar(df_stats["type"], df_stats["usr_cnt_rate"], color=colors)ax_usr.set_title("RFM付费人数比例")plt.show() 通过上述图表可以看到: 重要价值客户占了全部付费用户人数的29.03%，贡献了81.24%的收入，他们是收入的主要来源，需重点维护。 新用户占了全部付费人数的33.29%，但是付费金额只占到2.6%。对于这类用户，可以给他们推荐新的玩法和角色，更进一步激发他们的付费能力。 重要发展客户占付费人数的9.99%，付费金额占9.45%；重要挽留客户占付费人数的6.11%，付费金额占4.28%。对于这两类客户来说，他们可能有一定的付费能力，需要进一步挖掘，分析用户的消费行为，制定有针对性的消费策略。 对于其他分类用户，他们可能付费能力有限，而更关注一些超值大礼包，因此，做运营活动促销时可以对他们进行推广和刺激，提高整体付费渗透率。 RFM的不足及改进 RFM模型分析虽然简单有效，但也存在一定的不足。 不足 用户评分过于简单：评分标准是根据选择的基准值划分用户群，而该基准值的选择容易受到分析者主观经验的影响。 用户行为变化无法追踪：RFM模型仅仅是利用统计周期内的数据对指标进行分级，无法体现用户的以前行为，不能发现用户不同时期的状态变化。 指标间的相互影响：在统计周期内的消费频率（F）和消费金额（M）两个指标存在共线性问题。 改进 用户分群：利用聚类分析对用户进行划分，可以分为超R、大R、中R、小R等。 更多维度描述用户：可以增加用户的历史付费总次数、历史付费总金额、周平均付费金额等指标，利用相关性分析查看各变量间的关系。 数据降维：利用主成分分析进行降维处理，消除各指标间的线性影响。 运营策略支持：运营可以利用用户分群进行精细化运营，甚至可以结合关联推荐、智能推荐系统发现不同用户群的购物习惯，对道具进行捆绑打折销售，或者利用协同过滤的推荐算法对个体玩家进行个性化推荐。]]></content>
      <categories>
        <category>游戏数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渠道分析—用户忠诚度]]></title>
    <url>%2F%E6%B8%B8%E6%88%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F%E6%B8%A0%E9%81%93%E5%88%86%E6%9E%90%E2%80%94%E7%94%A8%E6%88%B7%E5%BF%A0%E8%AF%9A%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[渠道分析—用户忠诚度 用户忠诚度是用来衡量某一渠道用户黏性的指标，可以使用每日新增用户/每日日活用户进行简单的判断，所以这个比值的区间范围为[0, 1]。比值越接近于1，说明用户忠诚度越低；比值越接近于0，则说明用户忠诚度越高。 数据 根据用户忠诚度的计算方法，假设某游戏的各个渠道同一日的用户数据如下： 行号 渠道名称 活跃用户 新增用户 0 渠道1 386421 53712 1 渠道2 342286 80033 2 渠道3 300085 66267 3 渠道4 213464 49684 4 渠道5 191074 65930 5 渠道6 164305 20282 6 渠道7 160536 24361 7 渠道8 157983 33623 8 渠道9 144279 47191 9 渠道10 8907 1048 渠道用户数据柱形图 读取上面的各个渠道同一日的用户数据，计算各渠道当日的用户忠诚度，绘制当日的新增用户、活跃用户柱形图，以及各渠道的用户忠诚度。 导入数据处理和绘图所需的python库 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] # 显示图形中的中文标签 读取渠道用户数据 1df = pd.read_csv('用户忠诚度.csv') 计算各个渠道当日的用户忠诚度，并查看各渠道的用户忠诚度 12df['用户忠诚度'] = round(df['新增用户'] / df['活跃用户'], 2) # 保留2位小数df 渠道名称 活跃用户 新增用户 用户忠诚度 0 渠道1 386421 53712 0.14 1 渠道2 342286 80033 0.23 2 渠道3 300085 66267 0.22 3 渠道4 213464 49684 0.23 4 渠道5 191074 65930 0.35 5 渠道6 164305 20282 0.12 6 渠道7 160536 24361 0.15 7 渠道8 157983 33623 0.21 8 渠道9 144279 47191 0.33 9 渠道10 8907 1048 0.12 绘制渠道用户数据柱形图 123456789101112131415fig = plt.figure(dpi=100)ax1 = fig.add_subplot(111)ind = np.arange(len(df['渠道名称'])) # 各渠道柱形图位置索引width = 0.35 # 柱形图宽度ax1.bar(ind - width / 2, df['新增用户'], width, label='新增用户') # 新增用户柱形图ax1.bar(ind + width / 2, df['活跃用户'], width, label='活跃用户') # 活跃用户柱形图ax1.set_ylabel('用户数量')ax2 = ax1.twinx() # 增加用于表示用户忠诚度的y轴ax2.plot(df['渠道名称'], df['用户忠诚度'], marker='o', color='red', label='用户忠诚度')ax2.set_ylabel('用户忠诚度')fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, 0)) # 图例fig.tight_layout()plt.title('渠道用户数据柱形图')plt.subplots_adjust(bottom=0.15)plt.show() 可以看到，渠道1、渠道2、渠道3是该款游戏用户来源的主要渠道；渠道1、渠道6、渠道7、渠道10的用户忠诚度较低；渠道5、渠道9用户忠诚度较高。]]></content>
      <categories>
        <category>游戏数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渠道分析—用户质量评级]]></title>
    <url>%2F%E6%B8%B8%E6%88%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F%E6%B8%A0%E9%81%93%E5%88%86%E6%9E%90%E2%80%94%E7%94%A8%E6%88%B7%E8%B4%A8%E9%87%8F%E8%AF%84%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[渠道分析—用户质量评级 在渠道分析中，若单独查看各渠道的关键指标，不能很好的体现渠道的整体质量，也不容易进行不同游戏在某一渠道的横向对比。因此，需要根据业务需求，使用一些关键指标，构建通用的渠道用户评价体系，对渠道质量进行综合打分评级。 渠道用户评价体系 构建渠道用户评价体系的目的是在短期内根据业务模型综合判断渠道用户的质量，该体系需要同时满足一定的效度、信度以及普适性的要求。 效度：能够准确代表渠道品质。 信度：在不同的阶段和时间内保持稳定，可以排除干扰因素的影响。 普适性：对于不同游戏如ARPG，SLG、休闲益智类等采用同一算法都可以适用。 渠道用户质量评分模型 渠道用户质量评分模型应根据业务模型需求设计评分规则，以满足渠道用户评价体系的效度和信度要求。 以自然周为研究周期，选取不同游戏的周收入、周活跃、周ARPPU、周付费渗透率、第7日留存率作为基础指标，分别对这些指标进行评分，然后将各指标评分的得分乘以根据业务需求所设置的权重后再进行求和，得出各个渠道最后的用户质量评分的总分。评分规则如下： 选取某一自然周作为起始周，使全部渠道都处于同一评分周期内，且所有渠道初始评分都是10分。 波动性得分 = 5 * (本周实际值 - 上周实际值)/最近四周的最大值 量级得分 = 5 * 渠道本周值 / 所有渠道本周总值（只有周收入、周活跃有量级指标， 其它三个指标不考虑量级评分）。 各指标本周得分 = 上周得分 + 波动得分 + 量级得分 渠道本周得分 = A指标本周得分 * A指标权重 + B指标本周得分 * B指标权重 … 统计期内的渠道得分 = 第1周渠道得分 + 第2周渠道得分 + … + 统计期内最后一周渠道得分 以波动性得分来衡量指标效度，考虑到不同游戏在不同时期的用户质量会有所不同，所以选用最近四周的最大值作为分母；对于周收入和周活跃指标需要消除量级的影响，因此，以渠道自身值/所有渠道本周总值得到量级得分。 渠道用户质量评分 读取渠道用户数据，并查看数据 12rawdata &lt;- read.csv('渠道用户数据.csv')head(rawdata) 游戏名称 渠道名称 自然周 周收入 周活跃用户数 第7日留存率 周付费率 周ARPPU 1 游戏C 渠道A 第1周 311 989 0.114 0.024 0.315 2 游戏A 渠道A 第1周 814 740 0.149 0.012 1.100 3 游戏B 渠道A 第1周 37040 85714 0.105 0.017 0.432 4 游戏F 渠道A 第1周 21592 66074 0.059 0.015 0.327 5 游戏E 渠道A 第1周 64420 46371 0.094 0.055 1.389 6 游戏D 渠道A 第1周 27211 20316 0.073 0.041 1.339 查看数据类型明细 1str(rawdata) 123456789'data.frame': 830 obs. of 8 variables: $ 游戏名称 : Factor w/ 7 levels "游戏A","游戏B",..: 3 1 2 6 5 4 7 3 1 7 ... $ 渠道名称 : Factor w/ 12 levels "渠道A","渠道B",..: 1 1 1 1 1 1 1 1 1 1 ... $ 自然周 : Factor w/ 10 levels "第10周","第1周",..: 2 2 2 2 2 2 2 3 3 3 ... $ 周收入 : int 311 814 37040 21592 64420 27211 6847 40 482 9014 ... $ 周活跃用户数: int 989 740 85714 66074 46371 20316 23407 644 870 24516 ... $ 第7日留存率 : num 0.114 0.149 0.105 0.059 0.094 0.073 0.056 0.153 0.122 0.063 ... $ 周付费率 : num 0.024 0.012 0.017 0.015 0.055 0.041 0.015 0.026 0.014 0.017 ... $ 周ARPPU : num 0.315 1.1 0.432 0.327 1.389 ... 查看游戏名称 123&gt; unique(rawdata$游戏名称)[1] 游戏C 游戏A 游戏B 游戏F 游戏E 游戏D 游戏GLevels: 游戏A 游戏B 游戏C 游戏D 游戏E 游戏F 游戏G 载入评分规则的计算方法，并将“自然周”列的因子改为有序水平因子 1234567# 载入实现评分方法的函数source("channel_users_quality.R")# 将自然周因子改为有序水平变量levels &lt;- c(paste0("第", 1:length(unique(rawdata$自然周)), "周"))rawdata$自然周 &lt;- factor( rawdata$自然周, levels = levels, ordered = TRUE) 不同游戏的渠道用户质量评分 对游戏A、游戏B、游戏C进行渠道用户质量评分 游戏A的各渠道评分 1AnalysisChannel(rawdata, "游戏A") 渠道G从第2周开始，渠道用户评分一直呈现增长趋势，在第10周得分达到13分，属于表现优异的渠道；渠道J和渠道K用户评分整体下滑明显。 游戏B的各渠道评分 1AnalysisChannel(rawdata, "游戏B") 所有渠道在第2周到第6周的时间段内，各个渠道用户评分快速增加，都在第6周达到最高评分，第6周过后，渠道用户评分开始迅速下降。查看各渠道的指标评分变化情况，试图分析引起下降的指标情况。 12# 查看各渠道的指标评分变化情况TargetScore(rawdata, "游戏B") 所有渠道的指标评分在第6周迅速下降，导致各渠道总体评分也迅速下降。在第8周时，7日留存率评分明显上升，其它指标评分依旧下降。这可能说明，该款游戏可能在第2周到第6周开启了某种促销活动，使得在这期间各指标评分迅速上升，当活动结束后，只有7日留存率指标评分呈现波动趋势，而其它指标评分呈现下降趋势。 游戏C的各渠道评分 1AnalysisChannel(rawdata, "游戏C") 渠道H从第1周到第3周，渠道用户评分一直呈现增长趋势，之后评分一直在12～14之间波动，属于表现优异的渠道；渠道D、渠道E、渠道F、渠道G、渠道J、渠道K用户评分整体下滑明显。]]></content>
      <categories>
        <category>游戏数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渠道分析—新增用户质量]]></title>
    <url>%2F%E6%B8%B8%E6%88%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F%E6%B8%A0%E9%81%93%E5%88%86%E6%9E%90%E2%80%94%E6%96%B0%E5%A2%9E%E7%94%A8%E6%88%B7%E8%B4%A8%E9%87%8F%2F</url>
    <content type="text"><![CDATA[渠道分析—新增用户质量 通过对新增用户、ARPPU、次日留存率绘制气泡图，查看不同渠道的新增用户质量。 数据 假设某游戏的各个渠道同一日的用户数据如下： 渠道名称 活跃用户 新增用户 ARPPU 次日留存率 0 渠道1 386421 53712 18.10 0.33 1 渠道2 342286 80033 19.12 0.31 2 渠道3 300085 66267 18.70 0.32 3 渠道4 213464 59684 18.39 0.30 4 渠道5 191074 65930 19.66 0.28 5 渠道6 164305 20282 18.93 0.32 6 渠道7 160536 24361 16.59 0.36 7 渠道8 157983 33623 21.09 0.32 8 渠道9 144279 47191 19.87 0.30 9 渠道10 8907 1048 19.59 0.40 渠道用户数据气泡图 导入用户渠道数据，将ARPPU表示气泡大小，新增用户数量作为X轴，次日留存率作为Y轴，绘制气泡图。 导入数据处理和绘图所需的python库 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] # 显示图形中的中文标签 读取渠道用户数据 1df = pd.read_csv(('渠道用户数据.csv')) 按ARPPU大小排序，设置气泡大小 1size = df['ARPPU'].rank() 绘制渠道用户数据气泡图 1234567891011121314plt.figure(dpi=150)colors = np.arange(len(df["渠道名称"].unique()))# 使用散点图绘制气泡，根据size变量设置点的大小plt.scatter(df['新增用户'], df['次日留存率'], s=size * 200, c=colors, alpha=0.5)plt.xlabel('新增用户')plt.ylabel('次日留存率')plt.title('渠道用户数据气泡图')for i, text in enumerate(df['渠道名称']): plt.text( df.loc[i]['新增用户'], df.loc[i]['次日留存率'], text, ha = 'center', va='center', fontsize=7 )plt.show() 可以看到，渠道10虽然新增用户最少，但其次日留存最高，说明渠道10的新增用户黏性高于其它渠道；渠道8的的气泡最大，说明该渠道ARPPU大于其它渠道，用户付费能力最强；渠道5的次日留存率最低，ARPPU处于中上水平，说明该渠道用户的黏性差，但付费能力较好。]]></content>
      <categories>
        <category>游戏数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大抽样分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E4%B8%89%E5%A4%A7%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[三大抽样分布 概览 统计量的构造 抽样分布密度函数 期望 方差 卡方分布\(Y = X_1^2 + \cdots + X_n^2\) \(p(y) = \frac{(1/2)^{\frac{n}{2}}}{\Gamma(n/2)}y^{\frac{n}{2} - 1}e^{-\frac{y}{2}}, \\ y &gt; 0\) \(n\) \(2n\) \(F\)分布\(Z = \frac{(Y_1+\cdots+Y_m)/m}{(X_1+\cdots+X_n)/n}\) \(p(y) = \frac{\Gamma(\frac{m + n}{2})(\frac{m}{n})^{m/2}}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}y^{\frac{m}{2}-1}(1 + \frac{m}{n}y)^{-\frac{m + n}{2}}\) \(\frac{n}{n - 2}, \\(n &gt; 2)\) \(\frac{2n^2(m+n-2)}{m(n-2)^2(n-4)},\\(n &gt; 4)\) \(t\)分布\(Z = \frac{Y}{\sqrt{(X_1+\cdots+X_n)/n}}\) \(p(y) = \frac{\Gamma(\frac{n + 1}{n})}{\sqrt{n\pi}\Gamma(\frac{n}{2})}(1 + \frac{y^2}{n})^{- \frac{n + 1}{2}}, \\ -\infty &lt; y &lt; +\infty\) \(0, \\ (n &gt; 1)\) \(\frac{n}{n - 2},\\(n &gt; 2)\) \(\chi^2\)分布（卡方分布） 定义 设\(X_1，X_2，\cdots, X_n​\)为独立同分布于标准正态分布\(N(0, 1)​\)，则\(Y = X_1^2 + X_2^2 + \cdots + X_n^2​\)的分布称为自由度为\(n​\)的\(\chi^2​\)分布，记为\(Y \sim \chi^2(n)​\)。 若\(X \sim N(0, 1)​\)，则\(X^2 \sim Ga(1/2, 1/2)​\)，根据伽马分布的可加性有\(Y \sim Ga(n/2, 1/2) = \chi^2(n)​\)，由此可见，\(\chi^2(n)​\)分布是伽马分布的特例，故\(\chi^2(n)​\)分布的密度函数为\[p(y) = \frac{(1/2)^{\frac{n}{2}}}{\Gamma(n/2)}y^{\frac{n}{2} - 1}e^{-\frac{y}{2}}, y &gt; 0​\]。 卡方分布的可加性：设\(X_1 \sim \chi^2(m)\)，\(X_2 \sim \chi^2(n)\)，且两者相互独立，则\(X_1 + X_2 \sim \chi^2(m + n)\)。 数学期望和方差 设随机变量\(X \sim \chi^2(n)​\)，则\(X​\)的数学期望\(E(X) = n​\)，\(X​\)的方差\(Var(X) = 2n​\)。 概率与分位数计算 若\(X​\)服从自由度为25的卡方分布，即\(X\sim\chi^2(25)​\)，求\(P(X &gt; 36)​\)，分位数\(\chi^2_{\alpha = 0.1}(25)​\)。 Excel 概率计算 概率\(P(X &gt; 36)\) = (=CHIDIST(36, 25)) = 0.071599862 分位数 分位数\(\chi_{\alpha = 0.1}^2(25)​\) = CHIINV(0.1, 25) = 34.38158702 F分布 定义 设\(Y \sim \chi^2(m)\)，\(X \sim \chi^2(n)\)，且相互独立，则称\(Y = \frac{Y/m}{X / n}\)的分布是自由度为\(m\)与\(n\)的分布，记为\(Z \sim F(m, n)\)，其中\(m\)称为分子自由度，\(n\)称为分母自由度。 F分布的密度函数为\(p(y) = \frac{\Gamma(\frac{m + n}{2})(\frac{m}{n})^{m/2}}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}y^{\frac{m}{2}-1}(1 + \frac{m}{n}y)^{-\frac{m + n}{2}}​\) 若\(Z \sim F(m, n)​\)，由\(F​\)分布定义可知\(1/Z \sim F(n, m)​\)，则有\(F_\alpha(n, m) = \frac{1}{F_{1-\alpha}(m, n)}​\) 数学期望和方差 设随机变量\(X \sim F(m, n)​\)，则\(X​\)的数学期望\(E(X) = \frac{n}{n - 2}, (n &gt; 2)​\)，\(X​\)的方差\(Var(X) = \frac{2n^2(m+n-2)}{m(n-2)^2(n-4)},(n &gt; 4)​\)。 概率与分位数计算 若\(X\)服从自由度为9和10的\(F\)分布，即\(X\sim F(9, 10)\)，求\(P(X &gt; 3)\)的概率，分位数\(F_{0.1}(9, 10)\) Excel 概率计算 概率\(P(X &gt; 3)\) = FDIST(3, 9, 10) = 0.051002478 分位数 分位数\(F_{0.1}(9, 10)\) = FINV(0.1, 9, 10) = 2.34730591 t分布 定义 设随机变量\(Y\)与\(X\)相互独立且\(Y \sim N(0, 1)\)，\(X \sim \chi^2(n)\)，则称\(Z = \frac{Y}{\sqrt{X/n}}\)的分布为自由度为\(n\)的\(t\)分布，记为\(Z \sim t(n)\)。 自由度为\(n​\)的\(t​\)分布的密度函数为\(p(y) = \frac{\Gamma(\frac{n + 1}{n})}{\sqrt{n\pi}\Gamma(\frac{n}{2})}(1 + \frac{y^2}{n})^{- \frac{n + 1}{2}}, -\infty &lt; y &lt; +\infty​\)。 数学期望和方差 设随机变量\(X \sim t(n)​\)，则\(X​\)的数学期望\(E(X) = 0, (n &gt; 1)​\)，\(X​\)的方差\(Var(X) = \frac{n}{n - 2},(n &gt; 2)​\)。 概率与分位数计算 若\(X\)服从自由度为5的\(t\)分布，即\(X \sim t(5)\)，求概率\(P(X &gt; 2)\)，分位数\(t_{0.1}(5)\) Excel 概率计算 概率\(P(X &gt; 2)\) = TDIST(2, 5, 1) = 0.050969739 分位数 分位数\(t_{0.1}(5)\) = TINV(0.1, 25) = 1.708140761 重要结论 以下这些结论在参数估计的枢轴量和假设检验中检验统计量的构造中有重要作用。 设\(x_1,\cdots,x_n​\)是来自正态总体\(N(\mu, \sigma^2)​\)的样本，其样本均值和样本方差分别为\(\overline{x} = \frac{1}{n}\sum_{i = 1}^n x_i​\)和\(s^2 = \frac{1}{n - 1}\sum_{i = 1}^n(x_i - \overline{x})^2​\)，则有 \(\overline{x}\)和\(s^2\)相互独立； \(\overline{x} \sim N(\mu, \sigma^2/n)\) \(\frac{(n - 1)\cdot s^2}{\sigma^2} \sim \chi^2(n - 1)\) \(\frac{\sqrt n(\overline{x} - \mu)}{s} \sim t(n - 1)\) 设\(x_1,\cdots,x_m\)是来自正态总体\(N(\mu_1, \sigma_1^2)\)的样本，\(y_1,\cdots,y_n\)是来自正态总体\(N(\mu_2, \sigma_2^2)\)的样本，且此两样本相互独立，记 \[ \overline{x} = \frac{1}{m}\sum_{i = 1}^mx_i, \quad s_x^2 = \frac{1}{m - 1}\sum_{i = 1}^m(x_i - \overline{x})^2, \\ \overline{y} = \frac{1}{n}\sum_{i = 1}^ny_i, \quad s_y^2 = \frac{1}{n - 1}\sum_{i = 1}^n(y_i - \overline{y})^2 \] 则有\[F = \frac{s_x^2/m}{s_y^2/n} \sim F(m - 1, n - 1)\] 特别，若\(\sigma_1^2 = \sigma_2^2​\)，则\(F = s_x^2/s_y^2 \sim F(m-1, n-1)​\)。 设\(x_1,\cdots,x_m\)是来自正态总体\(N(\mu_1, \sigma^2)\)的样本，\(y_1,\cdots,y_n\)是来自正态总体\(N(\mu_2, \sigma^2)\)的样本，且此两样本相互独立，记 \[ \overline{x} = \frac{1}{m}\sum_{i = 1}^mx_i, \quad s_x^2 = \frac{1}{m - 1}\sum_{i = 1}^m(x_i - \overline{x})^2, \\ \overline{y} = \frac{1}{n}\sum_{i = 1}^ny_i, \quad s_y^2 = \frac{1}{n - 1}\sum_{i = 1}^n(y_i - \overline{y})^2, \\ \begin{align*} s_w^2 &amp;= \frac{(m - 1)s_x^2 + (n - 1)s_y^2}{m + n - 2} \\ &amp;= \frac{\sum_{i=1}^m(x_i - \overline{x})^2 + \sum_{i = 1}^n(y_i - \overline{y})^2}{m + n - 2} \end{align*} \] 则有\[\frac{(\overline{x} - \overline{y}) - (\mu_1 - \mu_2)}{s_w\sqrt{\frac{1}{m} + \frac{1}{n}}} \sim t(m + n - 2)\]]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
        <tag>卡方分布</tag>
        <tag>F分布</tag>
        <tag>t分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计量及其分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E7%BB%9F%E8%AE%A1%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[统计量及其分布 总体与样本 概念 总体：在一个统计问题中，我们将所研究对象的全体称为总体 个体：构成总体的每个成员称为个体 样本：从总体中随机抽取\(n\)个个体，记其指标值为\(x_1, x_2, \cdots, x_n\)，则\(x_1, x_2, \cdots, x_n\)称为总体的一个样本，\(n\)称为样本容量，或简称样本量，样本中的个体称为样品。 样本二重性：一方面，由于样本是从总体中随机抽取的，抽取前无法预知它们的数值，因此样本是随机变量；另一方面，样本在抽取以后经观测就有确定的观测值，因此样本又是一组数值。 简单随机抽样：用简单随机抽样方法得到的样本称为简单随机样本。简单随机抽样方法有以下两个要求： 样本具有随机性，即要求总体中的每一个个体都有同等的机会呗选入样本，这意味着每一样品\(x_i\)和总体\(X\)有相同的分布。 样本要有独立性，即要求样本中每一样品的取值不影响其它样品的取值，这意味着\(x_1, x_2, \cdots, x_n\)相互独立。 样本联合分布函数：设总体\(X\)具有分布函数\(F(x_1, x_2, \cdots, x_n)\)为取自总体的容量为\(n\)的样本，则样本联合分布函数为\[F(x_1, x_2, \cdots, x_n) = \prod_{i = 1}^n F(x_i)\] 统计量及其分布 概念 统计量：设\(x_1, x_2, \cdots, x_n\)为取自某总体的样本，若样本函数\(T = T(x_1, x_2, \cdots, x_n)\)中不含有任何未知参数，则称\(T\)为统计量 抽样分布：统计量的分布称为抽样分布 样本均值：设\(x_1, x_2, \cdots, x_n\)为取自某总体的样本，其算术平均值称为样本均值，一般用\(\overline{x}\)表示，即\[ \overline{x} = \frac{x_1 + \cdots + x_n}{n} = \frac{1}{n} \sum_{i=1}^nx_i\] 在分组样本场合，样本均值的近似公式为\[ \overline{x} = \frac{x_1f_1 + \cdots + x_nf_n}{n}, n = \sum_{i=1}^kf_i\]，其中\(k\)为组数，\(x_i\)为第\(i\)组的组中值，\(f_i\)为第\(i\)组的频数。 样本方差：设\(x_1, x_2, \cdots, x_n\)为取自某总体的样本，则它关于样本均值\(\overline{x}\)的平均偏差平方和\[s^{*^2} = \frac{1}{n}\sum_{i = 1}^n(x_i - \overline{x})^2\]称为样本方差，其算术平方根\(s^* = \sqrt {s^{*^2}}\)称为样本标准差。在\(n\)不大时，常用\[s^{*^2} = \frac{1}{n - 1}\sum_{i = 1}^n(x_i - \overline{x})^2\]作为样本方差（也称无偏方差），其算术平方根\(s = \sqrt {s^2}\)也称为样本标准差。在这个概念中，\(n\)为样本量，\(\sum_{i = 1}^n(x_i - \overline{x})^2\)称为偏差平方和，\(n - 1\)称为偏差平方和的自由度。其含义是：在\(\overline{x}\)确定后，\(n\)个偏差\(x_1 - \overline{x}, \cdots, x_n - \overline{x}\)中只有\(n - 1\)个数据可以自由变动，而第\(n\)个则不能自由取值，因为\(\sum(x_i - \overline{x}) = 0\)。样本偏差平方和有三个不同的表达式：\[\sum(x_i - \overline{x})^2 = \sum x_i^2 - \frac{(\sum x_i)^2}{n} = \sum x_i^2 - n\overline{x}^2\]在分组样本场合，样本方差的近似计算公式为\[s^2 = \frac{1}{n - 1}\sum_{i = 1}^nf_i(x_i - \overline{x})^2 = \frac{1}{n - 1}[\sum_{i = 1}^k f_ix_i^2 - n\overline{x}^2]\]其中\(x_i\)，\(f_i\)分别为第\(i\)个区间的组中值和频数，\(\overline{x}\)为分组场合下的样本均值。 样本矩：设\(x_1, x_2, \cdots, x_n\)是样本，则统计量\(a_k = \frac{1}{n}\sum_{i=1}^n x_i^k\)称为样本\(k\)阶原点矩，特别，样本一阶原点矩就是样本均值。统计量\(b_k = \frac{1}{n}\sum_{i = 1}^n(x_i - \overline{x})^k\)称为样本\(k\)阶中心矩，特别，样本二阶中心矩就是样本方差。 样本偏度：设\(x_1, x_2, \cdots, x_n\)是样本，则称统计量\(\gamma_1 = b_3/b_2^{3/2}\)为样本偏度。样本偏度反映了总体分布密度曲线的对称信息。 样本峰度：设\(x_1, x_2, \cdots, x_n\)是样本，则称统计量\(\gamma_2 = \frac{b_4}{b_2^2} - 3\)为样本峰度。样本峰度反映了总体分布密度曲线在其峰值附近的陡峭程度。 样本分位数：设\(x_{(1)}, x_{(2)}, \cdots, x_{(n)}\)有序样本，则样本\(p\)分位数\(m_p\)可如下定义： \[ m_p = \begin{cases} x_{([np+1])}, &amp;若np不是整数;\\ \frac{1}{2}(x_{(np)} + x_{(np + 1)}),&amp;若np为整数. \end{cases} \] 譬如，若\(n = 10\)，\(p = 0.35\)，则\(m_{0.35}=x_{(4)}\)，若\(n = 20\)，\(p = 0.45\)，则\(m_{0.45} = \frac{1}{2}(x_{(9)} + x_{(10)})\)。 样本中位数：设\(x_{(1)}, x_{(2)}, \cdots, x_{(n)}\)有序样本，样本\(p = 0.5\)分位数\(m_{0.5}\)为样本中位数，定义为 \[ m_{0.5} = \begin{cases} x_{\frac{(n+1)}{2}}, &amp;n为奇数;\\ \frac{1}{2}(x_{(n/2)} + x_{(n/2 + 1)}),&amp;n为偶数. \end{cases} \] 譬如，若\(n = 5\)，则\(m_{0.5}=x_{(3)}\)，若\(n = 6\)，则\(m_{0.5} = \frac{1}{2}(x_{(3)} + x_{(4)})\)。 定理 若把样本中的数据与样本均值之差称为偏差，则样本所有偏差之和为0，即\(\sum_{i = 1}^n(x_i - \overline{x}) = 0\)。 数据观察值与均值的偏差平方和最小，即在形如\(\sum(x_i - c)^2\)的函数中，\(\sum(x_i - \overline{x})^2\)最小，其中\(c\)为任意给定常数。 设\(x_1,\cdots,x_n\)是来自某个总体的样本, \(\overline{x}\)是样本均值。 若总体分布为\(N(\mu, \sigma^2)\)，则\(\overline{x}\)的精确分布为\(N(\mu, \sigma^2/n)\)； 若总体分布未知或不是正态分布，但\(E(x) = \mu\)，\(Var(x) = \sigma^2\)，则\(n\)较大时\(\overline{x}\)的渐进分布为\(N(\mu, \sigma^2/n)\)，常记为\(\overline{x} \sim^\cdot N(\mu, \sigma^2/n)\)。这里渐进分布是指\(n\)较大时的近似分布。 设总体\(X\)具有二阶矩，即\(E(X) = \mu\)，\(Var(X) = \sigma^2 &lt; +\infty\)，\(x_1,\cdots,x_n\)为从该总体得到的样本，\(\overline{x}\)和\(s^2\)分别是样本均值和样本方差，则有\(E(\overline{x}) = \mu\)，\(Var(\overline{x})=\sigma^2/n\)，\(E(s^2) = \sigma^2\)。此定理表明，样本均值的均值与总体均值相同，而样本均值的方差是总体方差的\(1/n\)。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
        <tag>统计量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy爬取二手房信息]]></title>
    <url>%2FPython%2Fscrapy%E7%88%AC%E5%8F%96%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[Scrapy爬取二手房信息 Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。这里使用Scrapy对链家网网页中的广州二手房信息进行爬取。 网页结构分析 以链家网网页信息作为信息来源，打开二手房信息页面。 可以看到网页中，位置筛选条件分为“区域”和“地铁”两个选项。这里将采用对区域分别进行爬取，步骤如下： 按照区域划分，先选择一个区域，如“天河”，筛选出该区域的所有二手房信息； 按当前所选区域的所有二手房信息进行逐页爬取，提取该区域的所有二手房信息； 完成当前区域的所有二手房信息提取后，返回到第1步，继续选择下一个区域，直至所有区域二手房信息都被提取完成； 合并所有区域的二手房信息，然后输出称CSV文件。 Scrapy实现爬取 安装 运行环境 操作系统：macOS Mojave Python：3.7 pip：19.1.1 pip是一个安装python包的工具。安装方法：pip 安装 使用pip安装Scrapy，在终端输入以下命令： 1$ pip install Scrapy 待上面命令执行完后，输入 1$ scrapy --help 提示类似以下信息，表示已经安装成功。 创建爬虫 在开始爬取之前，必须先创建一个爬虫项目： 1scrapy startproject mySpider 其中，mySpider为项目名称。该命令执行完成后，会创建一个mySpider文件夹，目录结构大致如下： 1234567891011├── mySpider│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── settings.py│ └── spiders│ ├── __init__.py│ └── house.py└── scrapy.cfg2 directories, 8 files 这些文件分别是： scrapy.cfg： 项目的配置文件。 mySpider/items.py：项目中的item文件，用于定义提取内容的数据结构。 mySpider/spiders/：放置spider代码的目录，同一个项目可以有多个spider，都放在这个目录下。 mySpider/pipelines.py：项目中的pipelines文件，用于对提取内容进行结构化处理。 mySpider/settings.py：项目的设置文件，例如：可用于限制访问频率等。 数据结构定义 根据前面的网页结构分析，我们要提取网页中二手房的相关信息：房子标题、房子名称、户型、面积、楼层、行政区、行政街道、关注、发布日期、总价、单价。 在mySpider/items.py文件中创建一个HouseItem类，该类继承于scrapy.Item类，在HouseItem类中添加以上信息字段，并定义类型为scrapy.Field的类属性。 12345678910111213141516171819202122232425import scrapyclass HouseItem(scrapy.Item): # 标题 title = scrapy.Field() # 名称 name = scrapy.Field() # 户型 room = scrapy.Field() # 面积 area = scrapy.Field() # 楼层 floor = scrapy.Field() # 行政区 region = scrapy.Field() # 行政街道 street = scrapy.Field() # 关注 follow = scrapy.Field() # 发布日期 publish = scrapy.Field() # 总价，单位：万 price = scrapy.Field() # 单价，单位：平方米 unit_price = scrapy.Field() 编写爬虫 爬虫爬取网页的基本流程可以分为三个步骤： 模拟浏览器向指定的网页发起请求； 接收网站返回的响应数据； 根据网站返回的数据，提取所需信息。若要进行下一次爬取，则重复以上步骤。 在mySpider目录下，使用scrapy的genspider命令，可以创建包含以上流程的基本框架代码。输入命令： 1scrapy genspider house "lianjia.com" 将创建名为house的爬虫，且基本框架代码保存于spiders目录下的house.py文件中。 查看house.py文件内容： 12345678910# -*- coding: utf-8 -*-import scrapyclass HouseSpider(scrapy.Spider): name = 'house' allowed_domains = ['lianjia.com'] start_urls = ['http://lianjia.com/'] def parse(self, response): pass 可以看到，scrapy的genspider命令为爬虫创建一个名为HouseSpider的类，其继承了scrapy.Spider类，并创建了三个默认属性和一个默认方法。 name：爬虫名称，用于识别不同的爬虫。 allowed_domains：表示搜索的域名范围，规定了爬虫只爬取这个域名下的网页。 start_urls：表示爬虫初始请求的URL列表。 parse(self, response)：用于负责接收和解析返回的网页数据(response.body)，提取所需的网页数据。若有需要，生成下一页的URL请求。每个初始URL的网站返回响应数据后都将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数。 接下来，将要对house.py文件中的基本框架代码进行修改，以满足我们的爬取数据的需求。 发送获取二手房信息的初始请求 1234567891011121314class HouseSpider(scrapy.Spider): name = 'house' allowed_domains = ['lianjia.com'] headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36', &#125; # 起始请求地址 start_url = 'http://gz.lianjia.com/ershoufang' # 限制所有行政区域的爬取页数 limit_page = 1 def start_requests(self): yield scrapy.Request(self.start_url, callback=self.parse_region, headers=self.headers) 上述代码中，将start_urls属性改为start_url，其值为起始请求地址；增加limit_page属性，用于限制所有行政区域的爬取页数；增加headers属性，用于模拟浏览器请求。 新增方法start_request(self)用于向scrapy.Request函数传递指定参数，重定义起始请求的行为。callback参数意思是使用新增的方法self.parse_region解析此次请求的返回的网站数据，这里是用于解析行政区域信息。 提取二手房所有区域信息，并对每个区域逐个发送请求，获取对应区域的二手房信息。 通过在浏览器中选择页面中要提取的内容，右键点击检查元素，定位到相应的页面源码中元素的的位置 拷贝出该元素的xpath xpath语法 12345678910def parse_region(self, response): # 提取所有行政区域 region_list = response.xpath('/html/body/div[3]/div/div[1]/dl[2]/dd/div[1]/div[1]/a') for region in region_list: region_data = region.xpath('text()').extract() # 区域名称 region_href = region.xpath('@href').extract() # 区域链接 region_uri = region_href[0].split('/')[2] region_url = '/'.join([self.start_url, '&#123;&#125;/']).format(region_uri) meta_data = &#123;"region":region_data, "region_uri":region_uri&#125; yield scrapy.Request(url=region_url, headers=self.headers, callback=self.parse, meta=meta_data) 提取所有区域名称和链接，生成每个行政区域的链接地址，然后模拟浏览器发送请求。需要注意的是，对于每个行政区域请求的返回数据，callback的参数使用的是方法parse进行处理。参数meta用于向callback参数所使用的方法中的response参数传递数据。 提取页面中每个行政区域分页的二手房信息。 对于每个行政区域，其房产信息可能会分页展示，因此需要逐页对每个分页的二手房信息进行提取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def parse(self, response): info_list = response.xpath('//*[@id="content"]/div[1]/ul/li/div[@class="info clear"]') for info in info_list: item = self.make_item(info, response) yield item # 提取分页信息 page_data = response.xpath('//*[@id="content"]/div[1]/div[8]/div[2]/div/@page-data').extract() cur_page = 1 total_page = 1 if len(page_data): page_data = json.loads(page_data[0]) cur_page = page_data['curPage'] total_page = page_data['totalPage'] next_page = cur_page + 1 # 判断是否需要爬取下一分页的条件，若满足，则构建新的url，并发送请求 if (next_page &lt;= total_page and next_page &lt;= self.limit_page): uri_list = [self.start_url, response.meta['region_uri'], "pg&#123;&#125;/"] next_url = '/'.join(uri_list).format(next_page) yield scrapy.Request(next_url, callback=self.parse, headers=self.headers, meta=response.meta)# 提取二手房信息def make_item(self, info, response): item = HouseItem() # 提取房子相关信息 title = info.xpath('div[@class="title"]/a/text()').extract() name = info.xpath('div[@class="address"]/div/a/text()').extract() desc = info.xpath('div[@class="address"]/div/text()').extract() floor = info.xpath('div[@class="flood"]/div/text()').extract() street = info.xpath('div[@class="flood"]/div/a/text()').extract() follow = info.xpath('div[@class="followInfo"]/text()').extract() price = info.xpath('div[@class="priceInfo"]/div[@class="totalPrice"]/span/text()').extract() unit_price = info.xpath('div[@class="priceInfo"]/div[@class="unitPrice"]/@data-price').extract() item['region'] = response.meta['region'] # 行政区域 item['title'] = title[0] # 标题 item['name'] = name[0] # 名称 desc = [str.strip(x) for x in desc[0].split('|')] item['room'] = desc[1] # 户型 item['area'] = desc[2] # 面积 floor = floor[0].split() item['floor'] = floor[0] # 楼层 item['street'] = street[0] # 街道 follow = [str.strip(x) for x in follow[0].split('/')] item['follow'] = follow[0] # 关注 item['publish'] = follow[1] # 发布时间 item['price'] = float(price[0]) * 10000 # 总价：万 item['unit_price'] = unit_price[0] # 单价：平方米 return item 与提取所有区域内容一样，通过浏览器的检查元素，获取xpath，然后分别对所需内容进行提取。 爬虫配置 爬虫的逻辑代码完成后，需要对爬虫进行相应配置。配置文件为mySpider目录下settings.py文件。scrapy运行时会根据该配置文件内容设置爬取规则。打开settings.py文件，设置以下内容： 遵守爬虫规则文件 12# 遵守robots.txt规则文件ROBOTSTXT_OBEY = True 禁用COOKIES 12# 禁用COOKIES，默认是开启的COOKIES_ENABLED = False 限制访问速率 12345678910# 开启访问频率限制AUTOTHROTTLE ENABLED = True# 设置访问开始的延迟AUTOTHROTTLE START DELAY = 5# 设置访问之间的最大延迟AUTOTHROTTLE MAX DELAY = 60# 设置Scrapy 并行发给每台远程服务器的请求数量AUTOTHROTTLE TARGET CONCURRENCY= 1.0# 设置下裁之后的自动延迟DOWNLOAD DELAY = 3 运行爬虫 在mySpider目录输入命令 1$ scrapy crawl house 即可开始执行脚本。现要将爬取的内容保存为CSV文件，则只需在该命令后添加参数“-o”，用于指定输出格式文件。 1$ scrapy crawl house -o house.csv 命令执行完后，Scarpy在日志等级为DEBUG的情况下，会打印爬取操作过程中的状态信息及Item数据信息： 123456789101112131415161718192021222324252627282019-07-01 20:32:35 [scrapy.core.engine] INFO: Closing spider (finished)2019-07-01 20:32:35 [scrapy.extensions.feedexport] INFO: Stored csv feed (723 items) in: house.csv2019-07-01 20:32:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;&apos;downloader/request_bytes&apos;: 17393, &apos;downloader/request_count&apos;: 55, &apos;downloader/request_method_count/GET&apos;: 55, &apos;downloader/response_bytes&apos;: 904411, &apos;downloader/response_count&apos;: 55, &apos;downloader/response_status_count/200&apos;: 27, &apos;downloader/response_status_count/301&apos;: 28, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2019, 7, 1, 12, 32, 35, 139438), &apos;item_scraped_count&apos;: 723, &apos;log_count/DEBUG&apos;: 778, &apos;log_count/INFO&apos;: 13, &apos;memusage/max&apos;: 76783616, &apos;memusage/startup&apos;: 51191808, &apos;request_depth_max&apos;: 2, &apos;response_received_count&apos;: 27, &apos;robotstxt/request_count&apos;: 1, &apos;robotstxt/response_count&apos;: 1, &apos;robotstxt/response_status_count/200&apos;: 1, &apos;scheduler/dequeued&apos;: 53, &apos;scheduler/dequeued/memory&apos;: 53, &apos;scheduler/enqueued&apos;: 53, &apos;scheduler/enqueued/memory&apos;: 53, &apos;start_time&apos;: datetime.datetime(2019, 7, 1, 12, 29, 8, 618745)&#125;2019-07-01 20:32:35 [scrapy.core.engine] INFO: Spider closed (finished) 爬取结果：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系代数]]></title>
    <url>%2FSQL%2F%E5%85%B3%E7%B3%BB%E4%BB%A3%E6%95%B0%2F</url>
    <content type="text"><![CDATA[关系代数 关系可以理解为一张规范化的二维表。一个关系由关系名、关系模式和关系实例组成。它们通常分布对应于二维表的表名、表头和数据。下面有三张二维表： 每张二维表中的属性含义如下： 玩家角色信息表（Role） RId：玩家角色id； RName：玩家角色名称; RSex：玩家角色性别; RLev：玩家角色等级； RClass：玩家角色职业。 任务表（Task） TId：任务id； TName：任务名称。 任务评分表（Grade） RId：玩家角色id； TId：任务id； Score：任务评分。 以二维表Role为例，表名Role即为关系名，表头RId、RName、RSex、RLev、RClass即为一个关系模式，每一行数据表示一个关系实例。 关系代数就是在关系上定义了一些运算，这些运算具有封闭性，使得运算的结果仍然是关系。 关系代数运算 并运算 并运算是将两个关系产生出一个新关系，且新关系的关系模式与原来的两个关系的关系模式相同。因此，原来的两个关系的关系模式也是一致的。关系R并上关系S可记为\(R \cup S\)。 例： 交运算 交运算是将两个关系中所有完全相同的关系实例建立一个新的关系。与并运算一样，要求原来的两个关系的关系模式是一致的。关系R交上关系S可以记为\(R \cap S\)。 例： 差运算 关系R与关系S的差运算是将属于关系R中而不属于关系S中的关系实例建立的一个新关系。关系R与关系S模式也是一致的。关系R与关系S的差运算可记为\(R - S\)。 例： 广义笛卡尔积 若关系R的关系模式中有m个属性，关系S的关系模式中有n个属性，则关系R与关系S的广义笛卡尔积是一个关系模式有m+n个属性的新关系。在新关系中，关系模式由关系R和关系S的关系模式连接而成，即关系模式的前m个属性是来自关系R的，后n个属性是来自关系S的。若关系R有x个实例，关系S有y个实例，则新关系有x * y个实例。关系R和关系S的广义笛卡尔积可以记为\(R \times S\)。 例： 选择运算 选择运算是一个一元运算符，选择运算的结果是一个新的关系，新关系的关系模式与被操作关系的相同，关系实例是被操作关系中满足条件的关系实例，即新关系中的关系实例是被操作关系中的关系实例的子集。对关系R的选择运算可记为\(\sigma_F(R)\)，其中R表示被操作的关系，F表示选择条件，它是一个逻辑表达式。逻辑表达式F的基本形式为：\[X_1 \theta Y_1[\phi X_2 \theta Y_2]\cdots\] 其中\(\theta\)表示比较运算符，它可以是\(&gt;, \le, \ge, &lt;, =, \ne\)。\(X_1\)，\(Y_1\)等可表示属性名、常量或简单函数。\(\phi\)表示逻辑运算符，即\(\neg\)（非）、\(\land\)（与）、\(\lor\)（或）。 选择运算实际上是从关系R中选取逻辑表达式F为真的关系实例。这是从行的角度进行的运算。 例：从选择关系Role中性别为男且职业为光系的结果。关系代数：\[\sigma_{RSex = &#39;男&#39; \land RClass = &#39;水系&#39; }(Role)\] 投影运算 投影是指从指定的关系中保留一些列，去掉其他列后形成新的关系，记作\(\pi_A(R)\)，其中A为R中需要保留的属性组。投影操作是从列的角度进行的运算。 投影之后的新关系中，不仅删除了原关系中的某些属性列，而且还可能删除某些实例，因为删除某些属性例后，可能会出现重复的实例，而相同的实例中只会保留一条实例。 例：从关系Role中列出角色姓名和角色等级。关系代数：\[\pi_{RName, RLev}(Role)\] 连接运算 连接运算是从两个关系的广义笛卡尔积中，在列方向进行选择运算，在列方向进行投影运算，从而产生一个新的关系。关系R和关系S的连接运算可以记作\(R \Join_{条件} S\)，其中条件的一般形式是\(A \theta B\)，\(\theta = \{ =, &gt;, \le, &lt;, \ge, \ne\}\)，A、B分别为关系R、关系S中的一个属性，且A和B必须是同一个定义域。 例：对关系R与关系S进行RLev &lt; Score的连接运算。关系代数：\[R \Join_{RLev &lt; Score} S\] 连接运算中有两种常用的连接方式，一种是等值连接，另一种是自然连接。 等值连接 当连接运算中的条件\(\theta\)为“=”时，该连接运算可称为等值连接。它是从关系R和关系S的广义笛卡尔积中选取A，B属性值相等的那些关系实例。 例：对关系R和关系S基于属性RId进行等值连接。关系代数：\[R \Join_{R.RId = S.RId} S\] 自然连接 自然连接是一种特殊的连接，当连接条件为空时，该连接运算可以称为自然连接。它要求关系R中的属性A和关系S中的属性B名字相同，并且在结果中把重复的属性列去掉。一般的连接操作是从行的角度进行运算，但自然连接还需要删除重复的列，所以自然连接是同时从行和列的角度进行运算的。 例：对关系R和关系S进行自然连接。关系代数：\[R \Join S\] 外连接 关系R和关系S在做自然连接时，选择两个关系在公共属性上值相等的实例构成新的关系。此时，关系R中的某些实例可能在关系S中不存在相等的实例，从而造成R中的这些实例被舍弃了，同样，关系S中的某些实例也可能被舍弃。如果把舍弃的实例也保留在新的关系中，在新增加的属性上留空，那么这种连接就叫做外连接（outer join）。如果只保留关系R中要被舍弃的实例就叫左外连接，如果只保留关系S中要被舍弃的实例就叫右外连接。 外连接 左外连接 右外连接 除运算 给定关系R(X, Y)和关系S(Y, Z)，其中X，Y，Z为属性组。R中的Y与S中的Y可以有不同的属性名，但必须出自相同的域集。关系R与关系S的除运算得到一个新的关系P(X)，对于任何一个实例\(t \in P(X)\)，有\(t \in \pi_{X}(R)\)，并且\(t \times \pi_{Y}(S) \subseteq R\)。关系R与关系S的除运算可以记作\(R \div S\)。 关系代数运算示例 例1：查询TId为1024的RId和Score。\[\pi_{RId, Score}(\sigma_{TId = &#39;1024&#39;}(Grade))\] 例2：查询TId为1024的RId和RName。\[\pi_{RId, RName}(Role \Join (\sigma_{TId = &#39;1024&#39;}(Grade))))\] 例3：查询TName为“光明教皇”的RId和RName。\[\pi_{RId, RName}(Role \Join ((\sigma_{TName = &#39;光明教皇&#39;} (Task)) \Join Grade))\] 例4：查询TId为1024或1136的RId。\[\pi_{RId} (Role \Join (\sigma_{TId = &#39;1024&#39; \lor TId = &#39;1136&#39;}(Grade)))\] 例5：查询至少接受TId为1024与1136任务的RId。\[\pi_{RId}(Role \Join (\sigma_{TId = &#39;1024&#39;}(Grade))) \Join (Role \Join (\sigma_{TId = &#39;1136&#39;}(Grade)))\] 例6：查询没有接受TId为1156任务的玩家角色名称RName和职业RClass。\[\pi_{RName, RClass}(Role) - \pi_{RName, RClass}(Role \Join (\sigma_{TId = &#39;1156&#39;}(Grade)))\] 例7：查询接受所有任务的玩家角色RId。\[\pi_{RId}(Grade \div Task)\] 例8：查询所接任务中包含角色“盖亚”所接受任务的玩家角色名称。\[\pi_{RName}(Role \Join (\pi_{RId}(Grade \div \pi_{TId}(\sigma_{RName = &#39;盖亚&#39;}(Role) \Join Grade))))\]]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[参数估计]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[参数估计 参数估计，指在总体分布已知情况下，利用样本信息对总体的以下三类未知参数进行估计。 分布中所含的未知参数\(\theta\)，如：二点分布\(b(1, p)\)中的概率\(p\)，均匀分布\(U(a, b)\)中的\(a\)和\(b\)。 分布中所含的未知参数\(\theta\)的函数，如：服从正态分布\(N(\mu, \sigma^2)\)的变量\(X\)不超过某给定值\(a\)的概率\(P(X \leq a) = \Phi(\frac{a - \mu}{\sigma})\)是未知参数\(\mu\)，\(\sigma\)的函数。 分布的各种特征数，如数学期望\(E(X)\)，方差\(Var(X)\)等。 通常情况下，常用\(\theta\)表示参数，参数\(\theta\)所有可能取值组成的集合称为参数空间，常用\(\Theta\)表示。 参数估计有两种常用的形式，一种叫点估计，就是用一个具体的数值去估计一个未知参数；另一种叫区间估计，就是要找两个统计量\(\widehat{\theta}_L = \widehat{\theta}_L(x_1,\cdots,x_n)\)和\(\widehat{\theta}_U = \widehat{\theta}_U(x_1,\cdots,x_n)\)，使得\(\widehat{\theta}_L \leq \widehat{\theta}_U\)，在得到样本观测值之后，就把\(\theta\)估计在区间\([\widehat{\theta}_L, \widehat{\theta}_U]\)。 点估计 概念 设\(x_1,\cdots,x_n\)是来自总体的一个样本，\(\theta\)是总体待估参数，我们用一个统计量\(\widehat{\theta} = \widehat{\theta}(x_1, \cdots, x_n)\)的取值作为\(\theta\)的估计值，\(\widehat{\theta}\)称为\(\theta\)的一个估计量，简称估计。 &gt; 估计量是样本的一个函数（它是一个随机变量），而估计值是一个估计量的实现值（它是一个数）。 两种常用的点估计方法有：矩法和极大似然法。 矩法估计 矩法估计的基本思想是基于以下的替换原则进行估计的，其实质是用经验分布函数替换总体分布函数。 用样本的原点矩或中心矩去替换总体的原点矩或中心矩； 用样本的原点矩或中心矩的函数去替换总体的原点矩或中心矩函数。 例如： 用样本均值（一阶原点矩）\(\overline{x}\)估计总体均值； 用样本方差（二阶中心矩）\(s_n^2\)估计总体方差； 用事件\(A\)出现的频率估计事件\(A\)发生的概率； 用样本的\(p\)分位数估计总体的\(p\)分位数。 矩法估计计算方法 设总体\(X\)的分布函数为\(F(x;\theta_1,\cdots,\theta_l)\)，其中\(\theta_1,\cdots,\theta_l\)为待估参数，设\(x_1,\cdots,x_n\)是来自总体\(X\)的样本，使用\[M_k(k = 1, 2,\cdots,l)\]表示\(k\)阶样本原点矩（或中心矩）。若总体\(X\)的\(k(k \geq l)\)阶原点矩（或中心矩）存在，并记为\(\mu_k\)，若\(\mu_k\)能表示成参数\(\theta_1,\cdots,\theta_l\)的函数，即\[\mu_k = \mu_k(\theta_1,\cdots,\theta_l), (k = 1, 2,\cdots, l)\] 则令前\(l\)阶样本原点矩（中心矩）与相应的前\(l\)阶总体原点矩（中心矩）相等，这样就得到一个联立方程： \[ \begin{cases} \mu_1(\theta_1,\cdots,\theta_l) = M_1\\ \mu_2(\theta_1,\cdots,\theta_l) = M_2\\ \qquad\qquad \vdots\\ \mu_l(\theta_1,\cdots,\theta_l) = M_l \end{cases} \] 可解出\(\widehat{\theta}_k=\widehat{\theta}_k(x_1, \cdots, x_n), k = 1,2,\cdots,l\)，然后用\(\widehat{\theta}_k\)作为\(\theta_k\)的估计，用这种方法得到的估计就叫做矩估计。 例1:设总体\(X\)服从泊松分布，即\(X \sim P(\lambda), \lambda &gt; 0\)，其中\(\lambda\)为未知参数，\(x_1, \cdots, x_n\)是来自总体\(X\)的样本，求\(\lambda\)的矩估计\(\widehat{\lambda}\)。 解：因为只有\(\lambda\)一个未知参数，所以令阶矩\(k = 1\)。由于1阶原点矩（期望）\(\mu_1 = E(X) = \lambda\)，1阶样本原点矩（样本均值）\(M_1 = \overline{x}\)，所以\(\lambda\)的矩估计为\(\widehat{\lambda} = \overline{x}\)。 另外，由于2阶中心矩（方差）\(\mu_2 = Var(X) = \lambda\)，2阶样本中心矩（样本方差）\(M_2 = s^2\)，所以\(\lambda\)的矩估计也可以为\(\widehat{\lambda} = s^2\)。 提示：这说明矩估计可能不唯一，这是矩法估计的一个缺点，此时通常应该尽量采用低阶矩给出未知参数的估计。所以\(\lambda\)的矩估计\(\widehat{\lambda} = \overline{x}\)更为合适。 例2:设\(x_1, \cdots, x_n\)是来自\((a, b)\)上的均匀分布\(U(a, b)\)的样本，求未知参数\(a\)，\(b\)的矩估计\(\widehat{a}, \widehat{b}\)。 解：因为有\(a\)，\(b\)两个未知参数，所以令阶矩\(k = 2\)。由于1阶原点矩（期望）\(\mu_1 = \frac{a + b}{2}\)，1阶样本原点矩（样本均值）\(M_1 = \overline{x}\)，2阶中心矩（方差）\(\mu_2 = \frac{(b - a)^2}{12}\)，2阶样本中心矩（样本方差）\(M_2 = s^2\)，已知\(\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i, s^2 = \frac{1}{n - 1}\sum_{i = 1}^n (x_i - \overline{x})^2\)，所以得到联立方程： \[ \begin{cases} \frac{a + b}{2} = \overline{x}\\ \frac{(b - a)^2}{12} = s^2 \end{cases} \] 解方程得即得所求参数的矩估计：\(\widehat{a} = \overline{x} - \sqrt3 s, \widehat{b} = \overline{x} + \sqrt3 s\)。 极大似然估计 概念 概率函数 设\(X\)是一个连续型或离散型随机变量，其概率函数定义为 \[ p(x) = \begin{cases} P(X = x), &amp; X为离散型随机变量，P(X = x)为X分布列;\\ f(x), &amp; X为连续型随机变量，f(x)为X的密度函数 \end{cases} \] 似然函数 设\(x_1,\cdots,x_n\)是来自总体\(X\)的样本，\(X\)的概率函数为\(p(x;\theta)\)，其中参数\(\theta \in \Theta\)，\(\Theta\)为参数空间。样本\((x_1,\cdots,x_n)\)的联合概率函数\[p(x_1;\theta)p(x_2;\theta)\cdots p(x_n;\theta)\]作为\(\theta\)函数，称其为似然函数，记为\(L(\theta) = L(\theta;x_1,\cdots,x_n)\)，即\[L(\theta) = L(\theta;x_1,\cdots,x_n) = p(x_1;\theta)p(x_2;\theta)\cdots p(x_n;\theta)\] 极大似然估计 若\(\widehat{\theta}(x_1, \cdots, x_n)\)是一个统计量，满足条件\[L(\widehat{\theta};x_1,\cdots,x_n) = max_{\theta \in \Theta}L(\theta;x_1,\cdots,x_n)\] 则称\(\widehat{\theta}(x_1, \cdots, x_n)\)是\(\theta\)的极大似然估计，简称MLE。 注：由极大似然估计的定义可知，求MLE就是求似然函数关于参数\(\theta\)的最大值问题。 极大似然估计不变性：如果\(\widehat{\theta}\)是\(\theta\)的极大似然估计，则对任一函数\(g(\theta)\)，其最大似然估计为\(g(\widehat{\theta})\)。 区间估计 概念 置信区间 设\(\theta\)是总体的一个参数，其参数空间为\(\Theta\)，\(x_1,\cdots,x_n\)是来自该总体的样本，对给定的一个\(\alpha(0 &lt; \alpha &lt; 1)\)，若有两个统计量\(\widehat{\theta_L} = \widehat{\theta_L}(x_1,\cdots, x_n)\)和\(\widehat{\theta_U} = \widehat{\theta_U}(x_1,\cdots, x_n)\)，若对任意的\(\theta \in \Theta\)，有\[P_\theta(\widehat{\theta_L} \leq \theta \leq \widehat{\theta_U}) \geq 1 - \alpha\] 则称随机区间\([\widehat{\theta_L}, \widehat{\theta_U}]\)为\(\theta\)的置信水平为\(1 - \alpha\)的置信区间，或简称\([\widehat{\theta_L}, \widehat{\theta_U}]\)是\(\theta\)的\(1 - \alpha\)置信区间，\(\widehat{\theta_L}\)和\(\widehat{\theta_U}\)分布称为\(\theta\)的（双侧）置信下限和置信上限。 置信水平\(1 - \alpha\)有一个频率解释：在大量重复使用\(\theta\)的置信区间\([\widehat{\theta_L}, \widehat{\theta_U}]\)时，每次得到的样本观测值是不同的，从而每次得到的区间估计值也是不一样的。对一次具体的观测值而言，\(\theta\)可能在\([\widehat{\theta_L}, \widehat{\theta_U}]\)内，也可能不在。平均而言，在大量的区间估计观测值中，至少有\(100(1 - \alpha)\%\)包含\(\theta\)。 同等置信区间 对给定的一个\(\alpha(0 &lt; \alpha &lt; 1)\)，对任意的\(\theta \in \Theta\)，有\[P_\theta(\widehat{\theta_L} \leq \theta \leq \widehat{\theta_U}) = 1 - \alpha\] 则称\([\widehat{\theta_L}, \widehat{\theta_U}]\)为\(\theta\)的\(1 - \alpha\)同等置信区间。 （单侧）置信下限 设\(\widehat{\theta_L} = \widehat{\theta_L}(x_1,\cdots, x_n)\)是统计量，对给定的一个\(\alpha(0 &lt; \alpha &lt; 1)\)，对任意的\(\theta \in \Theta\)，有\[P_\theta(\widehat{\theta_L} \leq \theta) \geq 1 - \alpha\] 则称\(\widehat{\theta_L}\)为\(\theta\)的置信水平为\(1-\alpha\)的（单侧）置信下限。假如等号对一切\(\theta \in \Theta\)成立，则称\(\widehat{\theta_L}\)为\(\theta\)的\(1-\alpha\)同等置信下限 （单侧）置信上限 设\(\widehat{\theta_U} = \widehat{\theta_U}(x_1,\cdots, x_n)\)是统计量，对给定的一个\(\alpha(0 &lt; \alpha &lt; 1)\)，对任意的\(\theta \in \Theta\)，有\[P_\theta(\widehat{\theta_U} \geq \theta) \geq 1 - \alpha\] 则称\(\widehat{\theta_U}\)为\(\theta\)的置信水平为\(1-\alpha\)的（单侧）置信上限。假如等号对一切\(\theta \in \Theta\)成立，则称\(\widehat{\theta_U}\)为\(\theta\)的\(1-\alpha\)同等置信上限。 枢轴量法 构造未知参数\(\theta\)的置信区间的最常用方法是枢轴量法，其步骤可以概括为如下三步： 设法构造一个样本和\(\theta\)的函数\(G = G(x_1,\cdots,x_n, \theta)\)，使得\(G\)的分布不依赖于未知参数。一般称具有这种性质的\(G\)为枢轴量。 \(G\)的分布不依赖于未知参数，可以理解为G所服从的分布的分布函数或密度函数的表达式是已知的，举个例子，如\(G\)服从标准正态分布，即\(G \sim N(0, 1)\)，其\(\mu = 0\)，\(\sigma^2 = 1\)都是已知的。 适当地选择两个常数\(c\)、\(d\)，使对给定的\(\alpha(0 &lt; \alpha &lt; 1)\)，有\[P(c \leq G \leq d) = 1 - \alpha\] 假如能将\(c \leq G \leq d\)进行不等式等价变形化为\(\widehat{\theta_L} \leq \theta \leq \widehat{\theta_U}\)，则有\[P_\theta(\widehat{\theta_L} \leq \theta \leq \widehat{\theta_U}) = 1 - \alpha\] 这表明\([\widehat{\theta_L}, \widehat{\theta_U}]\)是\(\theta\)的\(1 - \alpha\)同等置信区间。 注：枢轴量的寻找一般从\(\theta\)的点估计出发。满足\(P(c \leq G \leq d) = 1 - \alpha\)的\(c\)、\(d\)可以有很多，假如可以找到使\(E_\theta(\widehat{\theta_U} - \widehat{\theta_L})\)最短当然是最好，不过在不少场合很难做到这一点。故常这样选择\(c\)和\(d\)，使得\[P_\theta(G&lt; c) = P_\theta(G &gt; d) = \alpha/2\] 这样的置信区间称为等尾置信区间。 单个正态总体参数的置信区间 设\(x_1,\cdots,x_n\)是来自总体\(X\)的样本，\(X \sim N(\mu, \sigma^2)\)，\(\overline{x}\)为样本均值，\(s^2\)为样本方差。 参数估计 条件 枢轴量 \(1 - \alpha\)置信区间 \(\mu\) \(\sigma\)已知 \(\frac{\overline{x} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1)\) \([\overline{x} - u_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}, \quad \overline{x} + u_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}]\) \(\mu\) $ $未知 \(\frac{\overline{x} - \mu}{s / \sqrt{n}} \sim t(n - 1)\) \([\overline{x} - t_{1 - \frac{\alpha}{2}}(n-1)\frac{s}{\sqrt n}, \quad \overline{x} + t_{1 - \frac{\alpha}{2}}(n-1)\frac{s}{\sqrt n}]\) \(\sigma^2\) \(\mu\)未知 \(\frac{(n - 1)s^2}{\sigma^2} \sim \chi^2(n - 1)\) \([\frac{(n - 1)s^2}{\chi_{1-\frac{\alpha}{2}}^2(n - 1)}, \quad \frac{(n - 1)s^2}{\chi_{\frac{\alpha}{2}}^2(n - 1)}]\) 两个正态总体下的置信区间 设\(x_1,\cdots,x_m\)是来自\(N(\mu_1, \sigma_1^2)\)的样本，\(y_1,\cdots,y_n\)是来自\(N(\mu_2, \sigma_2^2)\)的样本，且两个样本相互独立。\(\overline{x}\)和\(\overline{y}\)分别是它们的样本均值，\(s_x^2 = \frac{1}{m-1}\sum_{i=1}^m(x_i - \overline{x})^2\)和\(s_y^2 = \frac{1}{n-1}\sum_{i=1}^n(y_i - \overline{y})^2\)分别是它们的样本方差。 参数估计 条件 枢轴量 \(1-\alpha\)置信区间 \(\mu_1 - \mu_2\) \(\sigma_1^2\)和\(\sigma_2^2\)已知 \(\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}} \sim N(0, 1)\) \([\overline{x} - \overline{y} - u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}, \quad \overline{x} - \overline{y} + u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}]\) \(\mu_1 - \mu_2\) \(\sigma_1^2 = \sigma_2^2 = \sigma^2\)未知 \(\sqrt{\frac{mn(m + n - 2)}{m + n}}\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{(m - 1)s_x^2 + (n - 1)s_y^2}} \\ \sim t(m + n - 2)\) 记\(s_w^2 = \frac{(m-1)s_x^2 + (n - 1)s_y^2}{m+n-2}\)\([\overline{x} - \overline{y} - \sqrt{\frac{m+n}{mn}}s_wt_{1-\frac{\alpha}{2}}(m+n-2), \\ \overline{x} - \overline{y} + \sqrt{\frac{m+n}{mn}}s_wt_{1-\frac{\alpha}{2}}(m+n-2)]\) \(\mu_1 - \mu_2\) \(\frac{\sigma_2^2}{\sigma_1^2} = \theta\)已知 \(\sqrt{\frac{mn(m+n-2)}{m\theta + n}}\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{(m - 1)s_x^2 + (n - 1)\frac{s_y^2}{\theta}}} \\ \sim t(m + n - 2)\) 记\(s_t^2 = \frac{(m-1)s_x^2 + (n-1)\frac{s_y^2}{\theta}}{m+n-2}\)\([\overline{x} - \overline{y} - \sqrt{\frac{m\theta + n}{mn}}s_tt_{1 - \frac{\alpha}{2}}(m+n-2), \\ \overline{x} - \overline{y} + \sqrt{\frac{m\theta + n}{mn}}s_tt_{1 - \frac{\alpha}{2}}(m+n-2)]\) \(\frac{\sigma_1^2}{\sigma_2^2}\) \(\frac{s_x^2/\sigma_1^2}{s_y^2/\sigma_2^2} \sim F(m-1,n-1)\) \([\frac{s_x^2}{s_y^2}\frac{1}{F_{1-\frac{\alpha}{2}}(m-1,n-1)}, \quad \frac{s_x^2}{s_y^2}\frac{1}{F_{\frac{\alpha}{2}}(m-1,n-1)}]\)]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
        <tag>统计推断</tag>
        <tag>点估计</tag>
        <tag>区间估计</tag>
        <tag>置信水平</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据透视表]]></title>
    <url>%2FExcel%2F%E6%95%B0%E6%8D%AE%E9%80%8F%E8%A7%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[数据透视表 这里将通过OLEDB接口使用SQL语句合并多文件的数据源，然后使用合并后的数据创建多个数据透视表，并在某些透视表中增加计算项和条件格式处理，最后添加切片器控制多个透视表的图表。 最终效果如下： 数据文件 这里使用的数据是虚构的某家公司的销售数据，一共有6个数据文件，分别是："常熟.xlsx"、"昆山.xlsx"、"南京.xlsx"、"苏州.xlsx"、"无锡.xlsx"、"销售计划.xlsx"。其中，前5个数据文件保存的是每个城市的实际销售数据，其数据结构为： "销售计划.xlsx"保存的是各个城市的计划销售金额，其数据结构为： 现假设将它们放在电脑D盘的"D:_table"目录下，并在"D:_table"目录下创建文件"数据透视表.xlsx"，用于合并数据源及处理本文的所有数据透视表内容。 切片器控制多个图表 这里将通过OLEDB接口使用SQL语句将多个数据文件的数据合并到文件"数据透视表.xlsx"的工作表中，然后再使用合并的数据创建数据透视图及透视图相应的图表，最后再添加切片器控制多图表联动。当数据源的文件内容有更新时，可以在"数据透视表.xlsx"文件中进行“刷新”，使得相应的数据透视图数据和图表数据也进行更新。 合并数据源 打开文件"数据透视表.xlsx"，选择菜单栏中的“数据”菜单项，点击该菜单项中的“现有连接”按钮，弹出“现有连接”对话框，再点击“浏览更多”按钮，弹出选择文件对话框，然后找到"数据透视表.xlsx"文件所在位置，即"D:_table"。 选择"数据透视表.xlsx"文件并打开，弹出“选择表格”对话框，勾选“数据首行包含列标签”，然后点击确定。 点击确定后弹出“导入数据”对话框，在该对话框中，将数据在工作簿的显示方式设置为“表”，数据的放置位置设置为“新工作表”，然后点击属性按钮。 在弹出的连接属性对话框中，点击“定义”选项卡，在该选项卡中的“命令文本”框中输入以下SQL语句： 1SELECT "苏州" AS 所属区域, * FROM [D:\povit_table\data\苏州.xlsx].[销售数据$] UNION ALL SELECT "南京" AS 所属区域, * FROM [D:\povit_table\data\南京.xlsx].[销售数据$] UNION ALL SELECT "无锡" AS 所属区域, * FROM [D:\povit_table\data\无锡.xlsx].[销售数据$] UNION ALL SELECT "常熟" AS 所属区域, * FROM [D:\povit_table\data\常熟.xlsx].[销售数据$] UNION ALL SELECT "昆山" AS 所属区域, * FROM [D:\povit_table\data\昆山.xlsx].[销售数据$] 该SQL语句的作用是将不同数据源文件的销售数据合并到同一张表中，并添加“所属区域”字段用于区分数据来源。输入完SQL语句后，点击确定，回到“导入数据”对话框中，再次点击确定，即可将数据导入到"数据透视表.xlsx"文件中。 将导入在"数据透视表.xlsx"文件中数据所在的工作区命名为“销售数据源”，接下来将使用该数据表进行数据透视表的创建。 创建数据透视表 这里将使用“销售数据源”工作区中数据创建4张数据透视表，每张数据透视表使用一个新的工作区，并为每张透视表数据创建图形。 按年、季度查看各个销售部门的销售金额 将订购日期按年、季度进行组合，并创建各个销售部门的销售金额随时间变化的趋势图。 查看各所属区域的销售金额和成本的对比 查看各产品类别的销售金额 查看各销售部门的销售金额 创建切片器 新建一个名为“切片器演示”的工作区，将上面所有数据透视表的图形都拷贝到该工作区，然后再任意一个数据透视表中插入三个切片器：年份、销售人员、产品类别。 将这三个切片器剪切到“切片器演示”工作区中后，分别选中每个切片器，单击右键，选择“报表连接”选项，然后勾选中上面4个透视表进行连接，最后点击确定即可将切片器与多个数据透视表和透视表图形联动起来。 最后效果如下： 条件格式 合并数据源 这里合并数据源的操作与切片器控制多个图表部分合并数据源的操作几乎是一样的操作，唯一的区别在于所获取数据的SQL语句不同。这里所使用的SQL语句如下： 1SELECT "实际" AS 类型, "苏州" AS 所属区域, * FROM [D:\povit_table\data\苏州.xlsx].[销售数据$] UNION ALL SELECT "实际" AS 类型, "南京" AS 所属区域, * FROM [D:\povit_table\data\南京.xlsx].[销售数据$] UNION ALL SELECT "实际" AS 类型, "无锡" AS 所属区域, * FROM [D:\povit_table\data\无锡.xlsx].[销售数据$] UNION ALL SELECT "实际" AS 类型, "常熟" AS 所属区域, * FROM [D:\povit_table\data\常熟.xlsx].[销售数据$] UNION ALL SELECT "实际" AS 类型, "昆山" AS 所属区域, * FROM [D:\povit_table\data\昆山.xlsx].[销售数据$] UNION ALL SELECT "计划" AS 类型, 所属区域, NULL, NULL, NULL, 销售部门, NULL, NULL, NULL, 产品类别, NULL, 金额, NULL FROM [D:\povit_table\data\销售计划.xlsx].[数据$] 该SQL语句的作用是将不同数据源文件的销售数据及销售计划数据合并到同一张表中，并添“类型”字段区分实际完成和计划完成，添加“所属区域”字段区分数据来源。执行SQL语句得到数据如下： 将该数据表所在的工作区命名为“业绩数据源”，接下来将使用该数据表进行数据透视表的创建。 创建数据透视表 这部分要创建一张关于所属区域的各个销售部门的实际完成销售金额和计划完成销售金额的对比。因此该数据透视表需要使用的字段有：所属区域、产品类别、类型，其中类型字段中包含“实际”和“计划”两个项。既然要对比，所以还需在类型字段中增加表示差额的一项。因此，可以让该数据透视表使用类型和产品类别作为行，所属区域作为列。 可以看到，对该数据透视表进行列总计是没有意义的，所以需要将其去掉。接下来将在要在类型字段中插入计算项。 计算项 选中类型字段，点击菜单栏中的“数据透视表分析”选项卡，再点击“字段、项目和集”下拉菜单，点击“计算项”，弹出计算项对话框。 在计算项对话框中，名称文本框中填入“差额”，公式文本框中填入“实际 - 计划”公式，点击“添加”按钮，再点击确定。 可以看到，类型字段中增加了差额一项。有了差额项之后，可以将计划一项的明细合并起来。为了使数据透视表表现的更有张力，接下来将对实际和差额两项所对应的销售金额进行条件格式处理。 实际 因为实际项部分的销售金额体现的是数量的概念，所以使用条件格式中的数据条进行填充显示。 差额 因为差额项部分的销售金额体现的是目标是否达成的概念，所以是用条件格式中图表集的三色交通灯表示。 如果使用三色交通灯的本意是，“绿灯”表示超额完成，“黄灯”表示刚好完成，“红灯”表示未完成。但上图中的有些未完成计划的数据也显示了“绿灯”，所以这时需要修改差额项销售金额的条件格式的管理规则。 编辑规则，使得规则符合三色交通灯的本意。 同样，对差额汇总也进行相同的处理。 创建切片器 如果需要查看各个销售部门的完成情况，可以在数据透视表中插入“销售部门”切片器。 最后效果如下：]]></content>
      <categories>
        <category>Excel</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用概率分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[常用概率分布 概览 分布 分布列\(p_k\)或分布密度\(p(x)\) 数学期望 方差 二项分布\(b(n, p)\) \(P(X = k) = {n \choose k}p^k(1- p)^{n - k}, \\ k = 0,1,\cdots,n.\) \(np\) \(np(1-p)\) 泊松分布\(P(\lambda)\) \(P(X = k) = \frac{\lambda ^ k}{k!}e^{-\lambda} \\ k=0,1,\cdots.\) \(\lambda\) \(\lambda\) 超几何分布\(h(n, N, M)\) \(P(X = k) = \frac{\\{M \choose k\\}{N-M \choose n-k}}{N \choose n} \\ k = 0,1\cdots,r; r = min\{M, n\}\) \(n\frac{M}{N}\) \(\frac{nM(N - M)(N - n)}{N ^ 2(N - 1)}\) 负二项分布\(Nb(r, p)\) \(P(X = k) = {k - 1 \choose r - 1}p^r(1-p)^{k - r}, \\ k = r, r + 1, \cdots.\) \(\frac{r}{p}\) \(\frac{r(1 - p)}{p^2}\) 几何分布\(Ge(p)\) \(P(X = k) = (1 - p) ^ {k - 1}p, \\ k = 1, 2, \cdots.\) \(\frac{1}{p}\) \(\frac{(1 - p)}{p^2}\) 正态分布\(N(\mu, \sigma^2)\) \(p(x) = \frac{1}{\sqrt{2\pi}\sigma}e^-\frac{(x - \mu)^2}{2\sigma^2}, \\ -\infty &lt; x &lt; +\infty\) \(\mu\) \(\sigma^2\) 均匀分布\(U(a, b)\) \(p(x) = \frac{1}{b - a}, \\ a &lt; x &lt; b\) \(\frac{a + b}{2}\) \(\frac{(b - a)^2}{12}\) 指数分布\(Exp(\lambda)\) \(p(x) = \lambda e^{-\lambda x}, \\ x \geq 0\) \(\frac{1}{\lambda}\) \(\frac{1}{\lambda^2}\) 伽马分布\(Ga(\alpha, \lambda)\) \(p(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}, \\ x \geq 0\) \(\frac{\alpha}{\lambda}\) \(\frac{\alpha}{\lambda ^ 2}\) 贝塔分布\(Be(a,b)\) \(p(x) = \frac{1}{B(a, b)}x^{a - 1}(1 - x)^{b - 1}, \\ 0 &lt; x &lt; 1\) \(\frac{a}{a + b}\) \(\frac{ab}{(a + b)^2(a + b + 1)}\) 常用离散分布 二项分布 定义 如果在\(n\)重独立重复试验中，每次试验的结果为两个，则称这种试验为\(n\)重伯努利试验。 如果记\(X\)为\(n\)重伯努利重复试验中成功的次数（记为事件\(A\)），则\(X\)的可能取值为\(0,1,...,n\)。记\(p\)为每次试验中\(A\)发生的概率，即\(P(A) = p\)，则\(P(\overline A) = 1- p\)，则事件\(\{ X = k \}\)的分布列为： \[P(X = k) = {n \choose k}p^k(1 - p)^{n - k}, k = 1,\ldots,n.\]称为二项分布，记为\(X \sim b(n, p)\)。 数学期望与方差 设随机变量\(X \sim b(n, p)​\)，则\(X​\)的数学期望\(E(X) = np​\)，\(X​\)的方差\(Var(X) = n(n - 1)p^2 + np​\)。 概率计算 例：某特效药的临床有效率为0.95，今有10人服用，问至少有8人治愈的概率是多少？ 解：设\(X\)为10人中被治愈的人数，则\(X \sim b(10, 0.95)​\)，所以 \[ \begin{align*} P(X \geq 8) &amp;= P(X = 8) + P(X = 9) + P(X = 10) \\ &amp;= {10 \choose 8}0.95^80.05^2 + {10 \choose 9}0.95^90.05 + {10 \choose 10}0.95^{10}\\ &amp;= 0.0746 + 0.3151 + 0.5987\\ &amp;= 0.9885 \end{align*} \] Excel函数 在Excel中，提供BINOMDIST函数用于二项分布概率计算。 \[ \text{BINOMDIST(number_s, trials, probability_s, cumulative)} \] 其中各参数含义如下: 参数 含义 number_s 试验发生的次数，即\(k\) trials 独立试验的总次数，即\(n\) probability_s 试验发生的概率\(p\) cummulative 逻辑值，若为TURE，表示累积概率\(P(X \leq x)\)；否则为FALSE，表示概率函数值\(P(X = x)​\) 函数参数cummulative为FALSE \(X\) BINOMDIST函数 \(P(X)\) 8 BINOMDIST(8, 10, 0.95, FALSE) 0.0746 9 BINOMDIST(9, 10, 0.95, FALSE) 0.3151 10 BINOMDIST(10, 10, 0.95, FALSE) 0.5987 则\(P(X \geq 8) = 0.0746 + 0.3151 + 0.5987 = 0.9885​\)。 函数参数cummulative为TRUE 用函数BINOMDIST(7, 10, 0.95, TRUE)求得\(P(X \leq 7)\)的概率为0.0115，则 \(P(X \geq 8) = 1 - P(X \leq 7) = 1 - 0.0115 = 0.9885​\) 泊松分布 定义 \[ P(X = k) = \frac{\lambda ^ k}{k!}e^{-\lambda}​\] 其中\(\lambda &gt; 0\)，记为\(X \sim P(\lambda)\)。泊松分布常与单位时间（或单位面积、单位产品等）上的计数过程相联系，如： 在单位时间内，电话总机接到用户呼唤的次数； 一件铸件上的砂眼数； 1平方米内，玻璃上的气泡数 数学期望和方差 设随机变量\(X \sim P(\lambda)​\)，则\(X​\)的数学期望\(E(X) =\lambda​\)，\(X​\)的方差\(Var(X) = \lambda​\) 概率计算 例：一铸件的砂眼（缺陷）数服从参数为\(\lambda = 0.5\)的泊松分布，试求此铸件上至多有1个砂眼（合格品）的概率？ 解：以\(X\)表示这种铸件的砂眼数，则\(X \sim P(0.5)\)，所以所求概率为 \[ \begin{align*} P(X\leq1) &amp;= P(X = 0) + P(X = 1)\\ &amp;= \frac{0.5 ^ 0}{0!}e^{-0.5} + \frac{0.5 ^ 1}{1!}e^{-0.5}\\ &amp;= 0.6065 + 0.3033\\ &amp;= 0.9098 \end{align*} \] Excel函数 在Excel中，提供了POISSON函数用于计算泊松分布概率值。 \[ \text{POISSON(x, mean, cumulative)} ​\] 其中各参数含义如下: 参数 含义 x 事件数，即定义中的\(k\) mean 期望值，即\(\lambda\) cummulative 逻辑值，若为TURE，表示累积概率\(P(X \leq x)\)；否则为FALSE，表示概率函数值\(P(X = x)​\) 函数参数cummulative为FALSE \(X\) POISSON函数 \(P(X)\) 0 POISSON(0, 0.5, FALSE) 0.6065 1 POISSON(1, 0.5, FALSE) 0.3033 则\(P(X \leq 1) = 0.6065 + 0.3033 = 0.9098\)。 函数参数cummulative为TRUE 用函数POISSON(1, 0.5, TRUE)求得\(P(X \leq 1)\)的概率为0.9098。 超几何分布 定义 设有\(N​\)个产品，其中有\(M​\)个不合格品。若从中不放回地随机抽取n个，则其中含有的不合格品的个数\(X​\)服从超几何分布，记为\(X \sim h(n, N, M)​\)，其分布列为\(P(X = k) = \frac{\\{M \choose k\\}{N-M \choose n-k}}{N \choose n}, k = 0,1\cdots,r.​\) 其中\(r = min\{M, n\}​\)且\(M \leq N, n \leq N​\)，\(n​\)，\(N​\)，\(M​\)均为正整数。 数学期望和方差 设随机变量\(X \sim h(n, N, M)​\)，则\(X​\)的数学期望\(E(X) = n\frac{M}{N}​\)，\(X​\)的方差\(Var(X) = \frac{nM(N - M)(N - n)}{N ^ 2(N - 1)}​\)。 概率计算 例：有20块巧克力，其中有8块是焦糖的，其余12块是果仁的。如果随机选出4块，求恰好有2块是焦糖的概率是多少？ 解：以\(X​\)表示恰好为焦糖的块数，则\(X \sim h(4, 20, 8)​\)，所以所求概率为： \[P(X = 2) = \frac{\\{8 \choose 2\\}{20 - 8 \choose 4 - 2}}{20 \choose 4} = 0.3814\] Excel函数 在Excel中，提供了HYPGEOMDIST函数用于计算泊松分布概率值。 \[ \text{HYPGEOMDIST(sample_s, number_sample, population_s, number_population)} ​\] 其中各参数含义如下: 参数 含义 sample_s 样本中成功的次数，即\(k\) number_sample 抽取的样本容量，即\(n\) population_s 总体中成功的次数，即\(M\) number_population 样本总体的容量，即\(N\) 使用函数HYPGEOMDIST(2, 4, 8, 20) = 0.3814。 负二项分布 定义 在伯努利试验序列中，记每次试验中事件\(A​\)发生的概率为\(p​\)，如果\(X​\)为事件\(A​\)的第\(r​\)次出现时的试验次数，则称\(X​\)服从负二项分布，记为\(X \sim Nb(r, p)​\)，其分布列为：\[P(X = k) = {k - 1 \choose r - 1}p^r(1-p)^{k - r}, k = r, r + 1, \cdots.​\] 数学期望和方差 设随机变量\(X \sim Nb(r, p)​\)，则\(X​\)的数学期望\(E(X) = \frac{r}{p}​\)，\(X​\)的方差\(Var(X) = \frac{r(1 - p)}{p^2}​\)。 概率计算 例：在独立重复试验中，每次成功的概率为0.3，在5次成功之前，失败15次的概率为NEGBINOMDIST(15, 5, 0.3) = 0.0447。 Excel函数 在Excel中，提供了NEGBINOMDIST函数用于计算泊松分布概率值。 \[ \text{NEGBINOMDIST(number_f, number_s, probability_s)} \] 其中各参数含义如下: 参数 含义 number_f \(A\)不发生的次数，即\(n-r\) number_s \(A\)发生的次数，即\(r\) probability_s 每次试验中\(A\)发生的概率，即\(p\) 几何分布 定义 在伯努利试验序列中，记每次试验中事件\(A​\)的发生概率为\(p​\)，如果\(X​\)为事件\(A​\)首次出现时的试验次数，则称\(X​\)服从几何分布，记为\(X \sim Ge(p)​\)，其分布列为：\[P(X = k) = (1 - p) ^ {k - 1}p, k = 1, 2, \cdots.​\] 由负二项分布\(P(X = k) = {k - 1 \choose r - 1}p^r(1-p)^{k - r}​\)可知，当​\(r = 1​\)时，负二项分布即为几何分布。实际中有不少随机变量服从几何分布，譬如： 某产品的不合格率为0.05，则首次查到不合格品的检查次数\(X \sim Ge(0.05)​\)； 某射手的命中率为0.8，则首次击中目标的射击次数\(Y\sim Ge(0.8)​\). 数学期望和方差 设随机变量\(X \sim Ge(p)​\)，则\(X​\)的数学期望\(E(X) = \frac{1}{p}​\)，\(X​\)的方差\(Var(X) = \frac{(1 - p)}{p^2}​\)。 概率计算 Excel函数 因为当负二项分布在\(r = 1​\)时，即为几何分布，所以也可以使用NEGBINOMDIST函数计算几何分布的概率，即NEGBINOMDIST(15, 1, 0.3) = 0.0014。 常用连续分布 正态分布 定义 若随机变量\(X​\)的密度函数为\[p(x) = \frac{1}{\sqrt{2\pi}\sigma}e^-\frac{(x - \mu)^2}{2\sigma^2},-\infty &lt; x &lt; +\infty​\]，则称\(X​\)服从正态分布（也称高斯分布），称\(X​\)为正态变量，记作\(X\sim N(\mu, \sigma^2)​\)，其中参数\(-\infty &lt; \mu &lt; +\infty​\)，\(\sigma &gt; 0​\)。 正态分布\(N(\mu, \sigma^2)​\)的分布函数为\[F(x) = \frac{1}{\sqrt{2\pi} \sigma} \int_{-\infty} ^ x e ^ -\frac{(t - \mu) ^ 2}{2 \sigma ^ 2}dt,-\infty &lt; x &lt; +\infty​\] 当\(\mu = 0\)，\(\sigma = 1\)时，称正态分布为标准正态分布。通常记标准正态变量为\(U\)，记标准正态分布的密度函数为\(\varphi(u)\)，分布函数为\(\Phi(u)\)，即 \[\varphi(u) = \frac{1}{\sqrt{2\pi}}e^-\frac{u^2}{2},-\infty &lt; u &lt; +\infty\] \[\Phi(u) = \frac{1}{\sqrt{2\pi}} \int_{-\infty} ^ u e ^ -\frac{t ^ 2}{2}dt,-\infty &lt; u &lt; +\infty​\] 标准正态分布常用等式 \(\Phi(-u) = 1 - \Phi(u)\) \(P(U &gt; u) = 1 - \Phi(u)\) \(P(a &lt; U &lt; b) = \Phi(b) - \Phi(a)\) \(P(|U| &lt; c) = 2\Phi(c) - 1​\) 一般正态分布标准化 若\(X \sim N(\mu, \sigma^2)\)，则\(U = \frac{U - \mu}{\sigma} \sim N(0, 1)\)。这说明对于一般正态分布都可以通过一个线性变换（标注化）化成标准正态分布，则 \[P(X \leq c) = \Phi(\frac{c - \mu}{\sigma})\] \[P(a&lt;X \leq b) = \Phi(\frac{b - \mu}{\sigma}) - \Phi(\frac{a - \mu}{\sigma})\] 正态分布的3\(\sigma​\)原则 设\(X \sim N(\mu, \sigma^2)\)，则 \[ P(|X - \mu| &lt; k\sigma ) = \Phi(k) - \Phi(-k) = \left\{ \begin{array}{ll} 0.6826,k = 1;\\ 0.9545,k = 2;\\ 0.9973,k=3. \end{array} \right. \] 从上式可以看出：尽管正态变量的取值范围是\((-\infty,+\infty)\)，但它的99.73%的值落在\((\mu - 3\sigma, \mu + \sigma)\)内。 数学期望和方差 设随机变量\(X \sim N(\mu, \sigma)​\)，则\(X​\)的数学期望\(E(X) = \mu​\)，\(X​\)的方差\(Var(X) = \sigma ^ 2​\)。 概率计算 例：设随机变量\(X \sim N(108, 3^2)\)，求\(P(102 &lt; X &lt; 117)\) 解： \[ \begin{align*} P(102 &lt; X &lt; 117) &amp;= P(X \leq 117) - P(X \leq 102)\\ &amp;= \Phi(\frac{117 - 108}{3}) - \Phi(\frac{102 - 108}{3})\\ &amp;= \Phi(3) - \Phi(-2) = \Phi(3) + \Phi(2) - 1\\ &amp;= 0.9987 + 0.9772 - 1 = 0.9759 \end{align*} \] Excel函数 在Excel中，使用NORMDIST函数用于一般正态分布的计算，使用NORMSDIST函数进行标准正态分布的计算。 NORMDIST函数 $ $ 其中各参数含义如下: 参数 含义 x 需要计算其分布的数值 mean 期望值\(\mu\) standard_dev 标准差\(\sigma\) cummulative 逻辑值，若为TURE，表示累积概率\(P(X \leq x)\)；否则为FALSE，表示概率函数值\(P(X = x)​\) 因此\(P(X \leq 102)​\) = NORMDIST(102,108,3,TRUE)} = 0.0228，\(P(X \leq 117)​\) = NORMDIST(117,108,3,TRUE) = 0.9987，则\(P(102 &lt; X &lt; 117)​\) = 0.9987 - 0.0228​ = 0.9759。 NORMSDIST函数 $ ​$ 参数 含义 z 需要计算其分布的数值 因此$ (-2) $ = NORMSDIST(-2) = 0.0228，$ (3)$ = NORMSDIST(3) = 0.9987，则\(P(102 &lt; X &lt; 117)\) = 0.9987 - 0.0228 = 0.9759。 均匀分布 定义 若随机变量\(X\)的密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \frac{1}{b - a},a &lt; x &lt; b;\\ 0,其它. \end{array} \right. \] 则称\(X​\)服从区间\((a, b)​\)上的均匀分布，记作\(X \sim U(a, b)​\)，其分布函数为 \[ F(x) = \left\{ \begin{array}{ll} 0, x &lt; a;\\ \frac{x - a}{b - a},a \leq x &lt; b;\\ 1,x \geq b. \end{array} \right. \] 均匀分布的背景可视作随机点\(X\)落在区间\((a, b)\)上的位置。 数学期望和方差 设随机变量\(X \sim U(a, b)\)，则\(X\)的数学期望\(E(X) = \frac{a + b}{2}\)，\(X\)的方差\(Var(X) = \frac{(b - a)^2}{12}\) 概率计算 例：设随机变量\(X \sim U(0, 10)\)，现对\(X\)进行4次独立观测，求至少有3次观测值大于5的概率。 解：设随机变量\(Y\)是4次独立观测值大于5的次数，则\(Y \sim b(4, p)\)，其中\(p = P(X &gt; 5)\)。由\(X \sim U(0, 10)\)，知\(X\)的密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \frac{1}{10},0 &lt; x &lt; 10;\\ 0,其它. \end{array} \right. \] 所以\[p = P(X &gt; 5) = \int_5 ^ {10} \frac{1}{10} dx = \frac{1}{2}\]，于是 \[ \begin{align*} P(Y \geq 3) &amp;= {4 \choose 3}p^3(1-p) + {4 \choose 4}p^4\\ &amp;=4(\frac{1}{2})^4 + (\frac{1}{2})^4 = \frac{5}{16} \end{align*} \] 指数分布 定义 若随机变量\(X​\)的密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \lambda e^{-\lambda x}, x \geq 0;\\ 0,x &lt; 0. \end{array} \right. \] 则称\(X\)服从指数分布，记作\(X \sim Exp(\lambda)\)，其中参数\(\lambda &gt; 0\)。指数分布的分布函数为 \[ F(x) = \left\{ \begin{array}{ll} 1 - e^{-\lambda x}, x \geq 0;\\ 0,x &lt; 0. \end{array} \right. \] 因为指数分布随机变量只可能取非负实数，所以指数分布常被用作各种"寿命"分布，譬如电子元件寿命、电话的通话时间等。 数学期望和方差 设随机变量\(X \sim Exp(\lambda)\)，则\(X\)的数学期望\(E(X) = \frac{1}{\lambda}\)，\(X\)的方差\(Var(X) = \frac{1}{\lambda ^ 2}​\) 概率计算 Excel函数 在Excel中，使用EXPONDIST函数用于指数分布计算。 \[\text{EXPONDIST(x, lambda, cumulative)}​\] 其中各参数含义如下: 参数 含义 x 需要计算其分布的数值 lambda 指数分布参数值$ $ cummulative 逻辑值，若为TURE，表示分布函数值； 否则为FALSE，表示密度函数值 EXPONDIST(0.2, 10, FALSE) = 1.3534，EXPONDIST(0.2, 10, TRUE) = 0.8647。 伽马分布 定义 伽马函数 称以下函数 \[ \Gamma(\alpha) = \int_0^{+\infty} x^{\alpha - 1}e^{-x} dx \] 为伽马函数，其中\(\alpha &gt; 0\)。伽马函数具有如下性质： \(\Gamma(1) = 1,\Gamma(\frac{1}{2}) = \sqrt\pi;\) \(\Gamma(\alpha + 1) = \alpha\Gamma(\alpha)\). 伽马分布 若随机变量\(X\)的密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},x \geq 0;\\ 0,x &lt; 0. \end{array} \right. \] 则称\(X\)服从伽马分布，记作\(X \sim Ga(\alpha, \lambda)\)，其中\(\alpha &gt; 0, \lambda &gt; 0\)。伽马分布的分布函数为 \[ F(x) = \left\{ \begin{array}{ll} \frac{\lambda^\alpha}{\Gamma(\alpha)}\int_0^x t^{\alpha-1}e^{-\lambda t}dt,x \geq 0;\\ 0,x &lt; 0. \end{array} \right. \] 伽马分布的两个特例 \(\alpha = 1\)时的伽马分布就是指数分布，即\(Ga(1, \lambda) = Exp(\lambda)\). 若\(\alpha = n/2, \lambda = 1/2​\)时，伽马分布可称为自由度为\(n​\)的\(\chi ^ 2​\)（卡方）分布，记为\(\chi ^ 2(n)​\)，即\[Ge(\frac{n}{2}, \frac{1}{2}) = \chi^2(n)​\] 其密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}e^{-\frac{x}{2}}x^{\frac{n}{2} - 1}, x&gt;0;\\ 0,x \leq 0. \end{array} \right. \] 数学期望和方差 设随机变量\(X \sim Ga(\alpha, \lambda)​\)，则\(X​\)的数学期望\(E(X) = \frac{\alpha}{\lambda}​\)，\(X​\)的方差\(Var(X) = \frac{\alpha}{\lambda ^ 2}​\)。 当\(\alpha = n/2, \lambda = 1/2\)时，卡方分布的数学期望\(E(X) = n\)，方差\(Var(X) = 2n\) 概率计算 Excel函数 在Excel中，使用GAMMADIST函数用于伽马分布的计算。 \[\text{GAMMADIST(x, alpha, beta, cumulative)}\] 其中各参数含义如下: 参数 含义 x 用来进行函数计算的值 alpha 分布参数$ $ beta 分布参数$ $ cummulative 逻辑值，若为TURE，则返回分布函数值； 否则为FALSE，表示密度函数值 GAMMADIST(31.41, 10, 2, TRUE)=0.9500，即自由度为20的卡方分布在31.41点处的分布函数值为0.9500。 GAMMADIST(20, 10, 2, FALSE)=0.0626，即自由度为20的卡方分布在20点处的密度函数值为0.0626。 贝塔分布 定义 贝塔函数 称以下函数 \[ B(a, b) = \int_0^1 x^{a - 1}(1 - x)^{b - 1}dx \] 为贝塔函数，其中参数\(a &gt; 0, b &gt; 0\)。贝塔函数具有如下性质： \(B(a, b) = B(b, a)\) 贝塔函数与伽马函数间有关系 \[B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}​\] 贝塔分布 若随机变量\(X\)的密度函数为 \[ p(x) = \left\{ \begin{array}{ll} \frac{1}{B(a, b)}x^{a - 1}(1 - x)^{b - 1}, 0 &lt; x &lt; 1;\\ 0,其它. \end{array} \right. \] 则称\(X\)服从贝塔分布，记作\(X \sim Be(a, b)\)，其中\(a &gt; 0, b &gt; 0\)。因为服从贝塔分布\(Be(a, b)\)的随机变量是仅在区间\((0, 1)\)取值的，所以可以选用贝塔分布作为不合格品率，机器维修率、市场占有率等各种比率的概率分布是恰当的。贝塔分布的分布函数为 \[ F(x) = \left\{ \begin{array}{ll} \frac{1}{B(a, b)}\int_0^x t^{a - 1}(1 - t)^{b - 1}dt, 0 &lt; x &lt; 1;\\ 0,其它. \end{array} \right. \] 由于\(B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}​\)，因此，贝塔分布的密度函数也可以表示为 \[ p(x) = \left\{ \begin{array}{ll} \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}x^{a - 1}(1 - x)^{b - 1}, 0 &lt; x &lt; 1;\\ 0,其它. \end{array} \right. \] 则贝塔分布的分布函数也可以表示为 \[ F(x) = \left\{ \begin{array}{ll} \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_0^x t^{a - 1}(1 - t)^{b - 1}dt, 0 &lt; x &lt; 1;\\ 0,其它. \end{array} \right. \] 数学期望和方差 设随机变量\(X \sim Be(a, b)​\)，则\(X​\)的数学期望\(E(X) = \frac{a}{a + b}​\)，\(X​\)的方差\(Var(X) = \frac{ab}{(a + b)^2(a + b + 1)}​\)。 概率计算 Excel函数 在Excel中，使用BETADIST函数用于贝塔分布的计算。 \[\text{BETADIST(x, alpha, beta, A, B)}​\] 其中各参数含义如下: 参数 含义 x 用来进行函数计算的值 alpha 分布参数a beta 分布参数b A 可选，数值\(x\)取值范围的下界，标准贝塔分布使用\(A = 0\) B 可选，数值\(x\)取值范围的上界，标准贝塔分布使用\(B = 1\) BETADIST(x, 1, 1, 0, 1)就是均匀分布U(0, 1)的分布函数，BETADIST(0.5, 2, 4) = 0.8125。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验及R实现]]></title>
    <url>%2FR%E8%AF%AD%E8%A8%80%2F%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E5%8F%8Ar%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[假设检验及R实现 主要内容： 已知总体服从正态分布的条件下，单个正态总体和两个正态总体的参数假设检验问题。 比例的检验，大样本检验。 分布拟合检验：Neyman-Pearson卡方检验，Kolmogrov-Smirnov检验（KS检验），Shapiro-Wilk检验（W检验）。 正态总体参数假设检验 单个正态总体的检验 检验对象 条件 假设检验方法 R语言实现 均值\(\mu\) \(\sigma^2\)已知 检验 z.test（程序包BSDA） 均值\(\mu\) \(\sigma^2\)未知 检验 t.test（自带函数） 方差$^2 $ \(\mu\)已知 卡方检验 自定义函数 方差\(\sigma^2\) \(\mu\)未知 卡方检验 自定义函数 场景数据：假设2012年各月北京的新建住宅价格指数样本数据如下： 1月 2月 3月 4月 5月 6月 7月 8月 9月 10月 11月 12月 102.5 102.4 102 101.8 101.8 102.1 102.3 102.5 102.6 102.8 103.4 104.2 均值的检验 方差\(\sigma^2\)已知 若假设上面北京的新建住房价格指数服从正态分布\(N(\mu, 0.45)\)，试问在显著性水平\(\alpha = 0.05\)下，猜测价格指数总体均值为102.4，问是否能拒绝这个猜测？ 解：总体服从\(N(\mu, 0.45)\)，待检验的原假设为：\(\mu = 102.4\)，备择假设为：\(\mu \neq 102.4\)。这是一个双侧检验，已知\(\sigma^2 = 0.45\)，所以使用\(Z\)检验。 自定义函数 1234567891011121314151617181920x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)# 参数x为样本，mu为原假设范围内的值，sigma为标准差# alternative："two.sided"双侧检验，"less"左侧检验，"greater"为右侧检验custom.z.test &lt;- function(x, mu, sigma, alternative = "two.sided")&#123; n &lt;- length(x) # 样本容量 mean &lt;- mean(x) # 样本均值 z &lt;- sqrt(n) * (mean - mu) / sigma # 统计量 ret &lt;- list() # 构造一个空的list，用于存放输出结果 ret$mean &lt;- mean ret$z &lt;- z if (alternative == "two.sided") ret$p &lt;- 2 * pnorm(abs(z), lower.tail = FALSE) else if (alternative == "less") ret$p &lt;- pnorm(z) else if (alternative == "greater") ret$p &lt;- pnorm(z, lower.tail = FALSE) ret&#125;custom.z.test(x, 102.4, sqrt(0.45)) # 调用函数 输出结果： 123456$mean[1] 102.5333$z[1] 0.6885304$p[1] 0.4911189 可以看到显著性水平\(\alpha = 0.05\)小于检验的\(p = 0.4911189\)值，所以在显著性水平\(\alpha = 0.05\)下，不能拒绝原假设。 BSDA包的z.test函数 1234567install.packages("BSDA") # 安装BSDA包，若已安装，则删除此行library(BSDA)x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)# 参数x为样本，mu为原假设范围内的值，sigma为标准差# alternative："two.sided"双侧检验，"less"左侧检验，"greater"为右侧检验z.test(x = x, mu = 102.4, sigma.x = sqrt(0.45), alternative = "two.sided") 输出结果： 123456789 One-sample z-Testdata: xz = 0.68853, p-value = 0.4911alternative hypothesis: true mean is not equal to 102.495 percent confidence interval: 102.1538 102.9129sample estimates:mean of x 102.5333 输出结果显示，统计量z = 0.68853，检验的p值p-value = 0.4911，置信水平为0.95时的置信区间[102.1538, 102.9129]。因为显著性水平\(\alpha = 0.05\)小于检验的\(p=0.4911\)值，所以在显著性水平\(\alpha = 0.05\)下，不能拒绝原假设。 方差\(\sigma^2\)未知 若假设上面北京的新建住房价格指数所服从正态分布的总体期望\(\mu\)未知，总体方差\(\sigma^2\)未知，试问在显著性水平\(\alpha = 0.05\)下，猜测价格指数总体均值为102.4，问是否能拒绝这个猜测？ 解：总体服从\(N(\mu, \sigma^2)\)，待检验的原假设为：\(\mu = 102.4\)，备择假设为：\(\mu \neq 102.4\)。这是一个双侧检验，由于\(\sigma^2\)未知，所以使用\(t\)检验。 自定义函数 123456789101112131415161718192021222324x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)# 参数x为样本，mu为原假设范围内的值，sigma为标准差# alternative："two.sided"双侧检验，"less"左侧检验，"greater"为右侧检验custom.t.test &lt;- function(x, mu, alternative = "two.sided")&#123; n &lt;- length(x) # 样本容量 df &lt;- n - 1 # 自由度 mean &lt;- mean(x) # 样本均值 # sigma &lt;- sqrt(sum((x - mean)^2) / df) # 样本标准差 sigma &lt;- sd(x) # 样本标准差 z &lt;- sqrt(n) * (mean - mu) / sigma # 统计量 ret &lt;- list() # 构造一个空的list，用于存放输出结果 ret$mean &lt;- mean ret$z &lt;- z ret$df &lt;- df if (alternative == "two.sided") ret$p &lt;- 2 * pt(abs(z), df = df, lower.tail = FALSE) else if (alternative == "less") ret$p &lt;- pt(z, df = df) else if (alternative == "greater") ret$p &lt;- p(z, df = df, lower.tail = FALSE) ret&#125;custom.t.test(x = x, mu = 102.4, alternative = "two.sided") # 调用函数 输出结果： 12345678$mean[1] 102.5333$z[1] 0.6700594$df[1] 11$p[1] 0.5166397 可以看到显著性水平\(\alpha = 0.05\)小于检验的\(p = 0.5166397\)值，所以在显著性水平\(\alpha = 0.05\)下，不能拒绝原假设。 t.test函数 12345x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)# 参数x为样本，mu为原假设范围内的值# alternative："two.sided"双侧检验，"less"左侧检验，"greater"为右侧检验t.test(x = x, mu = 102.4, alternative = "two.sided") 输出结果： 123456789 One Sample t-testdata: xt = 0.67006, df = 11, p-value = 0.5166alternative hypothesis: true mean is not equal to 102.495 percent confidence interval: 102.0954 102.9713sample estimates:mean of x 102.5333 输出结果显示，统计量z = 0.67006，检验的p值p-value = 0.5166，置信水平为0.95时的置信区间为[102.0954, 102.9713]，因为显著性水平\(\alpha = 0.05\)小于检验的\(p=0.5166\)值，所以在显著性水平\(\alpha = 0.05\)下，不能拒绝原假设。 方差的检验 若假设上面北京的新建住房价格指数服从正态分布\(N(\mu, \sigma^2)\)，试问在显著性水平\(\alpha = 0.05\)下，猜测价格指数总体方差是否超过0.25，问是否能拒绝这个猜测？ 解：这是一个假设检验的问题，总体\(N(\mu, \sigma^2)\)，待检验的原假设为：\(\sigma^2 \geq 0.25\)，备择假设为：\(\sigma^2 &lt; 0.25\)，这是一个单侧检验。在方差\(\sigma^2\)的检验中，无论总体期望\(\mu\)是否已知，都可以使用卡方检验。下面是将总体期望\(\mu\)已知和未知两种情况下的方差检验写在同一个自定义函数中。 自定义函数 123456789101112131415161718192021222324252627x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)chisq.var.test &lt;- function(x, var, mu = Inf, alternative = "two.sided")&#123; n &lt;- length(x) # 样本容量 if(mu == Inf) # 总体均值已知的情况 &#123; df &lt;- n - 1 # 自由度 v &lt;- var(x) # 样本方差 &#125; else # 总体均值未知的情况 &#123; df &lt;- n # 自由度 v = sum((x - mu)^2) / n # 总体方差的极大似然估计MLE &#125; chi &lt;- df * v / var # 统计量 ret &lt;- list() # 创建一个空的list，用于存放输出结果 ret$df &lt;- df ret$var &lt;- v ret$chi &lt;- chi if (alternative == "two.sided") ret$p &lt;- 2 * min(pchisq(chi, df), pchisq(chi, df, lower.tail = FALSE)) else if (alternative == "less") ret$p &lt;- pchisq(chi, df = df) else if (alternative == "greater") ret$p &lt;- pchisq(chi, df = df, lower.tail = FALSE) ret&#125; 若总体期望\(\mu = 102.4\) 1chisq.var.test(x, 0.25, mu = 102.4, alternative = "less") # 总体期望已知 输出结果： 12345678$df[1] 12$var[1] 0.4533333$chi[1] 21.76$p[1] 0.9597007 若总体期望\(\mu\)未知 1chisq.var.test(x, 0.25, alternative = "less") # 总体期望未知 输出结果： 12345678$df[1] 11$var[1] 0.4751515$chi[1] 20.90667$p[1] 0.965649 可以看到，无论在总体期望\(\mu\)已知或未知的情况下，显著性水平\(\alpha = 0.05\)都小于检验的\(p\)值，所以在显著性水平\(\alpha = 0.05\)下，不能拒绝原假设。说明新建住宅价格指数的方差大于0.25，变动很大。 两个正态总体的检验 检验对象 条件 假设检验方法 R语言实现 比较两总体均值 方差\(\sigma_1^2\)、\(\sigma_2^2\)已知 \(Z\)检验 z.test（程序包BSDA） 比较两总体均值 方差\(\sigma_1^2\)、\(\sigma_2^2\)未知但相等 \(t\)检验 t.test（自带函数） 比较两总体均值 方差\(\sigma_1^2\)、\(\sigma_2^2\)未知且不等 近似\(t\)检验 t.test（自带函数） 成对数据的两样本均值检验 \(t\)检验 t.test（自带函数） 比较两总体方差 \(F\)检验 var.test 比率 精确检验 二项分布检验 binom.test 比率 近似检验（样本量较小） 正态检验 prop.test 场景数据：假设某零售公司延长营业时间前后27个典型周的周销售额数据（以万元为单位）为： 延长之前：67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62 延长之后：86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36 均值差的检验 两个总体的方差已知 假设上面销售额样本服从正态分布，且满足延长之前\(Y \sim N(\mu_1, 8)\)，延长之后\(X \sim N(\mu_2, 12)\)。延长营业时间后，在显著性水平\(\alpha = 0.05\)下，周销售额是否更高？ 解：待检验的原假设为：\(\mu_1 - \mu_2 \geq 0\)，备择假设为：\(\mu_1 - \mu_2 &lt; 0\)，这是一个单侧检验问题。由于两个总体的方差\(\sigma_1^2\)、\(\sigma_2^2\)已知，使用双样本的\(z\)检验。 自定义函数 1234567891011121314151617181920212223242526x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本custom.z.test &lt;- function(x, y, sigma1, sigma2, alternative = "two.sided")&#123; m &lt;- length(x) # 样本x的容量 n &lt;- length(y) # 样本y的容量 mean.x &lt;- mean(x) # 样本x的均值 mean.y &lt;- mean(y) # 样本y的均值 z &lt;- (mean.x - mean.y) / sqrt(sigma1^2 / m + sigma2^2 / n) # 统计量 ret &lt;- list() # 创建一个空的list，用于存放输出结果 ret$z &lt;- z if (alternative == "two.sided") ret$p &lt;- 2 * pnorm(abs(z), lower.tail = FALSE) else if (alternative == "less") ret$p &lt;- pnorm(z) else if (alternative == "greater") ret$p &lt;- pnorm(z, lower.tail = FALSE) ret&#125;custom.z.test(y, x, 8, 12, alternative = "less") 输出结果： 1234$z[1] -8.842544$p[1] 4.678191e-19 BSDA包的z.test函数 1234567891011install.packages("BSDA") # 安装BSDA包，若已安装，则删除此行library(BSDA)x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本z.test(y, x, sigma.x = 8, sigma.y = 12, alternative = "less") 输出结果： 123456789 Two-sample z-Testdata: y and xz = -8.8425, p-value &lt; 2.2e-16alternative hypothesis: true difference in means is less than 095 percent confidence interval: NA -19.97758sample estimates:mean of x mean of y 60.61222 85.15519 上面两个输出结果显示，z为检验统计量，p-value为检验的\(p\)值，因为p-value &lt; 显著性水平\(\alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，应当拒绝原假设。 两个总体的方差未知 假设上面销售额样本服从正态分布，且满足延长之前\(Y \sim N(\mu_1, \sigma_1^2)\)，延长之后\(X \sim N(\mu_2, \sigma_2^2)\)。延长营业时间后，在显著性水平\(\alpha = 0.05\)下，周销售额是否更高？ 解：待检验的原假设为：\(\mu_1 - \mu_2 \geq 0\)，备择假设为：\(\mu_1 - \mu_2 &lt; 0\)，这是一个单侧检验问题。由于两个总体的方差\(\sigma_1^2\)、\(\sigma_2^2\)未知，使用双样本的\(t\)检验。 输入样本数据： 12345678x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本 若两个总体方差相等，\(\sigma_1^2 = \sigma_2^2\) 1t.test(y, x, alternative = "less") 输出结果： 123456789 Welch Two Sample t-testdata: y and xt = -8.3792, df = 43.567, p-value = 6.257e-11alternative hypothesis: true difference in means is less than 095 percent confidence interval: -Inf -19.62045sample estimates:mean of x mean of y 60.61222 85.15519 若两个总体方差不等\(\sigma_1^2 \neq \sigma_2^2\)，则设置t.test函数中的var.equal参数为FALSE 1t.test(y, x, var.equal = FALSE, alternative = "less") 输出结果： 123456789 Welch Two Sample t-testdata: y and xt = -8.3792, df = 43.567, p-value = 6.257e-11alternative hypothesis: true difference in means is less than 095 percent confidence interval: -Inf -19.62045sample estimates:mean of x mean of y 60.61222 85.15519 上面两个输出结果显示，t为检验统计量，df为自由度，p-value为检验的\(p\)值，置信水平为0.95时的置信区间为[-Inf, -19.62045]。因为\(p = 6.257 \times 10^{-11} &lt; \alpha = 0.05\)，所以在显著性水平\(\alpha = 0.05\)下，应当拒绝原假设。这说明延长营业时间后销售额更高。 成对数据的比较检验 例：银行经理发觉目前过于强调顾客的存款数，他认为必须同时强调存款的期限。为此设计了一种将存款数与存款期相乘的指数，然后介绍了一种有刺激性的有奖计划，尽量减少顾客取款。现在他随机选择了15个储户，比较了在引用新计划前后的指数，指数数据如下： 储户编号 后 前 差\(Y_i\)后 - 前（元） 1 10540 10020 520 2 780 720 60 3 9453 9105 348 4 1573 1062 511 5 3962 3905 57 6 4673 4401 272 7 8205 8100 105 8 12458 12011 447 9 959 847 112 10 7444 6853 591 11 4982 4602 380 12 8831 8452 379 13 648 182 466 14 6969 6740 229 15 2408 2378 30 试在显著性水平\(\alpha = 0.01\)下，检验该经理的计划是否有效。 解：以\(Y\)记为"新旧方法之下一储户存款指数之差"。这里每储户在新旧两种方法下的指数自然地构成了一个对，故表中每个\(Y_i\)正反映了这两种方法的差别。若假定\(Y \sim N(\mu, \sigma^2)\)，且把"新方法无效"作为原假设，则问题归结为假设检验问题，即原假设：\(\mu \leq 0\)，备择假设：\(\mu &gt; 0\)。这是一个单样本单侧\(t\)检验问题，所以使用t.test函数来进行检验。 123456x &lt;- c(520, 60, 348, 511, 57, 272, 105, 447, 112, 591, 380, 379, 466, 229, 30)# conf.level为置信水平0.99 = 1 - 0.01# alternative = "greater"表示为右侧检验t.test(x = x, conf.level = 0.99, alternative = "greater") 输出结果： 123456789 One Sample t-testdata: xt = 6.0944, df = 14, p-value = 1.385e-05alternative hypothesis: true mean is greater than 099 percent confidence interval: 171.0729 Infsample estimates:mean of x 300.4667 输出结果显示，t为检验统计量值，df为自由度，p-value为检验\(p\)值，置信水平为0.99时的置信区间为[171.0729, Inf]。因为\(p = 1.385\times 10^{-5} &lt; \alpha = 0.01\)，所以在显著性水平\(\alpha=0.01\)下，应当拒绝原假设。这表明观察值显著地支持新方法有助于提高存款指数。 方差的检验 方差比的检验 假设上面销售额样本服从正态分布，且满足延长之前\(Y \sim N(\mu_1, \sigma_1^2)\)，延长之后\(X \sim N(\mu_2, \sigma_2^2)\)。延长营业时间后，在显著性水平\(\alpha = 0.05\)下，周销售额的波动是否与之前一致？ 解：待检验的原假设为：\(\sigma_1^2 = \sigma_2^2\)，备择假设为：\(\sigma_1^2 \neq \sigma_2^2\)，这是一个双侧检验问题。可以使用双样本的\(F\)检验检测。 var.test函数 123456789x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本var.test(y, x) 输出结果： 123456789 F test to compare two variancesdata: y and xF = 0.38893, num df = 26, denom df = 26, p-value = 0.01914alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 0.1772458 0.8534348sample estimates:ratio of variances 0.3889315 输出结果显示，F为检验统计量，num df为分子自由度，denom df为分母自由度，p-value为检验的\(p\)值，置信水平为0.95时的置信区间为[0.1772458, 0.8534348]。因为p-value = 0.01914 &lt; 显著性水平\(\alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，应当拒绝原假设。这说明延长营业时间前后的销售额的方差不相同。 其它分布参数的假设检验 比例的检验 例：某厂生产某种产品，每批次产量很大，出厂标准是次品率不超过2%。现从一批产品中随机抽取400件，发现12件次品。在显著性水\(\alpha = 0.05\)下，能否让这批产品出厂？ 解：这是关于不合格品率\(p\)的假设检验，假设原假设：\(p = 0.02\)，备择假设：\(p \neq 0.02\)。假设以\(x\)表示样品中不合格品的个数，当从产品中随机抽取1件样品，则\(x\)服从二点分布（合格或不合格），即\(x \sim b(1, p)\)。现从产品中抽取400件样品\((x_1,\cdots,x_n;n=400)\)，由分布的可列可加性得到统计量$ T = _{i = 1}^{400}x_i\(服从二项分布，即\)T b(400, p)$。所以可以使用R中的binom.test函数完成检验。 12345# x表示n次试验中，事件发生的次数# n表示试验次数# p表示原假设中，事件发生的概率# conf.level表示置信水平，即0.95 = 1 - 显著性水平0.05binom.test(x = 12, n= 400, p = 0.02, conf.level = 0.95) 输出结果： 12345678910 Exact binomial testdata: 12 and 400number of successes = 12, number of trials = 400, p-value =0.1507alternative hypothesis: true probability of success is not equal to 0.0295 percent confidence interval: 0.01559555 0.05181722sample estimates:probability of success 0.03 输出结果显示，检验的\(p\)值为0.1507，置信水平0.95下的置信区间为[0.01559555, 0.05181722]，不合格品率的点估计为0.03。因为\(p = 0.1507 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，不能拒绝原假设。这说明可以认为这批产品的次品率仍为2%，可以出厂。 大样本检验 上面使用二项分布检验比例\(p\)值，有一个问题：若样本量\(n\)太小，则检验难于作出可靠结论。因此如果样本量\(n\)较大，可以使用近似的检验方法：大样本检验。大样本检验的一般思路是：设\(x_1,\cdots,x_n\)是来自某总体的样本，又设该总体均值为\(\theta\)，方差为\(\theta\)的的函数，记为\(\sigma^2(\theta)\)。在样本量\(n\)充分大时，根据中心极限定理可知，样本均值\(\overline{x}\)近似服从\(N(\theta,\sigma^2(\theta)/n)\)，从而可以把问题转化为正态分布去处理。 对于上面不合格品率的场景，样本比例的抽样分布近似服从正态分布，可以使用prop.test函数进行检验。 12345# x表示n次试验中，事件发生的次数# n表示试验次数# p表示原假设中，事件发生的概率# conf.level表示置信水平，即0.95 = 1 - 显著性水平0.05prop.test(x = 12, n = 400, p = 0.02, conf.level = 0.95) 输出结果： 123456789 1-sample proportions test with continuity correctiondata: 12 out of 400, null probability 0.02X-squared = 1.5625, df = 1, p-value = 0.2113alternative hypothesis: true p is not equal to 0.0295 percent confidence interval: 0.01632970 0.05325459sample estimates: p 0.03 输出结果显示，检验的\(p\)值为0.2113，置信水平0.95下的置信区间为[0.01632970, 0.05325459]，不合格品率的点估计为0.03。因为\(p = 0.2113 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，不能拒绝原假设。这说明可以认为这批产品的次品率仍为2%，可以出厂。 分布拟合检验 Neyman-Pearson拟合度卡方检验 场景数据：假设2012年各月北京的新建住宅价格指数样本数据如下： 1月 2月 3月 4月 5月 6月 7月 8月 9月 10月 11月 12月 102.5 102.4 102 101.8 101.8 102.1 102.3 102.5 102.6 102.8 103.4 104.2 使用Neyman-Pearson卡方检验来判断北京市新建住宅价格指数是否服从正态分布。 输入数据，绘制直方图，对数据的分布形态进行初步了解。 123x &lt;- c(102.5, 102.4, 102, 101.8, 101.8, 102.1, 102.3, 102.5, 102.6, 102.8, 103.4, 104.2)hist(x) 输出结果： 对数据进行分组，计算频数分布。 12345678k &lt;- 5 # 组个数min &lt;- min(x)max &lt;- max(x)d &lt;- (max - min) / k # 组距# 分组端点endPoints &lt;- c(min - 0.1, min + d, min + 2 * d, min + 3 * d, min + 4 * d, max + 0.1)# 频数分布(freq &lt;- table(cut(x, breaks = endPoints))) 输出结果： 1234(101.7,102.3] (102.3,102.8] (102.8,103.2] (103.2,103.7] 4 5 1 1 (103.7,104.3] 1 计算原假设条件下数据落入各区间的理论值，由于假设的时正态分布，所以先计算样本均值和标准差，再用正态分布函数pnorm计算落入各小区间内的理论概率。 123points &lt;- c(min, min + d, min + 2 * d, min + 3 * d, min + 4 * d, max)p &lt;- pnorm(points, mean(x), sd(x))(p &lt;- c(p[1], p[2] - p[1], p[3] - p[2], p[4] - p[3], 1 - p[4])) 输出结果： 1[1] 0.1436956 0.2129215 0.2722409 0.2185020 0.1526400 使用函数chisq.test完成Neyman-Pearson拟合度卡方检验 1chisq.test(freq, p = p) 输出结果： 1234Chi-squared approximation may be incorrect Chi-squared test for given probabilitiesdata: freqX-squared = 8.2968, df = 4, p-value = 0.08129 输出结果显示，X-squared为检验统计量的值，df为自由度，p-value为检验的\(p\)值。因为\(p = 0.08129 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，不能拒绝原假设。这说明可以认为北京市新建住宅价格指数服从正态分布。 Kolmogrov-Smirnov检验 单样本检验 场景数据：某气象站收集了44个独立的年降雨量数据，资料如下： 520, 556, 561, 616, 635, 669, 686, 692, 704, 704, 711, 713, 714, 719, 727, 735, 740, 744, 745, 750, 776, 777, 786, 786, 791, 794, 821, 822, 826, 834, 837, 851, 862, 873, 879, 889, 900, 904, 922, 926, 952, 963, 1056, 1074 根据这批数据验证其服从正态分布。 解：这是一个分布拟合的假设检验问题。原假设：降雨量总体服从正态分布，备择假设：降雨量总体不服从正态分布。在原假设成立时，可以求出该正态分布的期望和方差的点估计，然后根据点估计使用Kolmogrov-Smirnov检验进行检测检验。 1234567891011121314151617181920212223x &lt;- c(520, 556, 561, 616, 635, 669, 686, 692, 704, 704, 711, 713, 714, 719, 727, 735, 740, 744, 745, 750, 776, 777, 786, 786, 791, 794, 821, 822, 826, 834, 837, 851, 862, 873, 879, 889, 900, 904, 922, 926, 952, 963, 1056, 1074) # 输入样本# 似然函数f &lt;- function(para)&#123; mean &lt;- mean(x) le.fun &lt;- dnorm(x, para[1], para[2]) # 因为nlminb求的是最小值，而这里要的是最大值，所以要添加一个负号 return(-sum(log(le.fun)))&#125;lower &lt;- c(0.0001, 0.0001) # 参数取值范围下界upper &lt;- c(Inf, Inf) # 参数取值范围上界ret &lt;- nlminb(c(mean(x), var(x)), f, lower = lower, upper = upper) # 求极大似然估计# 采用秩统计量，在排序过程中若样本x中有重复值，就会显示# 警告：ties should not be present for the Kolmogorov-Smirnov test# 可以使用jitter(x)为样本增加一些噪声，但不影响分布，也不会影响最终的检测结果。# y是要检验的分布的分布函数的名称，这里要检验拟合的是正态分布，所以使用"pnorm"# ret$par[1]为期望的极大似然估计# ret$par[2]方差的极大似然估计ks.test(jitter(x), y = "pnorm", ret$par[1], ret$par[2]) 输出结果： 1234 One-sample Kolmogorov-Smirnov testdata: jitter(x)D = 0.070192, p-value = 0.9712alternative hypothesis: two-sided 输出结果显示，D为检验统计量，p-value为检验的\(p\)值。因为\(p = 0.9712 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，不能拒绝原假设。 双样本检验 123456789# 使用标准正态分布分别随机生成样本量为30的x样本和40的y样本x &lt;- rnorm(30)y &lt;- rnorm(40)# 使用均匀分布随机生成35个0到1之间随机数的样本zz &lt;- runif(35)# 检验x样本和y样本是否为同一分布ks.test(x, y)# 检验x样本和z样本是否为同一分布ks.test(x, z) 输出结果： 123456789 Two-sample Kolmogorov-Smirnov testdata: x and yD = 0.2, p-value = 0.4509alternative hypothesis: two-sided Two-sample Kolmogorov-Smirnov testdata: x and zD = 0.5381, p-value = 7.968e-05alternative hypothesis: two-sided 输出结果显示，在x样本和y样本的检验中\(p = 0.4509 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，可以认为x样本和y样本为同一分布。在x样本和z样本的的检验中\(p = 7.968 \times 10^{-5} &lt; \alpha = 0.05\)，所以所以在显著性水平\(\alpha=0.05\)下，可以认为x样本和z样本不是同一分布。 \(W\)检验 使用上面Kolmogrov-Smirnov检验中单样本检验的降雨量的数据，使用Shapiro-Wilk检验（简称\(W\)检验）进行正态分布拟合检验。 12345x &lt;- c(520, 556, 561, 616, 635, 669, 686, 692, 704, 704, 711, 713, 714, 719, 727, 735, 740, 744, 745, 750, 776, 777, 786, 786, 791, 794, 821, 822, 826, 834, 837, 851, 862, 873, 879, 889, 900, 904, 922, 926, 952, 963, 1056, 1074)shapiro.test(x) 输出结果： 123 Shapiro-Wilk normality testdata: xW = 0.98596, p-value = 0.8622 输出结果显示，因为\(p = 0.8622 &gt; \alpha = 0.05\)，所以在显著性水平\(\alpha=0.05\)下，不能拒绝原假设，可以认为降雨量总体服从正态分布。]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
        <tag>z检验</tag>
        <tag>t检验</tag>
        <tag>F检验</tag>
        <tag>卡方检验</tag>
        <tag>KS检验</tag>
        <tag>W检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数估计及R实现]]></title>
    <url>%2FR%E8%AF%AD%E8%A8%80%2F%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E5%8F%8Ar%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[参数估计的R语言实现 概要 简要描述参数估计相关概念，并使用R语言实现参数估计中常用的点估计和区间估计，其中点估计也包含两种常用的方法：矩法估计和极大似然估计。 \[ \begin{cases} \text{点估计} \begin{cases} \text{矩法估计} \\ \text{极大似然估计} \end{cases}\\ \text{区间估计} \end{cases} \] 参数估计 参数估计，指对于所研究的总体，在总体分布类型已知情况下，根据样本信息对总体分布中所包含的未知参数进行估计。例如，假设总体\(X \sim N(\mu, 1)\)，\(\mu\)为未知参数，\(x_1,\cdots,x_n\)是来自总体\(X\)的一个样本，然后通过样本对未知参数进行估计。参数估计有两种常用的形式，一种叫点估计，就是用一个具体的数值去估计一个未知参数；另一种叫区间估计，就是要找两个统计量\(\widehat{\theta}_L = \widehat{\theta}_L(x_1,\cdots,x_n)\)和\(\widehat{\theta}_U = \widehat{\theta}_U(x_1,\cdots,x_n)\)，使得\(\widehat{\theta}_L \leq \widehat{\theta}_U\)，在得到样本观测值之后，就把\(\theta\)估计在区间\([\widehat{\theta}_L, \widehat{\theta}_U]\)。 点估计 点估计有两种常用方法：矩法估计和极大似然估计。 矩法估计 矩法估计的基本思想是基于替换原则对参数进行估计，可以理解为使用样本的原点矩（中心矩）替换总体的原点矩（中心矩）对参数进行估计。矩法估计的计算方法如下： 设总体\(X\)的分布函数为\(F(x;\theta_1,\cdots,\theta_l)\)，其中\(\theta_1,\cdots,\theta_l\)为待估参数，\(l\)为未知参数个数。设\(x_1,\cdots,x_n\)是来自总体\(X\)的样本，使用\[M_k(k = 1, 2,\cdots,l)\]表示\(k\)阶样本原点矩（或中心矩）。若总体\(X\)的\(k(k \geq l)\)阶原点矩（或中心矩）存在，并记为\(\mu_k\)，若\(\mu_k\)能表示成参数\(\theta_1,\cdots,\theta_l\)的函数，即\[\mu_k = \mu_k(\theta_1,\cdots,\theta_l), (k = 1, 2,\cdots, l)\] 则令前\(l\)阶样本原点矩（中心矩）与相应的前\(l\)阶总体原点矩（中心矩）相等，这样就得到一个联立方程： \[ \begin{cases} \mu_1(\theta_1,\cdots,\theta_l) = M_1\\ \mu_2(\theta_1,\cdots,\theta_l) = M_2\\ \qquad\qquad \vdots\\ \mu_l(\theta_1,\cdots,\theta_l) = M_l \end{cases} \] 解方程组可得\(\widehat{\theta}_k=\widehat{\theta}_k(x_1, \cdots, x_n), k = 1,2,\cdots,l\)，然后用\(\widehat{\theta}_k\)作为\(\theta_k\)的估计，即得所有未知参数\(\theta_1,\cdots,\theta_l\)的矩法估计。 R语言实现 由矩法估计的计算方法可以看到，矩法估计通过替换原则，最后归结为求解方程组的问题。在R语言中常用的解方程函数有： 函数 功能 程序包 uniroot 求解一元（非线性）方程 stats multiroot 给定\(n\)个（非线性）方程，求解\(n\)个根 rootSolve uniroot函数的使用 函数原型： 12345uniroot(f, interval, ..., lower = min(interval), upper = max(interval), f.lower = f(lower, ...), f.upper = f(upper, ...), extendInt = c("no", "yes", "downX", "upX"), check.conv = FALSE, tol = .Machine$double.eps^0.25, maxiter = 1000, trace = 0) 其中主要参数含义如下: 参数 含义 f 要求解的方程的函数 interval 数值向量，要求解的根的搜索区间 ... 传递给第一个参数f函数的参数 lower min(interval)，搜索区间的左端点 upper max(interval)，搜索区间的右端点 tol 所需精度 maxiter 最大迭代次数 multiroot函数求解方程组 函数原型： 123456multiroot(f, start, maxiter = 100, rtol = 1e-6, atol = 1e-8, ctol = 1e-8, useFortran = TRUE, positive = FALSE, jacfunc = NULL, jactype = "fullint", verbose = FALSE, bandup = 1, banddown = 1, parms = NULL, ...) 这里所用到的参数及其含义如下： 参数 含义 f 所要求解的方程的函数 start 给定根的初始值 例：设总体\(X\)服从均匀分布\(U(a, b)\)，其中\(a\)，\(b\)为未知参数，从总体\(X\)中获得容量为\(n=5\)的样本：\((x_1 = 4.5，x_2 = 5.0，x_3 = 4.7，x_4 = 4.0，x_5 = 4.2)\)，求参数\(a\)，\(b\)的矩估计\(\widehat{a}, \widehat{b}\)。 解：因为有\(a\)，\(b\)两个未知参数，所以令阶矩\(k = 2\)。已知1阶样本原点矩（样本均值）\(M_1 = \overline{x} = \frac{1}{n}\sum_{i=1}^n x_i\)，2阶样本中心矩（样本方差）\(M_2 = s^2 = \frac{1}{n - 1}\sum_{i = 1}^n (x_i - \overline{x})^2\)，由于1阶原点矩（期望）\(\mu_1 = \frac{a + b}{2}\)，2阶中心矩（方差）\(\mu_2 = \frac{(b - a)^2}{12}\)，所以得到联立方程组： \[ \begin{cases} \frac{a + b}{2} = \overline{x}\\ \frac{(b - a)^2}{12} = s^2 \end{cases} \] 使用multiroot求解方程组求时，需先安装R包rootSolve 1install.packages("rootSolve") 安装完成后，即可使用multiroot函数求解方程组。需要注意的是，multiroot函数的第一个参数f要求求解的方程组的函数值为0的根。 12345678910library(rootSolve) # 载入rootSolvex &lt;- c(4.5，5.0，4.7，4.0，4.2)mean &lt;- mean(x) # 样本均值var &lt;- var(x) # 样本方差model &lt;- function(theta, mean, var)&#123; # 方程组向量c(f1 = (theta[1] + theta[2]) / 2 - mean, f2 = (theta[2] - theta[1])^2/12-var) &#125; # 求解方程组的函数# 使用multiroot求解方程组, 参数mean，var会传递到model函数对应的mean,var中multiroot(f = model, start = c(0,10), mean = mean, var = var) 输出结果： 123456789$root[1] 3.793706 5.166294 $f.rootf1 f2 -1.580958e-13 7.194056e-11 $iter[1] 6$estim.precis[1] 3.604933e-11 第一行$root给出的输出结果就是所求的参数估计值\(\widehat{a} = 3.793706, \widehat{b}=5.166294\)。 极大似然估计 极大似然估计本质上是对似然函数关于未知参数求最大值问题，所求出的极大值就作为相应未知参数的极大似然估计。 概率函数 设\(X\)是一个连续型或离散型随机变量，其概率函数定义为 \[ p(x) = \begin{cases} P(X = x), &amp; X为离散型随机变量，P(X = x)为X分布列;\\ f(x), &amp; X为连续型随机变量，f(x)为X的密度函数 \end{cases} \] 似然函数 设\(x_1,\cdots,x_n\)是来自总体\(X\)的样本，\(X\)的概率函数为\(p(x;\theta)\)，其中参数\(\theta \in \Theta\)，\(\Theta\)为参数空间。样本\((x_1,\cdots,x_n)\)的联合概率函数\[p(x_1;\theta)p(x_2;\theta)\cdots p(x_n;\theta)\]作为\(\theta\)函数，称其为似然函数，记为\(L(\theta) = L(\theta;x_1,\cdots,x_n)\)，即\[L(\theta) = L(\theta;x_1,\cdots,x_n) = p(x_1;\theta)p(x_2;\theta)\cdots p(x_n;\theta)\] 极大似然估计 若\(\widehat{\theta}(x_1, \cdots, x_n)\)是一个统计量，满足条件\[L(\widehat{\theta};x_1,\cdots,x_n) = max_{\theta \in \Theta}L(\theta;x_1,\cdots,x_n)\] 则称\(\widehat{\theta}(x_1, \cdots, x_n)\)是\(\theta\)的极大似然估计，简称MLE。 R语言实现 例：已知在文学家萧伯纳的&lt;&lt;An Intelligent Woman's Guide To Socialism&gt;&gt;一书中，一个句子的单词数\(X\)近似服从对数正态分布，即\(Z = ln X \sim N(\mu, \sigma^2)\)。现从该书中随机地取20个句子，这些句子中的单词数分别为：52, 24, 15, 67, 15, 22, 63, 26, 16, 32, 7, 33, 28, 14, 7, 29, 10, 6, 59, 30。求该书中一个句子单词数均值\(E(X) = e^{\mu + \sigma^2/2}\)的最大似然估计 nlminb函数 nlminb函数时非线性最小化函数。 1234567891011121314x &lt;- c(52, 24, 15, 67, 15, 22, 63, 26, 16, 32, 7, 33, 28, 14, 7, 29, 10, 6, 59, 30) # 样本lfun &lt;- function(para)&#123; # 似然函数 f &lt;- dlnorm(x, para[1], para[2]) logfun &lt;- sum(log(f)) # 使用对数函数进行求极值 return(-logfun) # 因为nlminb求的是极小值，所以应返回相反数&#125;start &lt;- c(3, 3) # 参数求解的初始值lower &lt;- c(0.0001, 0.0001) # 参数取值范围下界upper &lt;- c(Inf, Inf) # 参数取值范围上界para &lt;- nlminb(lfun, start = start, lower = lower, upper = upper) # 求似然函数极值c(mu &lt;- para$par[1], sigma &lt;- para$par[2], exp(mu + sigma^2 / 2)) 输出结果： 1[1] 3.0890327 0.7128333 28.3066946 第1、2个输出结果分别为\(\mu\)、\(\sigma\)的极大似然估计，\(\widehat{\mu} = 3.0890327\)，\(\widehat{\sigma} = 0.7128333\)；由极大似然估计不变性可知，所求该书中一个句子单词数均值的极大似然估计\(E(X) = e^{\mu + \sigma^2/2} = 28.3066959\) 极大似然估计不变性：如果\(\widehat{\theta}\)是\(\theta\)的极大似然估计，则对任一函数\(g(\theta)\)，其最大似然估计为\(g(\widehat{\theta})\)。 maxLik函数 maxLik函数在maxLik包中，maxLik函数使用简单，而且即使在初始值偏离较大的情况下，计算得到的估计值也非常接近真值，计算准确度较高。 123456789101112x &lt;- c(52, 24, 15, 67, 15, 22, 63, 26, 16, 32, 7, 33, 28, 14, 7, 29, 10, 6, 59, 30) # 样本lfun &lt;- function(para)&#123; # 似然函数 f &lt;- dlnorm(x, para[1], para[2]) logfun &lt;- sum(log(f)) # 使用对数函数进行求极值 return(logfun)&#125;start &lt;- c(3, 3) # 参数求解的初始值para &lt;- maxLik(lfun, start = start)$estimate # 求似然函数极值c(mu &lt;- para[1], sigma &lt;- para[2], exp(mu + sigma^2 / 2)) 输出结果： 1[1] 3.0890327 0.7128333 28.3066946 第1、2个输出结果分别为\(\mu\)、\(\sigma\)的极大似然估计，\(\widehat{\mu} = 3.0890327\)，\(\widehat{\sigma} = 0.7128332\)；由极大似然估计不变性可知，所求​该书中一个句子单词数均值的极大似然估计\(E(X) = e^{\mu + \sigma^2/2} = 28.3066928\)。 区间估计 单个正态总体参数的置信区间 设\(x_1,\cdots,x_n\)是来自总体\(X \sim N(\mu, \sigma^2)\)的样本，样本容量为\(n\)，\(\overline{x}\)为样本均值，\(s^2\)为样本方差。 方差\(\sigma\)已知，估计期望\(\mu\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\(u = \frac{\overline{x} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1)\)，期望\(\mu\)的置信区间为\[[\overline{x} - u_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}, \quad \overline{x} + u_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}]\] 其中\(u_{1 - \frac{\alpha}{2}}\)为标准正态分布\(N(0, 1)\)的\(1 - \frac{\alpha}{2}\)分位数。 例：0.5，1.25，0.8，2是取自总体\(X\)的样本，已知\(Y = ln X \sim N(\mu, 1)\)，求\(\mu\)的置信水平为0.95%的置信区间。 解：由题意，对\(X\)的样本求对数，得到\(Y\)样本，然后使用\(Y\)的样本根据分布条件计算置信区间 自定义函数 12345678910sample &lt;- c(0.5, 1.25, 0.8, 2)x &lt;- log(sample) # 样本# 参数x为样本，sigma为已知的总体标准差，alpha为显著性水平conf.int &lt;- function(x, sigma, alpha = 0.05)&#123; mean &lt;- mean(x) n &lt;- length(x) z &lt;- qnorm(p = 1-alpha/2, lower.tail = TRUE) c(mean - z * sigma / sqrt(n), mean + z * sigma / sqrt(n))&#125;conf.int(x, sigma = 1, alpha = 0.05) # 置信水平1 - 0.05 = 0.95 输出结果： 1[1] -0.979982 0.979982 求得\(\mu\)的置信水平为0.95%的置信区间为\([-0.979982,0.979982]\) BSDA包的z.test函数 BSDA（Basic Statistics and Data Analysis）称为基本统计和数据分析程序包。 123456install.packages("BSDA") # 安装BSDA包，若已安装，则注释此行library(BSDA) # 加载BSDA包sample &lt;- c(0.5, 1.25, 0.8, 2)x &lt;- log(sample) # 样本ret &lt;- z.test(x, sigma.x = 1) # 默认置信水平为0.95ret$conf.int # 置信区间 输出结果： 123[1] -0.979982 0.979982attr(,"conf.level")[1] 0.95 求得\(\mu\)的置信水平为0.95%的置信区间为\([-0.979982,0.979982]\)，conf.level属性显示的就是置信水平0.95。 UsingR包的simple.z.test函数 12345install.packages("UsingR") # 安装UsingR包，若已安装，则注释此行library(UsingR) # 加载Using包sample &lt;- c(0.5, 1.25, 0.8, 2)x &lt;- log(sample) # 样本simple.z.test(x, sigma = 1) # 默认置信水平为0.95 输出结果： 1[1] -0.979982 0.979982 求得\(\mu\)的置信水平为0.95%的置信区间为\([-0.979982,0.979982]\)。 方差\(\sigma\)未知，估计期望\(\mu\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\(\frac{\overline{x} - \mu}{s / \sqrt{n}} \sim t(n - 1)\)，\(\mu\)的置信区间为\[[\overline{x} - t_{1 - \frac{\alpha}{2}}(n-1)\frac{s}{\sqrt n}, \quad \overline{x} + t_{1 - \frac{\alpha}{2}}(n-1)\frac{s}{\sqrt n}]\] 其中\(t_{1 - \frac{\alpha}{2}}\)为\(t(n-1)\)分布的\(1 - \frac{\alpha}{2}\)分位数。 例：假设轮胎的寿命服从正态分布。为估计某种轮胎的平均寿命，现随机地抽取12只轮胎试用，测得它们的寿命（单位：万公里）如下：4.68, 4.85, 4.32, 4.85, 4.61, 5.02, 5.20, 4.60, 4.58, 4.72, 4.38, 4.70，试求平均寿命的0.95置信区间。 自定义函数 123456789101112x &lt;- c(4.68, 4.85, 4.32, 4.85, 4.61, 5.02, 5.20, 4.60, 4.58, 4.72, 4.38, 4.70) # 样本conf.int &lt;- function(x, alpha = 0.05)&#123; n &lt;- length(x) # 样本容量 df &lt;- n - 1 # 自由度 mean &lt;- mean(x) # 样本均值 s &lt;- sd(x) # 样本标准差 quan &lt;- qt(p = 1 - alpha / 2, df = df) # t(n-1)分布的1 - alpha/2分位数 min &lt;- mean - quan * s / sqrt(n) max &lt;- mean + quan * s / sqrt(n) c(min, max)&#125;conf.int(x, alpha = 0.5) # 置信水平1 - 0.05 = 0.95 输出结果： 1[1] 4.551601 4.866732 求平均寿命的0.95置信区间为\([4.551601, 4.866732]\) t.test函数 123x &lt;- c(4.68, 4.85, 4.32, 4.85, 4.61, 5.02, 5.20, 4.60, 4.58, 4.72, 4.38, 4.70) # 样本ret &lt;- t.test(x, conf.level = 0.95) # 置信水平0.95ret$conf.int 输出结果： 123[1] 4.551601 4.866732attr(,"conf.level")[1] 0.95 求平均寿命的0.95置信区间为\([4.551601, 4.866732]\) 期望\(\mu\)未知，估计方差\(\sigma^2\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\(\frac{(n - 1)s^2}{\sigma^2} \sim \chi^2(n - 1)\)，\(\sigma^2\)的置信区间为\[[\frac{(n - 1)s^2}{\chi_{1-\frac{\alpha}{2}}^2(n - 1)}, \quad \frac{(n - 1)s^2}{\chi_{\frac{\alpha}{2}}^2(n - 1)}]\] 其中\(\chi_{\frac{\alpha}{2}}\)，\(\chi_{1 - \frac{\alpha}{2}}\)分别为\(\chi^2(n - 1)\)分布的\(\frac{\alpha}{2}\)，\(1 - \frac{\alpha}{2}\)分位数。 例：某厂生产的零件重量服从正态分布\(N(\mu, \sigma^2)\)，现从该厂生产的零件中抽取9个，测得其质量为（单位：g）：45.3, 45.4, 45.1, 45.3, 45.5, 45.7, 45.4, 45.3, 45.6，试求总体标准差\(\sigma\)的0.95置信区间。 自定义函数 1234567891011x &lt;- c(45.3, 45.4, 45.1, 45.3, 45.5, 45.7, 45.4, 45.3, 45.6)# 默认置信水平为1 - 0.05 = 0.95conf.int &lt;- function(x, alpha = 0.05)&#123; n &lt;- length(x) # 样本容量 df &lt;- n - 1 # 自由度 varSample &lt;- var(x) # 样本方差 min &lt;- df * varSample / qchisq(1 - alpha / 2, df = df) max &lt;- df * varSample / qchisq(alpha / 2, df = df) c(min, max)&#125;conf.int(x) 输出结果： 1[1] 0.01482787 0.11928079 求总体标准差\(\sigma\)的0.95置信区间为\([0.01482787 0.11928079]\)。 两个正态总体下的置信区间 设\(x_1,\cdots,x_m\)是来自\(N(\mu_1, \sigma_1^2)\)的样本，\(y_1,\cdots,y_n\)是来自\(N(\mu_2, \sigma_2^2)\)的样本，且两个样本相互独立。\(\overline{x}\)和\(\overline{y}\)分别是它们的样本均值，\(s_x^2 = \frac{1}{m-1}\sum_{i=1}^m(x_i - \overline{x})^2\)和\(s_y^2 = \frac{1}{n-1}\sum_{i=1}^n(y_i - \overline{y})^2\)分别是它们的样本方差。 例：假设某零售公司延长营业时间前后27个典型周的周销售额数据（以万元为单位）为： 延长之前：67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62 延长之后：86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36 又假设销售额样本服从正态分布，且满足延长之前\(Y \sim N(\mu_1, 8)\)，延长之后\(X \sim N(\mu_2, 12)\)。 ####\(\sigma_1^2\)和\(\sigma_2^2\)已知，估计\(\mu_1 - \mu_2\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\(\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}} \sim N(0, 1)\)，\(\mu_1 - \mu_2\)的置信区间为\[[\overline{x} - \overline{y} - u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}, \quad \overline{x} - \overline{y} + u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}]\] 其中\(u_{1 - \frac{\alpha}{2}}\)为标准正态分布\(N(0, 1)\)的\(1 - \frac{\alpha}{2}\)分位数。 用零售公司的例子，求置信水平为0.95时，\(\mu_1 - \mu_2\)的置信区间。 自定义函数 1234567891011121314151617181920x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本# 置信水平1 - 0.05 = 0.95conf.int &lt;- function(x, y, sigma.x, sigma.y, alpha = 0.05)&#123; m &lt;- length(x) # 样本x的容量 n &lt;- length(y) # 样本y的容量 mean.x &lt;- mean(x) # 样本x的均值 mean.y &lt;- mean(y) # 样本y的均值 quan &lt;- qnorm(1 - alpha / 2) # 标准正态分布的1 - alpha/2分位数 min &lt;- mean.x - mean.y - quan * sqrt(sigma.x^2 / m + sigma.y^2 / n) max &lt;- mean.x - mean.y + quan * sqrt(sigma.x^2 / m + sigma.y^2 / n) c(min, max)&#125;conf.int(x, y, 12, 8) 输出结果： 1[1] 19.10298 29.98295 z.test函数 123456789101112install.packages("BSDA") # 安装BSDA包，若已安装，则注释此行library(BSDA)x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本ret &lt;- z.test(x, y, sigma.x = 12, sigma.y = 8) # 默认置信水平为0.95ret$conf.int 输出结果： 123[1] 19.10298 29.98295attr(,"conf.level")[1] 0.95 求得置信区间的结果为\([19.10298, 29.98295]\)。所得的置信区间结果说明，零售公司延长营业时间后，周营业额明显增加，在0.95的置信水平下，营业额增加的范围是\([19.10298, 29.98295]\)。 方差\(\sigma_1^2 = \sigma_2^2 = \sigma^2\)未知，估计\(\mu_1 - \mu_2\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\[\sqrt{\frac{mn(m + n - 2)}{m + n}}\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{(m - 1)s_x^2 + (n - 1)s_y^2}} \sim t(m + n - 2)\] 记\(s_w^2 = \frac{(m-1)s_x^2 + (n - 1)s_y^2}{m+n-2}\)，\(\mu_1 - \mu_2\)的置信区间为\[[\overline{x} - \overline{y} - \sqrt{\frac{m+n}{mn}}s_wt_{1-\frac{\alpha}{2}}(m+n-2), \overline{x} - \overline{y} + \sqrt{\frac{m+n}{mn}}s_wt_{1-\frac{\alpha}{2}}(m+n-2)]\] 用零售公司的例子，求置信水平为0.95时，\(\mu_1 - \mu_2\)的置信区间。 自定义函数 12345678910111213141516171819202122232425x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本# 置信水平1 - 0.05 = 0.95conf.int &lt;- function(x, y, alpha = 0.05)&#123; m &lt;- length(x) # 样本x的容量 n &lt;- length(y) # 样本y的容量 mean.x &lt;- mean(x) # 样本x的均值 mean.y &lt;- mean(y) # 样本y的均值 var.x &lt;- var(x) # 样本x的方差 var.y &lt;- var(y) # 样本y的方差 df &lt;- m + n - 2 # 自由度 quan &lt;- qt(1 - alpha / 2, df) # t(m + n - 2)的分位数 sw &lt;- sqrt(((m - 1) * var.x + (n - 1) * var.y) / (m + n - 2)) radius &lt;- sqrt((m + n) / (m * n)) * sw * quan min &lt;- mean.x - mean.y - radius max &lt;- mean.x - mean.y + radius c(min, max)&#125;conf.int(x, y) 输出结果： 1[1] 18.66541 30.42051 t.test函数 12345678910x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本ret &lt;- t.test(x, y, var.equal = TRUE) # var.equal = TRUE表示方差相等ret$conf.int 输出结果： 123[1] 18.66541 30.42051attr(,"conf.level")[1] 0.95 求得置信区间的结果为\([18.66541, 30.42051]\)。所得的置信区间结果说明，零售公司延长营业时间后，周营业额明显增加，在0.95的置信水平下，营业额增加的范围是\([18.66541, 30.42051]\)。 \(\frac{\sigma_2^2}{\sigma_1^2} = \theta\)已知，估计\(\mu_1 - \mu_2\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\[\sqrt{\frac{mn(m+n-2)}{m\theta + n}}\frac{\overline{x} - \overline{y} - (\mu_1 - \mu_2)}{\sqrt{(m - 1)s_x^2 + (n - 1)\frac{s_y^2}{\theta}}} \sim t(m + n - 2)\] 记\(s_t^2 = \frac{(m-1)s_x^2 + (n-1)\frac{s_y^2}{\theta}}{m+n-2}\)，\(\mu_1 - \mu_2\)的置信区间\[[\overline{x} - \overline{y} - \sqrt{\frac{m\theta + n}{mn}}s_tt_{1 - \frac{\alpha}{2}}(m+n-2), \overline{x} - \overline{y} + \sqrt{\frac{m\theta + n}{mn}}s_tt_{1 - \frac{\alpha}{2}}(m+n-2)]\] 用零售公司的例子，求置信水平为0.95时，\(\mu_1 - \mu_2\)的置信区间。 自定义函数 123456789101112131415161718192021222324x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本# 置信水平1 - 0.05 = 0.95conf.int &lt;- function(x, y, theta, alpha = 0.05)&#123; m &lt;- length(x) # 样本x的容量 n &lt;- length(y) # 样本y的容量 mean.x &lt;- mean(x) # 样本x的均值 mean.y &lt;- mean(y) # 样本y的均值 var.x &lt;- var(x) # 样本x的方差 var.y &lt;- var(y) # 样本y的方差 df &lt;- m + n - 2 # 自由度 quan &lt;- qt(1 - alpha / 2, df) # t(m + n - 2)的分位数 radius &lt;- sqrt((m * theta + n) / (m * n)) * sqrt(((m - 1) * var.x + (n - 1) * var.y / theta) / (m + n - 2)) * quan min &lt;- mean.x - mean.y - radius max &lt;- mean.x - mean.y + radius c(min, max)&#125;conf.int(x, y, 12/8) 输出结果： 1[1] 18.28586 30.80007 求得置信区间的结果为\([18.28586, 30.80007]\)。所得的置信区间结果说明，零售公司延长营业时间后，周营业额明显增加，在0.95的置信水平下，营业额增加的范围是\([18.28586, 30.80007]\)。 估计\(\frac{\sigma_1^2}{\sigma_2^2}\)的置信区间 在置信水平为\(1-\alpha\)的情况下，\(\frac{s_x^2/\sigma_1^2}{s_y^2/\sigma_2^2} \sim F(m-1,n-1)\)，\(\frac{\sigma_1^2}{\sigma_2^2}\)的置信区间\[[\frac{s_x^2}{s_y^2}\frac{1}{F_{1-\frac{\alpha}{2}}(m-1,n-1)}, \quad \frac{s_x^2}{s_y^2}\frac{1}{F_{\frac{\alpha}{2}}(m-1,n-1)}]\] 用零售公司的例子，求置信水平为0.95时，\(\frac{\sigma_1^2}{\sigma_2^2}\)的置信区间。 自定义函数 123456789101112131415161718192021x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本# 置信水平1 - 0.05 = 0.95conf.int &lt;- function(x, y, alpha = 0.05)&#123; m &lt;- length(x) # 样本x的容量 n &lt;- length(y) # 样本y的容量 df.x &lt;- m - 1 # 分子自由度 df.y &lt;- n - 1 # 分母自由度 var.x &lt;- var(x) # 样本x的方差 var.y &lt;- var(y) # 样本y的方差 min &lt;- var.x/var.y * qf(alpha / 2, df.x, df.y) # 置信区间下限 max &lt;- var.x/var.y * qf(1 - alpha / 2, df.x, df.y) # 置信区间上限 c(min, max)&#125;conf.int(y, x) 输出结果： 1[1] 0.1772458 0.8534348 var.test函数 方差比的区间估计与方差的假设检验密不可分，所以R中的函数var.test()可以用来直接计算两正态总体方差比的置信区间。 12345678910x &lt;- c(86.10, 71.13, 116.25, 102.60, 97.51, 65.39, 69.56, 70.80, 83.46, 87.27, 89.83, 80.51, 76.78, 71.11, 90.22, 87.55, 87.60, 90.08, 101.80, 84.96, 97.69, 66.39, 104.06, 72.98, 72.19, 91.01, 84.36) # 延长营业时间后的销售额样本y &lt;- c(67.90, 76.12, 68.64, 74.94, 63.32, 50.43, 44.92, 64.59, 61.68, 65.01, 55.90, 54.88, 46.78, 55.54, 67.90, 71.62, 63.00, 59.50, 63.20, 64.86, 59.32, 53.45, 62.51, 48.78, 53.55, 59.57, 58.62) # 延长营业时间前的销售额样本ret &lt;- var.test(y, x) # 默认置信水平为0.95ret$conf.int 输出结果： 123[1] 0.1772458 0.8534348attr(,"conf.level")[1] 0.95 求得置信区间的结果为\([0.1772458, 0.8534348]\)。所得的置信区间结果说明，零售公司延长营业时间后，周营业额的波动性比较大。]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
        <tag>R语言</tag>
        <tag>点估计</tag>
        <tag>区间估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用风险分析]]></title>
    <url>%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[信用风险分析 本文的信用风险是指由于借款人无法按时偿还而拖欠已经发生债务的违约风险。由于贷款人同时遭受了本金和利息的损失，所以这一风险由贷款人承担。因此，贷款人在向借款人发放贷款时需要对借款人的信用风险进行分析和预测评估。 本文根据从kaggle平台获取信用风险数据，主要对该数据中的借款人的信用风险与借款人的相关信息进行分析。找出可能影响信用风险的相关因素。 分析准备 获取数据 数据来源：kaggle.com/kabure/german-credit-data-with-risk 从数据来源的链接中下载数据，将得到csv格式的数据文件，然后读入到R环境中。 12345# 读入数据credit.df &lt;- read.csv( "german_credit_data.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE) 查看数据维度及其特征的详细信息 1str(credit.df) 123456789101112'data.frame': 1000 obs. of 11 variables: $ X : int 0 1 2 3 4 5 6 7 8 9 ... $ Age : int 67 22 49 45 53 35 53 35 61 28 ... $ Sex : chr "male" "female" "male" "male" ... $ Job : int 2 2 1 2 2 1 2 3 1 3 ... $ Housing : chr "own" "own" "own" "free" ... $ Saving.accounts : chr NA "little" "little" "little" ... $ Checking.account: chr "little" "moderate" NA "little" ... $ Credit.amount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... $ Duration : int 6 48 12 42 24 36 24 36 12 30 ... $ Purpose : chr "radio/TV" "radio/TV" "education" "furniture/equipment" ... $ Risk : chr "good" "bad" "good" "good" ... 数据共包含1000条实例，每一条实例包含一名银行客户的11个信息特征。每个特征的含有名称、数据类型以及样本值等信息。 数据特征含义 理解每个特征的含义： X：由于原始数据german_credit_data.csv文件的第一列是实例序号，当将文件读入R环境的data.frame类型时，data.frame会自动为其添加名称"X"，所以可以将其从数据集中删除。 Age：数值类型，表示年龄。 Sex ：文本类型，表示性别，"female"（女性），"male"（男性）。 Job ：数值类型，表示客户当前工作情况，0表示"unskilled and non-resident"（没有职业技能且不是常住居民的人员）；1表示"unskilled and resident"（没有职业技能但为常住居民的人员）；2表示"skilled"（拥有职业技能工作的人员）； 3表示"highly skilled"（拥有高级职业技能工作的人员）。 Housing ：文本类型，表示居住房子的所有权，"own"（自己拥有），"rent"（租住），"free"（免费居住）。 Saving accounts：文本类型，表示储蓄账户存款，"little"（少或贫穷），"moderate"（中等或适当），"rich"（富有），"quite rich"（比较富有）。 Checking account：文本类型，表示支票账户，"little"（少或贫穷），"moderate"（中等或适当），"rich"（富有）。 Credit amount：数值类型，表示贷款金额，单位德国马克。 Duration：数值类型，表示还款期限或贷款时间，单位月。 Purpose：文本类型，表示贷款的目的，"car"（购车），"furniture/equipment"（购买家具），"radio/TV"（购买家电），"domestic appliances, repairs"（家庭用具维修），"education"（教育），"business"（商业），"vacation/others"（ 度假或其它）。 Risk ：文本类型，表示信用风险的好坏，也是预测的目标值。因子水平：good（好）表示不会违约、bad（坏）违约。 分析工具准备 需要用到的R package package 描述 mice 处理缺失值 ggplot2 数据可视化 gridExtra 图形网格布局 gmodels 建立列联表 caret 分类和回归训练 分析开始时，可先将所需要的package安装并加载到R环境中。 分析所使用的自定义的工具函数 本文所使用的所有自定义的工具函数都放在名为"analytics_utils.R"的文件中。分析开始时，可先将该文件加载到R环境中。假设文件"analytics_utils.R"在当前的工作环境目录下，可使用source函数加载该文件内容。 12# 加载analytics_utils.Rsource("analytics_utils.R") analytics_utils.R 数据处理 删除第一列的实例序号数据，并查看前6行数据 12credit.df$X &lt;- NULL # 删除第一列数据colnames(credit.df) # 查看列名 1234[1] "Age" "Sex" "Job" [4] "Housing" "Saving.accounts" "Checking.account"[7] "Credit.amount" "Duration" "Purpose" [10] "Risk" 数据转换 将Risk列中的"good"和"bad"，分别改成1和0 1credit.df$Risk &lt;- ifelse(credit.df$Risk == "good", 1, 0) 将文本类型特征及特征Job转换成因子类型 123456factor.vars &lt;- c( "Risk", "Sex", "Job", "Housing", "Saving.accounts", "Checking.account", "Purpose")credit.df &lt;- to.factors(credit.df, factor.vars)str(credit.df) 1234567891011'data.frame': 1000 obs. of 10 variables: $ Age : int 67 22 49 45 53 35 53 35 61 28 ... $ Sex : Factor w/ 2 levels "female","male": 2 1 2 2 2 2 2 2 2 2 ... $ Job : Factor w/ 4 levels "0","1","2","3": 3 3 2 3 3 2 3 4 2 4 ... $ Housing : Factor w/ 3 levels "free","own","rent": 2 2 2 1 1 1 2 3 2 2 ... $ Saving.accounts : Factor w/ 4 levels "little","moderate",..: NA 1 1 1 1 NA 3 1 4 1 ... $ Checking.account: Factor w/ 3 levels "little","moderate",..: 1 2 NA 1 1 NA NA 2 NA 2 ... $ Credit.amount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... $ Duration : int 6 48 12 42 24 36 24 36 12 30 ... $ Purpose : Factor w/ 8 levels "business","car",..: 6 6 4 5 2 4 5 2 6 2 ... $ Risk : Factor w/ 2 levels "0","1": 2 1 2 2 1 2 2 2 2 1 ... 查看数据转换后的前3行数据 1head(credit.df, 3) Age Sex Job Housing Saving.accounts Checking.account Credit.amount Duration Purpose Risk 1 67 male 2 own NA little 1169 6 radio/TV 1 2 22 female 2 own little moderate 5951 48 radio/TV 0 3 49 male 1 own little NA 2096 12 education 1 可以看到数据中含有缺失值NA。 缺失值处理 识别缺失值 查看缺失值总数 1sum(is.na(credit.df)) 1[1] 577 数据集中共有577个缺失值。 查看含有缺失值的实例的总数 1(na.sum &lt;- nrow(credit.df) - sum(complete.cases(credit.df))) 1[1] 478 可以看到，数据集中共有1000条实例，含有缺失值的实例占比高达47.8%。 使用mice包中的md.partten函数查看缺失值的分布形式 1md.pattern(credit.df, plot = T) 123456 Age Sex Job Housing Credit.amount Duration Purpose Risk Saving.accounts Checking.account 522 1 1 1 1 1 1 1 1 1 1 0295 1 1 1 1 1 1 1 1 1 0 184 1 1 1 1 1 1 1 1 0 1 199 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 183 394 577 可以看到，缺失值主要集中在Saving.accounts和Checking.account两列。含有缺失值的实例共有3种模式： 只在Checking.account列含有缺失值的实例，共有295条。 只在Saving.accounts列含有缺失值的实例，共有84条。 同时在Checking.account和Saving.accounts两列都有缺失值的实例，共99条。 所以295 + 84 + 99 = 478与上面计算的含有缺失值的实例的总数一致。 多重插补填补缺失值 使用分类回归树算法"cart"对缺失值进行插补。因为多重插补是使用变量间已知的信息预测缺失的信息，而建模的目的是用自变量预测因变量。如果加入了因变量y进行预测，则相当于首先用因变量y预测缺失值的自变量x，然后在建模时使用填补结果x预测因变量y，由此极易造成误导，所以将Risk列剔除后再进行插补。 1234# 参数m表示插补次数imp &lt;- mice(credit.df[, -10], method = "cart", m = 1)# 查看插补信息summary(imp) 123456789101112131415161718192021222324Class: midsNumber of multiple imputations: 1 Imputation methods: Age Sex Job Housing "" "" "" "" Saving.accounts Checking.account Credit.amount Duration "cart" "cart" "" "" Purpose "" PredictorMatrix: Age Sex Job Housing Saving.accounts Checking.accountAge 0 1 1 1 1 1Sex 1 0 1 1 1 1Job 1 1 0 1 1 1Housing 1 1 1 0 1 1Saving.accounts 1 1 1 1 0 1Checking.account 1 1 1 1 1 0 Credit.amount Duration PurposeAge 1 1 1Sex 1 1 1Job 1 1 1Housing 1 1 1Saving.accounts 1 1 1Checking.account 1 1 1 从输出结果的Imputation methods部分看到，只对含有缺失值的Saving.accounts和Checking.account特征使用"cart"方法进行插补。 获取插补后的数据，并对数据进行整理。 1234credit.df.full &lt;- complete(imp) # 获取插补后的数据credit.df.full$Risk &lt;- credit.df$Risk # 添加插补前被剔除的Risk列# 将Risk特征放在第一列credit.df.full &lt;- credit.df.full[, c("Risk", colnames(credit.df.full[, -10]))] 查看插补后的数据是否还有缺失值。 1sum(is.na(credit.df.full)) 1[1] 0 结果表明数据中已经没有缺失值了。 描述性分析 特征分析 Risk Risk表示信用好坏。因子类型，因子水平：1表示良好，0表示差。 显示Risk的水平频数条形图 1visualize.barchart(credit.df.full$Risk) 从上图中可以看到，该数据集中信用良好的客户占比为70%，信用差的客户占比为30%。 Age Age表示年龄，数值类型。 查看Age描述性统计 1summary(credit.df.full$Age) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 19.00 27.00 33.00 35.55 42.00 75.00 贷款客户的最小年龄为19，最大年龄为75，年龄均值为35.55，年龄中位数为33。 显示Age的直方图和密度图 1visualize.distribution(credit.df.full$Age) 可以看到，Age的密度分布是一个右偏态分布，多数客户的年龄集中在20～50的年龄段中。 当Risk为0时，Age的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 0), "Age"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 19.00 25.00 31.00 33.96 40.00 74.00 当Risk为1时，Age的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 1), "Age"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 19.00 27.00 34.00 36.22 42.25 75.00 显示在Risk分别为0和1时，Age的直方图和密度图 1visualize.distribution.xy(credit.df.full$Age, credit.df.full$Risk) 可以看到，在20～30年龄段的客户中，信用差的客户数量密度要高于信用好的客户数量密度；在大于30年龄段的客户中，信用好的客户数量密度普遍要高于信用差的客户数量密度。这说明，该数据集中20～30年龄段的客户信用违约风险更高。 显示Age、Age与Risk不同水平间的箱线图 1visualize.boxplot(credit.df.full$Age, credit.df.full$Risk) 可以看到，信用差的客户的年龄的中位数要小于信用好的客户的年龄的中位数。 对Age与Risk进行双样本t检验，检验Risk与Age是否有显著差异。首先需要进行方差齐性检验，然后在进行双样本t检验。 方差齐性检验 1var.test(credit.df.full$Age ~ credit.df.full$Risk) 123456789 F test to compare two variancesdata: credit.df.full$Age by credit.df.full$RiskF = 0.97229, num df = 299, denom df = 699, p-value = 0.7843alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 0.8055823 1.1823076sample estimates:ratio of variances 0.9722949 检验的p值0.7843大于显著性水平0.05，所以接受原假设。这说明，在显著性水平0.05的条件下，信用好的客户年龄样本与信用差的客户年龄样本的方差是相同的。 双样本t检验 12# var.equal = TRUE表示方差齐性为真t.test(credit.df.full$Age ~ credit.df.full$Risk, var.equal = TRUE) 123456789 Two Sample t-testdata: credit.df.full$Age by credit.df.full$Riskt = -2.8908, df = 998, p-value = 0.003925alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -3.7957159 -0.7261889sample estimates:mean in group 0 mean in group 1 33.96333 36.22429 检验的p值0.003925小于显著性水平0.05，所以拒绝原假设。这说明，在显著性水平0.05的条件下，信用好的客户年龄样本与信用差的客户年龄样本存在显著性差异。 Sex Sex表是性别，因子类型，因子水平：female，male。 显示Sex水平频数条形图，以及Risk在Sex不同水平下的相应频数。 1visualize.barchart(credit.df.full$Sex, credit.df.full$Risk) 数据集中男性所占百分比为69%，女性所占百分比为31%。 在男性客户中，信用良好的客户数量要远远高于信用差的客户数量；在女性客户中，信用良好的客户数量也高于信用差的客户数量。 对Sex和Risk的建立列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Sex, chisq.test = TRUE) 1234567891011121314151617181920212223242526 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | female | male | Row Total | -------------|-----------|-----------|-----------| 0 | 109 | 191 | 300 | | 0.352 | 0.277 | | -------------|-----------|-----------|-----------| 1 | 201 | 499 | 700 | | 0.648 | 0.723 | | -------------|-----------|-----------|-----------|Column Total | 310 | 690 | 1000 | | 0.310 | 0.690 | | -------------|-----------|-----------|-----------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 5.699147 d.f. = 1 p = 0.01697316 Pearson's Chi-squared test with Yates' continuity correction ------------------------------------------------------------Chi^2 = 5.348516 d.f. = 1 p = 0.02073991 因为检验的p值0.01697316小于显著性水平0.05，所以拒绝Risk和Sex相互独立的假设。这说明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Sex之间存在一定的关联。 Job Job表示客户当前职业状况。因子类型，因子水平：0表示没有职业技能且不是常住居民的人员（unskilled and non-resident）；1表示没有职业技能但为常住居民的人员 （unskilled and resident）；2表示拥有职业技能工作的人员（skilled）； 3表示拥有高级职业技能工作的人员（highly skilled)。 显示Job水平频数条形图，以及Risk在Job不同水平下的相应频数。 1visualize.barchart(credit.df.full$Job, credit.df.full$Risk) 数据集的所有客户中，没有职业技能且不是常住居民的人员占2.2%，没有职业技能但为常住居民的人员20%，没有职业技能但为常住居民的人员占63%，拥有高级职业技能工作的人员占14.8%。可见后三种客户占数据集的所有客户的97.8%。 上述的每类客户中，信用好的客户数量明显要高于信用差的客户数量。在没有职业技能但为常住居民的人员和没有职业技能但为常住居民的人员这两类的客户中，信用好的客户数量要远高于相应同类人员中信用差的人员数量。 对Job和Risk的建立列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Job, chisq.test = TRUE) 12345678910111213141516171819202122 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | 0 | 1 | 2 | 3 | Row Total | -------------|-----------|-----------|-----------|-----------|-----------| 0 | 7 | 56 | 186 | 51 | 300 | | 0.318 | 0.280 | 0.295 | 0.345 | | -------------|-----------|-----------|-----------|-----------|-----------| 1 | 15 | 144 | 444 | 97 | 700 | | 0.682 | 0.720 | 0.705 | 0.655 | | -------------|-----------|-----------|-----------|-----------|-----------|Column Total | 22 | 200 | 630 | 148 | 1000 | | 0.022 | 0.200 | 0.630 | 0.148 | | -------------|-----------|-----------|-----------|-----------|-----------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 1.885156 d.f. = 3 p = 0.5965816 因为检验的p值0.5965816大于显著性水平0.05，所以接受Risk和Job相互独立的假设。这说明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Job相互独立。 Housing Housing表示居住房子所有权。因子类型，因子水平：own，rent，free。 显示Housing水平频数条形图，以及Risk在Housing不同水平下的相应频数。 1visualize.barchart(credit.df.full$Housing, credit.df.full$Risk) 水平"own"的客户数量占所有客户数量的百分比为71.3%；水平"rent"的客户数量占所有客户数量的百分比为17.9%；水平"free"的客户数量占所有客户数量的百分比为10.8%。 在水平为"own"的客户中，信用良好的客户数量要远远高于信用差的客户数量；在水平为"rent"或"free"的客户中，信用良好的客户数量也高于信用差的客户数量，但差距没有水平为"own"的情况明显。 对Housing和Risk建立列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Housing, chisq.test = TRUE) 12345678910111213141516171819202122 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | free | own | rent | Row Total | -------------|-----------|-----------|-----------|-----------| 0 | 44 | 186 | 70 | 300 | | 0.407 | 0.261 | 0.391 | | -------------|-----------|-----------|-----------|-----------| 1 | 64 | 527 | 109 | 700 | | 0.593 | 0.739 | 0.609 | | -------------|-----------|-----------|-----------|-----------|Column Total | 108 | 713 | 179 | 1000 | | 0.108 | 0.713 | 0.179 | | -------------|-----------|-----------|-----------|-----------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 18.19984 d.f. = 2 p = 0.0001116747 因为检验的p值0.0001116747小于显著性水平0.05，所以拒绝Risk和Housing相互独立的假设。这表明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Housing之间存在一定关联。 Saving.accounts Saving.accounts表示储蓄账户存款。因子类型，因子水平：little，moderate，rich，quite rich。 显示Saving.accounts水平频数条形图，以及Risk在Saving.accounts不同水平下的相应频数。 1visualize.barchart(credit.df.full$Saving.accounts, credit.df.full$Risk) 水平"little"的客户数量占所有客户数量的百分比为73.4%；水平"moderate"的客户数量占所有客户数量的百分比为13.2%；水平"quite rich"的客户数量占所有客户数量的百分比为7.8%；水平"rich"的客户数量占所有客户数量的百分比为5.6%。 在水平"little"中，信用良好的客户数量占该水平所有客户数量的67.3%；在"moderate"水平中，信用良好的客户数量占该水平所有客户数量的68.3%；在"quite rich"水平中，信用良好的客户数量占该水平所有客户数量的85.9%；在"rich"水平中，信用良好的客户数量占该水平所有客户数量的87.5%。 对Saving.accounts和Risk建立列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Saving.accounts, chisq.test = TRUE) 12345678910111213141516171819202122 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | little | moderate | quite rich | rich | Row Total | -------------|------------|------------|------------|------------|------------| 0 | 241 | 38 | 13 | 8 | 300 | | 0.328 | 0.292 | 0.171 | 0.136 | | -------------|------------|------------|------------|------------|------------| 1 | 494 | 92 | 63 | 51 | 700 | | 0.672 | 0.708 | 0.829 | 0.864 | | -------------|------------|------------|------------|------------|------------|Column Total | 735 | 130 | 76 | 59 | 1000 | | 0.735 | 0.130 | 0.076 | 0.059 | | -------------|------------|------------|------------|------------|------------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 16.37091 d.f. = 3 p = 0.0009517389 因为检验的p值0.0009517389小于显著性水平0.05，所以拒绝Risk和Saving.accounts相互独立的假设。这表明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Saving.accounts之间存在一定关联。 Checking.account Checking.account表示支票账户。因子类型，因子水平：little，moderate，rich。 查看Checking.accounts水平频数条形图，以及Risk在Checking.account不同水平下的相应频数。 1visualize.barchart(credit.df.full$Checking.account, credit.df.full$Risk) 水平"little"的客户数量占所有客户数量的百分比为42.2%；水平"moderate"的客户数量占所有客户数量的百分比为46.3%；水平"rich"的客户数量占所有客户数量的百分比为11.5%。 在水平"little"中，信用良好的客户数量占该水平所有客户数量的64.7%；在"moderate"水平中，信用良好的客户数量占该水平所有客户数量的71.9%；在"rich"水平中，信用良好的客户数量占该水平所有客户数量的81.7%。 对Checking.account和Risk的列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Checking.account, chisq.test = TRUE) 12345678910111213141516171819202122 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | little | moderate | rich | Row Total | -------------|-----------|-----------|-----------|-----------| 0 | 149 | 130 | 21 | 300 | | 0.353 | 0.281 | 0.183 | | -------------|-----------|-----------|-----------|-----------| 1 | 273 | 333 | 94 | 700 | | 0.647 | 0.719 | 0.817 | | -------------|-----------|-----------|-----------|-----------|Column Total | 422 | 463 | 115 | 1000 | | 0.422 | 0.463 | 0.115 | | -------------|-----------|-----------|-----------|-----------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 14.02318 d.f. = 2 p = 0.0009013754 因为检验的p值0.0009013754小于显著性水平0.05，所以拒绝Risk和Checking.account相互独立的假设。这表明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Checking.account之间存在一定关联。 Credit.amount Credit.amount表示贷款金额。数值类型，单位德国马克。 查看Credit.amount的描述性统计 1summary(credit.df.full$Credit.amount) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 250 1366 2320 3271 3972 18424 该数据集中，最小的贷款金额为250马克，最大的贷款金额为18424马克，贷款金额均值为3271，贷款金额中位数为2320。 显示Credit.amount的直方图和密度图 1visualize.distribution(credit.df.full$Credit.amount) 可以看到，Credit.amount的密度分布是一个右偏态分布，多数客户的贷款金额集中在250～10000之间。 当Risk为0时，Credit.amount的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 0), "Credit.amount"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 433 1352 2574 3938 5142 18424 当Risk为1时，Credit.amount的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 1), "Credit.amount"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 250 1376 2244 2985 3635 15857 显示在Risk分别为0和1时，Credit.amount的直方图和密度图 1visualize.distribution.xy(credit.df.full$Credit.amount, credit.df.full$Risk) 可以看到，信用好的客户主要集中在贷款金额250～5000马克的范围内，而且该范围内的信用好的客户数量密度要高于信用差的客户数量密度；在贷款金额大于5000马克年龄段的客户中，信用差的客户数量密度要高于信用好的客户数量密度。 显示Credit.amount、Credit.amount与Risk不同水平间的箱线图 1visualize.boxplot(credit.df.full$Credit.amount, credit.df.full$Risk) 数据分布的右偏性在箱线图中体现为大量的圆点，而信用差的客户贷款金额的中位数要比信用好的客户的贷款金额的中位数要高。 对Credit.amount与Risk进行双样本t检验，检验Risk与Credit.amount是否有显著差异。首先需要进行方差齐性检验，然后在进行双样本t检验。 方差齐性检验 1var.test(credit.df.full$Credit.amount ~ credit.df.full$Risk) 123456789 F test to compare two variancesdata: credit.df.full$Credit.amount by credit.df.full$RiskF = 2.1678, num df = 299, denom df = 699, p-value &lt; 2.2e-16alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 1.796126 2.636073sample estimates:ratio of variances 2.167828 检验的p值2.2e-16小于显著性水平0.05，所以拒绝原假设。这说明，在显著性水平0.05的条件下，信用好的客户贷款金额样本与信用差的客户贷款金额样本的方差是不同的。 双样本t检验 12# var.equal = FALSE表示方差齐性为假t.test(credit.df.full$Credit.amount ~ credit.df.full$Risk, var.equal = FALSE) 123456789 Welch Two Sample t-testdata: credit.df.full$Credit.amount by credit.df.full$Riskt = 4.2642, df = 421.86, p-value = 2.478e-05alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: 513.534 1391.805sample estimates:mean in group 0 mean in group 1 3938.127 2985.457 检验的p值2.478e-05小于显著性水平0.05，所以拒绝原假设。这说明，在显著性水平0.05的条件下，信用好的客户贷款金额样本与信用差的客户贷款金额样本存在显著性差异。 Duration Duration表示还款期限或贷款时间。数值类型，单位月。 查看Duration的描述性统计 1summary(credit.df.full$Duration) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 4 12 18 21 24 72 显示Duration的直方图和密度图 1visualize.distribution(credit.df.full$Duration) 可以看到，Credit.amount的密度分布是一个多峰分布。 当Risk为0时，Duration的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 0), "Duration"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 6.00 12.00 24.00 24.86 36.00 72.00 当Risk为1时，Duration的描述性统计 1summary(credit.df.full[which(credit.df.full$Risk == 1), "Duration"]) 12Min. 1st Qu. Median Mean 3rd Qu. Max. 4.00 12.00 18.00 19.21 24.00 60.00 显示在Risk分别为0和1时，Duration的直方图和密度图 1visualize.distribution.xy(credit.df.full$Duration, credit.df.full$Risk) 在贷款时间为4～30个月范围内的信用好的客户数量密度要高于信用差的客户数量密度；在贷款时间大于30个月范围内的客户中，信用差的客户数量密度要高于信用好的客户数量密度。 显示Duration、Duration与Risk不同水平间的箱线图 1visualize.boxplot(credit.df.full$Duration, credit.df.full$Risk) 信用差的客户贷款时间的中位数要比信用好的客户的贷款时间的中位数要高。 对Duration与Risk进行双样本t检验，检验Risk与Duration是否有显著差异。首先需要进行方差齐性检验，然后在进行双样本t检验。 方差齐性检验 1var.test(credit.df.full$Duration ~ credit.df.full$Risk) 123456789 F test to compare two variancesdata: credit.df.full$Duration by credit.df.full$RiskF = 1.4372, num df = 299, denom df = 699, p-value = 0.0001416alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 1.190790 1.747656sample estimates:ratio of variances 1.43722 检验的p值0.0001416小于显著性水平0.05，所以拒绝原假设。这说明，在显著性水平0.05的条件下，信用好的客户贷款期限样本与信用差的客户贷款期限样本的方差是不同的。 双样本t检验 12# var.equal = FALSE表示方差齐性为假t.test(credit.df.full$Duration ~ credit.df.full$Risk, var.equal = FALSE) 123456789 Welch Two Sample t-testdata: credit.df.full$Duration by credit.df.full$Riskt = 6.4696, df = 485.44, p-value = 2.404e-10alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: 3.936033 7.369682sample estimates:mean in group 0 mean in group 1 24.86000 19.20714 检验的p值2.404e-10小于显著性水平0.05，所以拒绝原假设。这说明，在显著性水平0.05的条件下，信用好的客户贷款期限样本与信用差的客户贷款期限样本存在显著性差异。 Purpose Purpose表示贷款目的。因子类型，因子水平：car，furniture/equipment，radio/TV，domestic appliances, repairs，education，business，vacation/others。 显示Purpose水平频数条形图，以及Risk在Purpose不同水平下的相应频数。 1visualize.barchart(credit.df.full$Purpose, credit.df.full$Risk) 水平 客户数量占所有客户数量的百分比（%） business 9.7 car 33.7 domestic appliances 1.2 education 5.9 furniture/equipment 18.2 radio/TV 28 repairs 2.2 vacation/others 1.2 水平 信用良好的客户数量占所有客户数量百分比（%） business 64.9 car 68.5 domestic appliances 66.7 education 61 furniture/equipment 68 radio/TV 77.9 repairs 63.6 vacation/others 58.3 对Purpose和Risk建立列联表，并使用卡方检验进行独立性检验。 1get.contingency.table(credit.df.full$Risk, credit.df.full$Purpose, chisq.test = TRUE) 12345678910111213141516171819202122 Cell Contents|-------------------------|| N || N / Col Total ||-------------------------|Total Observations in Table: 1000 | x.feature y.feature | business | car | domestic appliances | education | furniture/equipment | radio/TV | repairs | vacation/others | Row Total | -------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------| 0 | 34 | 106 | 4 | 23 | 58 | 62 | 8 | 5 | 300 | | 0.351 | 0.315 | 0.333 | 0.390 | 0.320 | 0.221 | 0.364 | 0.417 | | -------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------| 1 | 63 | 231 | 8 | 36 | 123 | 218 | 14 | 7 | 700 | | 0.649 | 0.685 | 0.667 | 0.610 | 0.680 | 0.779 | 0.636 | 0.583 | | -------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|Column Total | 97 | 337 | 12 | 59 | 181 | 280 | 22 | 12 | 1000 | | 0.097 | 0.337 | 0.012 | 0.059 | 0.181 | 0.280 | 0.022 | 0.012 | | -------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|Statistics for All Table FactorsPearson's Chi-squared test ------------------------------------------------------------Chi^2 = 13.64209 d.f. = 7 p = 0.05792591 因为检验的p值0.05792591大于显著性水平0.05，所以接受Risk和Job相互独立的假设。这说明，如果不受其它因素影响，在显著性水平为0.05的情况下，Risk和Purpose相互独立。 结论 Risk与Age、Sex、Housing、Saving.accounts、Checking.account、Credit.amount、Duration存在一定的关联。 Risk与Job、Purpose相互独立。]]></content>
      <categories>
        <category>信用风险分析</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用风险预测-SVM]]></title>
    <url>%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B-svm%2F</url>
    <content type="text"><![CDATA[信用风险预测-SVM 预测工具准备 需要用到的R package package 描述 car 共线性检查 ggplot2 数据可视化 caret 分类和回归训练 e1071 SVM模型 kernlab SVM模型的分类超平面 ROCR ROC曲线 分析开始时，可先将所需要的package安装并加载到R环境中。 所使用的自定义的工具函数 本文所使用的所有自定义的工具函数都放在名为"svm_utils.R"的文件中。分析开始时，可先将该文件加载到R环境中。假设文件"svm_utils.R"在当前的工作环境目录下，可使用source函数加载该文件内容。 12# 加载svm_utils.Rsource("svm_utils.R") svm_utils.R 数据处理 数据来源 本文将使用信用风险分析里处理过后的数据进行建模预测和模型评估。 原始数据来自kaggle：kaggle.com/kabure/german-credit-data-with-risk 标准化和归一化 对数据集中的特征Age、Credit.amount、Duration进行标准化和归一化。 12numeric.vars &lt;- c("Duration", "Age", "Credit.amount")credit.df.full &lt;- scale.features(credit.df.full, numeric.vars) 划分数据集 按照信用风险变量进行等比例抽样划分训练集和测试集。 1(prop.table(table(credit.df.full$Risk))) 12 0 1 0.3 0.7 信用风险为差的比例为0.3，信用风险为好的比例为0.7，所以训练集和测试集中信用风险差和好的比例也分别为0.3、0.7。 1234# 等比例划分训练集和测试集idx &lt;- createDataPartition(credit.df.full$Risk, times = 1, p = 0.7, list = FALSE)train.data &lt;- credit.df.full[idx, ] # 训练集test.data &lt;- credit.df.full[-idx, ] # 测试集 1(prop.table(table(train.data$Risk))) # 查看训练集信用风险比例 12 0 1 0.3 0.7 1(prop.table(table(test.data$Risk))) # 查看测试集信用风险比例 12 0 1 0.3 0.7 建立模型与评估模型 使用所有特征的SVM模型 使用训练集的所有特征创建SVM回归模型，并查看模型信息。 1234567# kernel = "radial"表示使用的是径向基核函数svm.model.all &lt;- svm( formula = as.formula("Risk~."), data = train.data, kernel = "radial", type = 'C-classification')# 查看模型信息summary(svm.model.all) 12345678910111213Call:svm(formula = as.formula("Risk~."), data = train.data, kernel = "radial", type = "C-classification")Parameters: SVM-Type: C-classification SVM-Kernel: radial cost: 1 gamma: 0.04545455 Number of Support Vectors: 450 ( 240 210 )Number of Classes: 2 Levels: 0 1 对测试集进行预测，并查看预测结果。 123456789# 对测试集的目标特征进行预测svm.all.predictions &lt;- predict( svm.model.all, test.feature, decision = TRUE)# 使用预测值与测试集目标特征的观测值进行建立混淆矩阵和相关分类指标(svm.all.conf.matrix &lt;- confusionMatrix( data = svm.all.predictions, reference = test.feature.target, positive = "1")) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 8 7 1 82 203 Accuracy : 0.7033 95% CI : (0.6481, 0.7545) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.4782 Kappa : 0.0729 Mcnemar's Test P-Value : 4.365e-15 Sensitivity : 0.96667 Specificity : 0.08889 Pos Pred Value : 0.71228 Neg Pred Value : 0.53333 Prevalence : 0.70000 Detection Rate : 0.67667 Detection Prevalence : 0.95000 Balanced Accuracy : 0.52778 'Positive' Class : 1 计算F1 score 1calc.f1.score(svm.all.conf.matrix) 1[1] 0.8202 预测的结果的Accuracy（准确度）为0.7033，Sensitivity（灵敏度）为0.9667，Specificity（特异性）为0.0889，F1 score为0.8202。这意味着这是一个十分激进的模型，它几乎将每位客户的信用风险都预测为好的。 交叉验证 使用K = 10折交叉验证进行分析，然后根据特征的重要性对特征进行选择，再使用选择后的特征建立模型。 12345control &lt;- trainControl(method = "repeatedcv", number = 10, repeats = 2)svm.model.cv &lt;- train( Risk~., data = train.data, method = "svmRadial", trControl = control) 查看特征的重要性 12svm.importance.cv &lt;- varImp(svm.model.cv, scale = FALSE)plot(svm.importance.cv) 选择前5个特征建立模型 12345678svm.formula.imp &lt;- as.formula("Risk ~ Duration + Age + Sex + Credit.amount + Checking.account")svm.model.imp &lt;- svm( formula = svm.formula.imp, data = train.data, kernel = "radial", type = 'C-classification')# 查看模型信息summary(svm.model.imp) 12345678910111213Call:svm(formula = svm.formula.imp, data = train.data, kernel = "radial", type = "C-classification")Parameters: SVM-Type: C-classification SVM-Kernel: radial cost: 1 gamma: 0.1428571 Number of Support Vectors: 430 ( 223 207 )Number of Classes: 2 Levels: 0 1 对测试集进行预测，并查看预测结果。 1234567svm.imp.predictions &lt;- predict( svm.model.imp, test.feature, decision = TRUE)(svm.imp.conf.matrix &lt;- confusionMatrix( data = svm.imp.predictions, reference = test.feature.target, positive = "1")) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 11 8 1 79 202 Accuracy : 0.71 95% CI : (0.6551, 0.7607) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.3793 Kappa : 0.1086 Mcnemar's Test P-Value : 6.153e-14 Sensitivity : 0.9619 Specificity : 0.1222 Pos Pred Value : 0.7189 Neg Pred Value : 0.5789 Prevalence : 0.7000 Detection Rate : 0.6733 Detection Prevalence : 0.9367 Balanced Accuracy : 0.5421 'Positive' Class : 1 计算F1 score 1calc.f1.score(svm.imp.conf.matrix) 1[1] 0.8228 预测的结果的准确度为0.71，灵敏度为0.9619，特异性为0.1222，F1 score为0.8228。这说明模型能够很好地预测信用风险好的客户，但对信用风险差的客户的预测效果并不理想。 网格搜索 使用网格搜索对交叉验证和特征选择后的模型进行参数调优，调优参数有惩罚系数cost和核函数参数gamma。 1234567cost.weights &lt;- c(0.1, 10, 100)gamma.weights &lt;- c(0.01, 0.05, 0.14, 0.25, 0.5, 1)(tuning.results &lt;- tune( svm, svm.formula.imp, data = train.data, kernel = "radial", ranges = list(cost = cost.weights, gamma = gamma.weights))) 123456Parameter tuning of ‘svm’:- sampling method: 10-fold cross validation - best parameters: cost gamma 100 0.01- best performance: 0.2914286 得到调优后的参数cost为100，gamma为0.01。 使用调优参数后的模型对测试集进行测试 123svm.model.tuning &lt;- tuning.results$best.model# 查看模型信息summary(svm.model.tuning) 1234567891011121314Call:best.tune(method = svm, train.x = svm.formula.imp, data = train.data, ranges = list(cost = cost.weights, gamma = gamma.weights), kernel = "radial")Parameters: SVM-Type: C-classification SVM-Kernel: radial cost: 100 gamma: 0.14 Number of Support Vectors: 406 ( 217 189 )Number of Classes: 2 Levels: 0 1 1234567svm.tuning.predictions &lt;- predict( svm.model.tuning, test.feature, decision = T)(svm.tuning.conf.matrix &lt;- confusionMatrix( data = svm.tuning.predictions, reference = test.feature.target, positive = "1")) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 20 16 1 70 194 Accuracy : 0.7133 95% CI : (0.6586, 0.7638) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.3321 Kappa : 0.1762 Mcnemar's Test P-Value : 1.096e-08 Sensitivity : 0.9238 Specificity : 0.2222 Pos Pred Value : 0.7348 Neg Pred Value : 0.5556 Prevalence : 0.7000 Detection Rate : 0.6467 Detection Prevalence : 0.8800 Balanced Accuracy : 0.5730 'Positive' Class : 1 计算F1 score 1calc.f1.score(svm.tuning.conf.matrix) 1[1] 0.8186 预测的结果的准确度为0.7133，灵敏度为0.9238，特异性为0.2222，F1 score为0。8186。这说明模型能够很好地预测信用风险好的客户，但对信用风险差的客户预测效果并不理想。 模型选择 绘制ROC曲线，比较三个模型的AUC。 ROC曲线 使用所有特征的SVM模型的ROC曲线 12345678svm.all.values &lt;- attributes(svm.all.predictions)$decision.valuessvm.all.pred &lt;- prediction( predictions = svm.all.values, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = svm.all.pred, title.text = "SVM ROC曲线")plot.pr.curve(predictions = svm.all.pred, title.text = "SVM Precision/Recall曲线") 交叉验证特征选择的SVM模型的ROC曲线 12345678svm.imp.values &lt;- attributes(svm.imp.predictions)$decision.valuessvm.imp.pred &lt;- prediction( predictions = svm.imp.values, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = svm.imp.pred, title.text = "SVM ROC曲线")plot.pr.curve(predictions = svm.imp.pred, title.text = "SVM Precision/Recall曲线") 对特征选择进行参数优化后的SVM模型的ROC曲线 12345678svm.tuning.values &lt;- attributes(svm.tuning.predictions)$decision.valuessvm.tuning.pred &lt;- prediction( predictions = svm.tuning.values, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = svm.tuning.pred, title.text = "SVM ROC曲线")plot.pr.curve(predictions = svm.tuning.pred, title.text = "SVM Precision/Recall曲线") 三个模型的ROC曲线比较 12345678910111213141516pred.list &lt;- list( "all" = svm.all.predictions, "imp" = svm.imp.predictions, "tuning" = svm.tuning.predictions)for (i in seq(1, length(pred.list))) &#123; pred.values &lt;- attributes(pred.list[[i]])$decision.values pred &lt;- prediction(predictions = pred.values, labels = test.feature.target) perf &lt;- performance(pred, "tpr", "fpr") if (i == 1) &#123; plot.act(perf, main = "ROC tpr/fpr", col = i, family = NULL) &#125; else &#123; plot.act(perf, add = TRUE, col = i, family = NULL) &#125;&#125;legend(0.6, 0.6, names(pred.list), seq(1, length(pred.list))) 模型评估指标 模型 准确度 灵敏度 特异性 F1 score AUC值 所有特征的SVM模型 0.7033 0.9667 0.0889 0.8202 0.66 特征选择的SVM模型 0.71 0.9619 0.1222 0.8228 0.6 对特征选择进行参数优化的SVM模型 0.7133 0.9238 0.2222 0.8186 0.61 至于选择哪个模型用于预测，并不完全依赖于模型的准确度，而是根据研究领域和业务需求来决定。如果模型将一个信用评级差的客户预测为信用评级好的客户，这意味着银行将批准一名最终违约的客户的贷款申请，这将导致银行的损失。但是，如果模型将一个信用评级好的客户预测为信用评级差的客户，这意味着银行将否决该客户的的贷款申请，银行将既不会盈利也不会有所损失。所以基于损失最小化，利益最大化的原则，这里将选用特异性较大的模型，即对特征选择进行参数优化的SVM模型。]]></content>
      <categories>
        <category>信用风险分析</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用风险预测-决策树]]></title>
    <url>%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[信用风险预测-决策树 预测工具准备 需要用到的R package package 描述 rpart 树模型 rpart.plot 树模型可视化 ggplot2 数据可视化 caret 分类和回归训练 ROCR ROC曲线 分析开始时，可先将所需要的package安装并加载到R环境中。 所使用的自定义的工具函数 本文所使用的所有自定义的工具函数都放在名为"dt_utils.R"的文件中。分析开始时，可先将该文件加载到R环境中。假设文件"dt_utils.R"在当前的工作环境目录下，可使用source函数加载该文件内容。 12# 加载dt_utils.Rsource("dt_utils.R") dt_utils.R 数据处理 数据来源 本文将使用信用风险分析里处理过后的数据进行建模预测和模型评估。 原始数据来自kaggle：kaggle.com/kabure/german-credit-data-with-risk 标准化和归一化 对数据集中的特征Age、Credit.amount、Duration进行标准化和归一化。 12numeric.vars &lt;- c("Duration", "Age", "Credit.amount")credit.df.full &lt;- scale.features(credit.df.full, numeric.vars) 划分数据集 按照信用风险变量进行等比例抽样划分训练集和测试集。 1(prop.table(table(credit.df.full$Risk))) 12 0 1 0.3 0.7 信用风险为差的比例为0.3，信用风险为好的比例为0.7，所以训练集和测试集中信用风险差和好的比例也分别为0.3、0.7。 1234# 等比例划分训练集和测试集idx &lt;- createDataPartition(credit.df.full$Risk, times = 1, p = 0.7, list = FALSE)train.data &lt;- credit.df.full[idx, ] # 训练集test.data &lt;- credit.df.full[-idx, ] # 测试集 1(prop.table(table(train.data$Risk))) # 查看训练集信用风险比例 12 0 1 0.3 0.7 1(prop.table(table(test.data$Risk))) # 查看测试集信用风险比例 12 0 1 0.3 0.7 建立模型与评估模型 划分测试特征和目标特征 1234# 测试特征test.feature &lt;- test.data[, -1]# 测试的目标特征test.feature.target &lt;- test.data[, 1] 决策树模型 使用训练集的所有特征创建决策树模型，并查看模型信息。 1234567dt.model.all &lt;- rpart( formula = as.formula("Risk ~ ."), method = "class", data = train.data, control = rpart.control(cp = 0.05))# 查看模型信息summary(dt.model.all) 123456789101112Call:rpart(formula = as.formula("Risk ~ ."), data = train.data, method = "class", control = rpart.control(cp = 0.05)) n= 700 CP nsplit rel error xerror xstd1 0.03095238 0 1 0 0Node number 1: 700 observations predicted class=1 expected loss=0.3 P(node) =1 class counts: 210 490 probabilities: 0.300 0.700 使用测试集进行预测，并查看预测结果。 1234567dt.predictions.all &lt;- predict( dt.model.all, test.feature, type = "prob")(dt.all.conf.matrix &lt;- confusionMatrix( as.factor(round(dt.predictions.all[, 2])), test.feature.target, positive = "1")) 12345678910111213141516171819202122232425262728Confusion Matrix and Statistics ReferencePrediction 0 1 0 0 0 1 90 210 Accuracy : 0.7 95% CI : (0.6447, 0.7513) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.5284 Kappa : 0 Mcnemar's Test P-Value : &lt;2e-16 Sensitivity : 1.0 Specificity : 0.0 Pos Pred Value : 0.7 Neg Pred Value : NaN Prevalence : 0.7 Detection Rate : 0.7 Detection Prevalence : 1.0 Balanced Accuracy : 0.5 'Positive' Class : 1 [1] 0.8235 这是一个过于激进的模型，将所有的客户都预测称信用风险好的客户。 交叉验证 使用K = 10折交叉验证进行分析，并调整类别权重。 12345678dt.model.cv &lt;- rpart( formula = as.formula("Risk ~ ."), method = "class", data = train.data, control = rpart.control(cp = 0.05, xval = 10), parms = list(prior = c(0.5, 0.5)))# 查看模型信息summary(dt.model.cv) 12345678910111213141516171819202122232425262728293031323334353637Call:rpart(formula = as.formula("Risk ~ ."), data = train.data, method = "class", parms = list(prior = c(0.5, 0.5)), control = rpart.control(cp = 0.05, xval = 10)) n= 700 CP nsplit rel error xerror xstd1 0.1659864 0 1.0000000 1.1156463 0.042514092 0.0500000 1 0.8340136 0.8707483 0.04850215Variable importance Duration Credit.amount 81 19 Node number 1: 700 observations, complexity param=0.1659864 predicted class=0 expected loss=0.5 P(node) =1 class counts: 210 490 probabilities: 0.500 0.500 left son=2 (122 obs) right son=3 (578 obs) Primary splits: Duration &lt; 0.8787763 to the right, improve=14.660940, (0 missing) Credit.amount &lt; 1.042514 to the right, improve= 9.998117, (0 missing) Saving.accounts splits as LLRR, improve= 8.421415, (0 missing) Housing splits as LRL, improve= 7.774902, (0 missing) Age &lt; -0.8831285 to the left, improve= 6.017898, (0 missing) Surrogate splits: Credit.amount &lt; 1.278278 to the right, agree=0.866, adj=0.23, (0 split)Node number 2: 122 observations predicted class=0 expected loss=0.3 P(node) =0.207483 class counts: 61 61 probabilities: 0.700 0.300 Node number 3: 578 observations predicted class=1 expected loss=0.4476395 P(node) =0.792517 class counts: 149 429 probabilities: 0.448 0.552 可视化该决策树模型 1prp(dt.model.cv, type = 1, extra = 3, varlen = 0, faclen = 0) 对测试集进行预测，并查看预测结果。 123456dt.predictions.cv &lt;- predict(dt.model.cv, test.feature, type = "prob")(dt.imp.conf.matrix &lt;- confusionMatrix( data = as.factor(round(dt.predictions.cv[, 2])), reference = test.feature.target, positive = "1")) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 22 29 1 68 181 Accuracy : 0.6767 95% CI : (0.6205, 0.7293) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.8278928 Kappa : 0.1214 Mcnemar's Test P-Value : 0.0001142 Sensitivity : 0.8619 Specificity : 0.2444 Pos Pred Value : 0.7269 Neg Pred Value : 0.4314 Prevalence : 0.7000 Detection Rate : 0.6033 Detection Prevalence : 0.8300 Balanced Accuracy : 0.5532 'Positive' Class : 1 计算F1 score 1calc.f1.score(dt.cv.conf.matrix) 1[1] 0.7887 预测的结果的准确度为0.6767，灵敏度为0.8619，特异性为0.2444，F1 score为0.7887。这说明模型能够很好地预测信用风险好的客户，但对信用风险差的客户的预测效果并不理想。 模型选择 绘制ROC曲线，比较两个模型的AUC。 ROC曲线 使用所有特征的决策树模型的ROC曲线 123456dt.all.pred &lt;- prediction( dt.predictions.all[, 2], test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(dt.all.pred, title.text = "决策树ROC曲线")plot.pr.curve(dt.all.pred, title.text = "决策树Precision/Recall曲线") 交叉验证并调整分类权重决策树模型的ROC曲线 123456dt.cv.pred &lt;- prediction( dt.predictions.cv[, 2], test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(dt.cv.pred, title.text = "决策树ROC曲线")plot.pr.curve(dt.cv.pred, title.text = "决策树Precision/Recall曲线") 模型评估指标 模型 准确度 灵敏度 特异性 F1 score AUC值 所有特征的决策树模型 0.7 1 0 0.8235 0.5 交叉验证调整分类权重决策树模型 0.6767 0.8619 0.2444 0.7887 0.55 至于选择哪个模型用于预测，并不完全依赖于模型的准确度，而是根据研究领域和业务需求来决定。如果模型将一个信用评级差的客户预测为信用评级好的客户，这意味着银行将批准一名最终违约的客户的贷款申请，这将导致银行的损失。但是，如果模型将一个信用评级好的客户预测为信用评级差的客户，这意味着银行将否决该客户的的贷款申请，银行将既不会盈利也不会有所损失。所以基于损失最小化，利益最大化的原则，这里将选用特异性较大的模型，即交叉验证调整分类权重决策树模型。]]></content>
      <categories>
        <category>信用风险分析</category>
      </categories>
      <tags>
        <tag>R语言</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用风险预测-逻辑回归]]></title>
    <url>%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90%2F%E4%BF%A1%E7%94%A8%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[信用风险预测-逻辑回归 预测工具准备 需要用到的R package package 描述 car 共线性检查 ggplot2 数据可视化 caret 分类和回归训练 ROCR ROC曲线 分析开始时，可先将所需要的package安装并加载到R环境中。 所使用的自定义的工具函数 本文所使用的所有自定义的工具函数都放在名为"lr_utils.R"的文件中。分析开始时，可先将该文件加载到R环境中。假设文件"lr_utils.R"在当前的工作环境目录下，可使用source函数加载该文件内容。 12# 加载lr_utils.Rsource("lr_utils.R") lr_utils.R 数据处理 数据来源 本文将使用信用风险分析里处理过后的数据进行建模预测和模型评估。 原始数据来自kaggle：kaggle.com/kabure/german-credit-data-with-risk 标准化和归一化 对数据集中的特征Age、Credit.amount、Duration进行标准化和归一化。 12numeric.vars &lt;- c("Duration", "Age", "Credit.amount")credit.df.full &lt;- scale.features(credit.df.full, numeric.vars) 划分数据集 按照信用风险变量进行等比例抽样划分训练集和测试集。 1(prop.table(table(credit.df.full$Risk))) 12 0 1 0.3 0.7 信用风险为差的比例为0.3，信用风险为好的比例为0.7，所以训练集和测试集中信用风险差和好的比例也分别为0.3、0.7。 1234# 等比例划分训练集和测试集idx &lt;- createDataPartition(credit.df.full$Risk, times = 1, p = 0.7, list = FALSE)train.data &lt;- credit.df.full[idx, ] # 训练集test.data &lt;- credit.df.full[-idx, ] # 测试集 1(prop.table(table(train.data$Risk))) # 查看训练集信用风险比例 12 0 1 0.3 0.7 1(prop.table(table(test.data$Risk))) # 查看测试集信用风险比例 12 0 1 0.3 0.7 建立模型与评估模型 划分测试特征和目标特征 1234# 测试特征test.feature &lt;- test.data[, -1]# 测试的目标特征test.feature.target &lt;- test.data[, 1] 使用所有特征的逻辑回归模型 使用训练集的所有特征创建逻辑回归模型，并查看模型信息。 1234# 创建逻辑回归模型lr.model.all &lt;- glm("Risk~.", data = train.data, family = "binomial")# 查看逻辑回归模型结果summary(lr.model.all) 1234567891011121314151617181920212223242526272829303132333435363738394041Call:glm(formula = "Risk~.", family = "binomial", data = train.data)Deviance Residuals: Min 1Q Median 3Q Max -2.3388 -1.0675 0.6186 0.8344 1.5892 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.02972 0.73642 0.040 0.9678 Age 0.17065 0.10182 1.676 0.0937 . Sexmale 0.50821 0.19902 2.553 0.0107 * Job1 0.07403 0.63453 0.117 0.9071 Job2 0.04020 0.61495 0.065 0.9479 Job3 -0.19141 0.65337 -0.293 0.7696 Housingown 0.45384 0.29422 1.543 0.1229 Housingrent 0.03935 0.35654 0.110 0.9121 Saving.accountsmoderate 0.14407 0.26978 0.534 0.5933 Saving.accountsquite rich 0.87966 0.39587 2.222 0.0263 * Saving.accountsrich 1.39033 0.55443 2.508 0.0122 * Checking.accountmoderate 0.04129 0.19557 0.211 0.8328 Checking.accountrich 0.60678 0.34522 1.758 0.0788 . Credit.amount 0.08047 0.11777 0.683 0.4944 Duration -0.53843 0.11354 -4.742 2.12e-06 ***Purposecar -0.19286 0.33430 -0.577 0.5640 Purposedomestic appliances -0.10257 0.95233 -0.108 0.9142 Purposeeducation -0.26250 0.45785 -0.573 0.5664 Purposefurniture/equipment -0.07121 0.36564 -0.195 0.8456 Purposeradio/TV 0.38793 0.34711 1.118 0.2637 Purposerepairs -0.39103 0.61230 -0.639 0.5231 Purposevacation/others 0.17053 0.84842 0.201 0.8407 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 855.21 on 699 degrees of freedomResidual deviance: 769.05 on 678 degrees of freedomAIC: 813.05Number of Fisher Scoring iterations: 4 使用测试集进行预测，并查看预测结果。 1234567# 对测试集的目标特征进行预测lr.predictions.all &lt;- predict(lr.model.all, newdata = test.feature, type = "response")# 使用预测值与测试集目标特征的观测值进行建立混淆矩阵和相关分类指标(lr.all.conf.matrix &lt;- confusionMatrix( data = as.factor(round(lr.predictions.all)), reference = test.feature.target, positive = '1')) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 17 19 1 73 191 Accuracy : 0.6933 95% CI : (0.6378, 0.745) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.6264 Kappa : 0.1188 Mcnemar's Test P-Value : 3.283e-08 Sensitivity : 0.9095 Specificity : 0.1889 Pos Pred Value : 0.7235 Neg Pred Value : 0.4722 Prevalence : 0.7000 Detection Rate : 0.6367 Detection Prevalence : 0.8800 Balanced Accuracy : 0.5492 'Positive' Class : 1 计算F1 score 1calc.f1.score(lr.all.conf.matrix) 1[1] 0.8059 预测的结果的Accuracy（准确度）为0.6933，Sensitivity（灵敏度）为0.9095，Specificity（特异性）为0.1889，F1 score为0.8059。这说明这个模型能够很好地预测信用风险好的客户，但对信用风险差的客户预测效果并不理想。 多重共线性诊断 使用方差扩大因子VIF进行共线性诊断。如果VIF &gt; 10，说明模型中有很强的共线性 1vif(lr.model.all) 12345678910 GVIF Df GVIF^(1/(2*Df))Age 1.252104 1 1.118974Sex 1.147869 1 1.071387Job 1.391610 3 1.056622Housing 1.408490 2 1.089403Saving.accounts 1.153972 3 1.024155Checking.account 1.206094 2 1.047961Credit.amount 1.934793 1 1.390968Duration 1.761424 1 1.327187Purpose 1.420764 7 1.025403 从结果中看到，上述特征的VIF值都比较小，没有特别明显的共线性。 逐步回归 根据AIC准则，选择特征时，优先考虑使AIC值最小的模型。 12# 将使用所有特征建立的逻辑回归模型进行逐步回归lr.model.step &lt;- step(lr.model.all) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Start: AIC=813.05Risk ~ Age + Sex + Job + Housing + Saving.accounts + Checking.account + Credit.amount + Duration + Purpose Df Deviance AIC- Purpose 7 776.26 806.26- Job 3 769.92 807.92- Credit.amount 1 769.52 811.52- Checking.account 2 772.47 812.47&lt;none&gt; 769.05 813.05- Housing 2 773.85 813.85- Age 1 771.93 813.93- Sex 1 775.52 817.52- Saving.accounts 3 782.16 820.16- Duration 1 792.50 834.50Step: AIC=806.26Risk ~ Age + Sex + Job + Housing + Saving.accounts + Checking.account + Credit.amount + Duration Df Deviance AIC- Job 3 777.14 801.14- Credit.amount 1 776.35 804.35&lt;none&gt; 776.26 806.26- Age 1 778.79 806.79- Checking.account 2 781.33 807.33- Housing 2 782.43 808.43- Sex 1 783.31 811.31- Saving.accounts 3 789.34 813.34- Duration 1 797.24 825.24Step: AIC=801.14Risk ~ Age + Sex + Housing + Saving.accounts + Checking.account + Credit.amount + Duration Df Deviance AIC- Credit.amount 1 777.14 799.14&lt;none&gt; 777.14 801.14- Age 1 779.43 801.43- Checking.account 2 782.43 802.43- Housing 2 783.63 803.63- Sex 1 784.31 806.31- Saving.accounts 3 790.66 808.66- Duration 1 797.99 819.99Step: AIC=799.14Risk ~ Age + Sex + Housing + Saving.accounts + Checking.account + Duration Df Deviance AIC&lt;none&gt; 777.14 799.14- Age 1 779.47 799.47- Checking.account 2 782.45 800.45- Housing 2 783.74 801.74- Sex 1 784.34 804.34- Saving.accounts 3 790.66 806.66- Duration 1 808.29 828.29 从结果中看到，逐步回归共进行了4步，AIC值由起始的813.05减小至799.14，依次剔除的特征为：Purpose、Job、Credit.amount。最后保留6个特征：Checking.account、Age、Housing、Saving.accounts、Sex、Duration，用于建立模型。 12# 查看逐步回归信息summary(lr.model.step) 12345678910111213141516171819202122232425262728293031Call:glm(formula = Risk ~ Age + Sex + Housing + Saving.accounts + Checking.account + Duration, family = "binomial", data = train.data)Deviance Residuals: Min 1Q Median 3Q Max -2.4675 -1.0850 0.6353 0.8286 1.7087 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.10879 0.30954 -0.351 0.72524 Age 0.14737 0.09781 1.507 0.13190 Sexmale 0.52298 0.19410 2.694 0.00705 ** Housingown 0.56502 0.28111 2.010 0.04444 * Housingrent 0.13639 0.34600 0.394 0.69345 Saving.accountsmoderate 0.12438 0.26651 0.467 0.64072 Saving.accountsquite rich 0.91491 0.39279 2.329 0.01985 * Saving.accountsrich 1.36635 0.55170 2.477 0.01326 * Checking.accountmoderate 0.08142 0.19090 0.427 0.66973 Checking.accountrich 0.73986 0.33858 2.185 0.02888 * Duration -0.48746 0.08928 -5.460 4.76e-08 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 855.21 on 699 degrees of freedomResidual deviance: 777.14 on 689 degrees of freedomAIC: 799.14Number of Fisher Scoring iterations: 4 使用逐步回归得到的模型对测试集的目标特征进行预测，并查看预测结果。 1234567# 对测试集的目标特征进行预测lr.predictions.step &lt;- predict(lr.model.step, newdata = test.feature, type = "response")# 使用预测值与测试集目标特征的观测值进行建立混淆矩阵和相关分类指标(lr.step.conf.matrix &lt;- confusionMatrix( data = as.factor(round(lr.predictions.step)), reference = test.feature.target, positive = '1')) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 18 22 1 72 188 Accuracy : 0.6867 95% CI : (0.6309, 0.7387) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.7165 Kappa : 0.1132 Mcnemar's Test P-Value : 4.327e-07 Sensitivity : 0.8952 Specificity : 0.2000 Pos Pred Value : 0.7231 Neg Pred Value : 0.4500 Prevalence : 0.7000 Detection Rate : 0.6267 Detection Prevalence : 0.8667 Balanced Accuracy : 0.5476 'Positive' Class : 1 计算F1 score 1calc.f1.score(lr.step.conf.matrix) 1[1] 0.8 预测的结果的准确度为0.6867，灵敏度为0.8952，特异性为0.2，F1 score为0.8。该模型也能够很好地预测信用评级良好的客户，但对信用评级差的客户预测效果并不理想。 交叉验证 使用K = 10折交叉验证进行分析，然后根据特征的重要性对特征进行选择。 1234# 模型训练控制参数，使用重复2次10折交叉验证control &lt;- trainControl(method = "repeatedcv", number = 10, repeats = 2)# 使用控制参数训练模型lr.model.cv &lt;- train(Risk~., data = train.data, method = "glm", trControl = control) 查看交叉验证后的模型信息 1summary(lr.model.cv) 1234567891011121314151617181920212223242526272829303132333435363738394041Call:NULLDeviance Residuals: Min 1Q Median 3Q Max -2.3388 -1.0675 0.6186 0.8344 1.5892 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.02972 0.73642 0.040 0.9678 Age 0.17065 0.10182 1.676 0.0937 . Sexmale 0.50821 0.19902 2.553 0.0107 * Job1 0.07403 0.63453 0.117 0.9071 Job2 0.04020 0.61495 0.065 0.9479 Job3 -0.19141 0.65337 -0.293 0.7696 Housingown 0.45384 0.29422 1.543 0.1229 Housingrent 0.03935 0.35654 0.110 0.9121 Saving.accountsmoderate 0.14407 0.26978 0.534 0.5933 `Saving.accountsquite rich` 0.87966 0.39587 2.222 0.0263 * Saving.accountsrich 1.39033 0.55443 2.508 0.0122 * Checking.accountmoderate 0.04129 0.19557 0.211 0.8328 Checking.accountrich 0.60678 0.34522 1.758 0.0788 . Credit.amount 0.08047 0.11777 0.683 0.4944 Duration -0.53843 0.11354 -4.742 2.12e-06 ***Purposecar -0.19286 0.33430 -0.577 0.5640 `Purposedomestic appliances` -0.10257 0.95233 -0.108 0.9142 Purposeeducation -0.26250 0.45785 -0.573 0.5664 `Purposefurniture/equipment` -0.07121 0.36564 -0.195 0.8456 `Purposeradio/TV` 0.38793 0.34711 1.118 0.2637 Purposerepairs -0.39103 0.61230 -0.639 0.5231 `Purposevacation/others` 0.17053 0.84842 0.201 0.8407 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 855.21 on 699 degrees of freedomResidual deviance: 769.05 on 678 degrees of freedomAIC: 813.05Number of Fisher Scoring iterations: 4 从结果中看到，在显著性水平为0.05时，显著的特征有：Sex、Saving.accounts、Duration。使用这3个特征进行模型训练。 1234formula.imp &lt;- "Risk ~ Duration + Saving.accounts + Sex"lr.model.imp &lt;- glm(formula.imp, data = train.data, family = binomial)# 查看模型新summary(lr.model.imp) 12345678910111213141516171819202122232425Call:glm(formula = formula.imp, family = binomial, data = train.data)Deviance Residuals: Min 1Q Median 3Q Max -2.4264 -1.1536 0.6725 0.8481 1.5563 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.34854 0.15594 2.235 0.025407 * Duration -0.53669 0.08597 -6.243 4.29e-10 ***Saving.accountsmoderate 0.05753 0.25463 0.226 0.821263 Saving.accountsquite rich 0.89298 0.38989 2.290 0.022002 * Saving.accountsrich 1.38468 0.54357 2.547 0.010853 * Sexmale 0.62673 0.18513 3.385 0.000711 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 855.21 on 699 degrees of freedomResidual deviance: 792.13 on 694 degrees of freedomAIC: 804.13Number of Fisher Scoring iterations: 4 使用得到的模型对测试集的目标特征进行预测，并查看预测结果。 12345lr.predictions.imp &lt;- predict(lr.model.imp, newdata = test.feature, type = "response")(lr.imp.conf.matrix &lt;- confusionMatrix( as.factor(round(lr.predictions.imp)), reference = test.feature.target, positive = '1')) 1234567891011121314151617181920212223242526Confusion Matrix and Statistics ReferencePrediction 0 1 0 13 21 1 77 189 Accuracy : 0.6733 95% CI : (0.6171, 0.7261) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.8577 Kappa : 0.0541 Mcnemar's Test P-Value : 2.763e-08 Sensitivity : 0.9000 Specificity : 0.1444 Pos Pred Value : 0.7105 Neg Pred Value : 0.3824 Prevalence : 0.7000 Detection Rate : 0.6300 Detection Prevalence : 0.8867 Balanced Accuracy : 0.5222 'Positive' Class : 1 计算F1 score 1calc.f1.score(lr.imp.conf.matrix) 1[1] 0.7941 预测的结果的准确度为0.6733，灵敏度为0.9，特异性为0.1444，F1 score为0.7941。该模型也能够很好地预测信用评级良好的客户，但对信用评级差的客户预测效果并不理想。 模型选择 绘制ROC曲线，比较三个模型的AUC。 ROC曲线 使用所有特征的逻辑回归模型的ROC曲线 1234lr.all.pred &lt;- prediction(predictions = lr.predictions.all, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = lr.all.pred, title.text = "逻辑回归ROC曲线")plot.pr.curve(predictions = lr.all.pred, title.text = "逻辑回归Precision/Recall曲线") 逐步回归的逻辑回归模型的ROC曲线 1234lr.step.pred &lt;- prediction(predictions = lr.predictions.step, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = lr.step.pred, title.text = "逻辑回归ROC曲线")plot.pr.curve(predictions = lr.step.pred, title.text = "逻辑回归Precision/Recall曲线") 交叉验证显著特征的逻辑回归模型的ROC曲线 1234lr.imp.pred &lt;- prediction(predictions = lr.predictions.imp, labels = test.feature.target)par(mfrow = c(1, 2))plot.roc.curve(predictions = lr.imp.pred, title.text = "逻辑回归ROC曲线")plot.pr.curve(predictions = lr.imp.pred, title.text = "逻辑回归Precision/Recall曲线") 三个模型的ROC曲线比较 1234567891011pred.list &lt;- list("all" = lr.predictions.all, "step" = lr.predictions.step, "imp" = lr.predictions.imp)for (i in seq(1, length(pred.list))) &#123; pred &lt;- prediction(predictions = pred.list[i], labels = test.feature.target) perf &lt;- performance(pred, "tpr", "fpr") if (i == 1) &#123; plot.act(perf, main = "ROC tpr/fpr", col = i, family = NULL) &#125; else &#123; plot.act(perf, add = TRUE, col = i, family = NULL) &#125;&#125;legend(0.6, 0.6, names(pred.list), seq(1, length(pred.list))) 模型评估指标 模型 准确度 灵敏度 特异性 F1 score AUC值 所有特征的逻辑回归模型 0.6933 0.9095 0.1889 0.8059 0.65 逐步回归的逻辑回归模型 0.6868 0.8952 0.2 0.8 0.66 交叉验证显著特征的逻辑回归模型 0.6733 0.9 0.1444 0.7941 0.62 至于选择哪个模型用于预测，并不完全依赖于模型的准确度，而是根据研究领域和业务需求来决定。如果模型将一个信用评级差的客户预测为信用评级好的客户，这意味着银行将批准一名最终违约的客户的贷款申请，这将导致银行的损失。但是，如果模型将一个信用评级好的客户预测为信用评级差的客户，这意味着银行将否决该客户的的贷款申请，银行将既不会盈利也不会有所损失。所以基于损失最小化，利益最大化的原则，这里将选用特异性较大的模型，即通过逐步回归得到的模型。]]></content>
      <categories>
        <category>信用风险分析</category>
      </categories>
      <tags>
        <tag>R语言</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[留存率预测]]></title>
    <url>%2F%E6%B8%B8%E6%88%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F%E7%95%99%E5%AD%98%E7%8E%87%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[留存率预测 留存率指t日的新增用户，在t+n日再次登录游戏占t日新增用户的比例。 留存率预测是根据前期的留存率对给定自然时间内的留存率进行预测。假设某款游戏的留存率数据如下： 次日留存 3日留存率 7日留存率 14日留存率 30日留存率 0.1694 0.0980 0.0544 0.298 0.0149 探索性分析 假设将留存日day作为自变量，留存率ratio看作自变量，绘制散点图。 123456# 输入数据留存率数据ratio &lt;- c(0.1694, 0.0980, 0.0544, 0.0298, 0.0149) day &lt;- c(1, 3, 7, 14, 30)# 绘制数据点图plot(day, ratio)lines(day, ratio) 从图中的散点之间的连线可以看到，两者间可能存在某种曲线关系。因此，我们假设两个变量满足某种曲线关系，并对该曲线关系拟合一个曲线模型，然后对曲线模型的显著性进行检验。若检验为显著的，则使用该曲线模型对新增用户在接下来365日每天的留存率进行预测。 根据散点图中的曲线关系的形态，再结合已知的常见的曲线模型，我们选择以下函数对曲线进行拟合 \[ \begin{align*} &amp;1. y = a * x ^ b \\ &amp;2. \frac{1}{y} = a + \frac{b}{x},(a&gt;0,b&lt;0) \\ &amp;3. y = a + blnx,(b &lt; 0) \\ &amp;4. y = ae^\frac{b}{x}, (b &gt; 0) \\ &amp;5. y = ae^{bx}, (b &lt; 0 ) \end{align*} \] 所以接下来要对自变量day（\(x\)）和因变量ratio（\(y\)）的曲线关系能否使用上面的函数拟合进行显著性检验。因为在数据挖掘和机器学习中，习惯将拟合称为回归，并且为了与以下介绍的R环境中的函数区别开来，将上述拟合函数称为回归模型，函数的系数称为模型的回归系数。 建立模型 对于day和ratio的曲线关系使用上述模型进行拟合的检验，首先分别对每个模型求出其回归系数（a和b）的估计值，然后再对得到回归系数估计值的模型进行显著性检验。由于上述模型都是非线性，所以可以使用非线性最小二乘法的方式求出上述模型的回归系数的估计值，再使用t检验进行显著性检验。 在R环境中，提供了nls函数（Nonlinear Least Squares，非线性最小二乘），该函数默认使用Gauss-Newton（高斯-牛顿）迭代法对模型参数进行求解，并给出显著性检验结果。 高斯-牛顿迭代法的基本思想是使用泰勒级数展开式去近似地代替非线性回归模型，然后通过多次迭代修正回归系数，使回归系数不断逼近使非线性回归模型残差平方和达到最小的回归系数。 建立模型 1234567891011## 建立非线性回归# 对应上述的模型1model1 &lt;- nls(ratio ~ a * day^b, start = list(a = 1, b = 1))# 对应上述的模型2model2 &lt;- nls(ratio ~ 1 / (a + b / day), start = list(a = 1, b = 1))# 对应上述的模型3model3 &lt;- nls(ratio ~ a + b * log(day), start = list(a = 1, b = -1))# 对应上述的模型4model4 &lt;- nls(ratio ~ a * exp(b / day), start = list(a = 1, b = 1)) # 对应上述的模型5model5 &lt;- nls(ratio ~ a * exp(b * day), start = list(a = 1, b = -1)) 查看模型结果 123456# 模型列表fit.model &lt;- list(fit1, fit2, fit3, fit4, fit5)# 查看所有模型结果lapply(fit.model, summary)# 获得模型的拟合值fit.values &lt;- lapply(fit.model, fitted.values) 输出结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[[1]]Formula: ratio ~ a * day^bParameters: Estimate Std. Error t value Pr(&gt;|t|) a 0.172523 0.007393 23.34 0.000172 ***b -0.604448 0.047449 -12.74 0.001044 ** ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.007621 on 3 degrees of freedomNumber of iterations to convergence: 9 Achieved convergence tolerance: 3.679e-06[[2]]Formula: ratio ~ 1/(a + b/day)Parameters: Estimate Std. Error t value Pr(&gt;|t|) a 22.296 7.040 3.167 0.0506 .b -16.443 7.149 -2.300 0.1050 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.03043 on 3 degrees of freedomNumber of iterations to convergence: 9 Achieved convergence tolerance: 3.407e-06[[3]]Formula: ratio ~ a + b * log(day)Parameters: Estimate Std. Error t value Pr(&gt;|t|) a 0.156923 0.011924 13.160 0.000948 ***b -0.046024 0.005497 -8.372 0.003573 ** ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.01456 on 3 degrees of freedomNumber of iterations to convergence: 1 Achieved convergence tolerance: 2.373e-07[[4]]Formula: ratio ~ a * exp(b/day)Parameters: Estimate Std. Error t value Pr(&gt;|t|) a 0.03966 0.01256 3.158 0.051 .b 1.47450 0.36370 4.054 0.027 *---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.02623 on 3 degrees of freedomNumber of iterations to convergence: 6 Achieved convergence tolerance: 2.084e-06[[5]]Formula: ratio ~ a * exp(b * day)Parameters: Estimate Std. Error t value Pr(&gt;|t|) a 0.19269 0.02129 9.050 0.00285 **b -0.18029 0.03914 -4.607 0.01924 * ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.01502 on 3 degrees of freedomNumber of iterations to convergence: 12 Achieved convergence tolerance: 7.599e-06 上述输出中，有5个输出结果，分别对应于相应的函数模型。可以看到，每个输出结果的组成部分都是一样的。 Formula：formula对象，即回归模型的表示方法 Parameters：回归系数相关信息 Estimate：相应回归系数的估计值 Std. Error：相应回归系数的标准误 t value：相应回归系数的t检验量的值 Pr(&gt;|t|)：相应回归系数的t检验的检验的p值，用于判定显著性 Signif. codes：显著性标识，检验的p值（这里对应于Pr(&gt;|t|)）越小，越显著。前面的数值（0, 0.001, 0.01等）表示相应的显著性水平。 Residual standard error：表示残差标准误；degrees of freedom为自由度，残差的自由度等于样本容量减2。这里样本容量为5，所以残差自由度为3。 Number of iterations to convergence：满足相应的满足逼近容忍度时的迭代次数 Achieved convergence tolerance：容忍度 在输出结果中，若指定显著性水平为0.05，可以看到模型2的回归系数b和模型4的回归系数a是不显著的，即认为在显著性水平为0.05的情况下，模型2和模型4不满足回归的假设，所以将选择剩余的模型进行比较。 模型比较 通过上面求出的模型的回归系数，得到以下3个回归模型： \[ \begin{align*} &amp;1. y = a * x ^ b \\ &amp;3. y = a + blnx,(b &lt; 0) \\ &amp;5. y = ae^{bx}, (b &lt; 0 ) \end{align*} \] 比较这3个模型的决定系数\(R^2\)和残差标准误\(s\)这两个标准，选择拟合程度更好的模型。 决定系数\(R^2\)越大，说明残差越小，回归曲线拟合越好，\(R^2\)从总体上给出一个拟合好坏程度的度量。 残差标准误\(s\)为诸观测值与由曲线给出的拟合值之间的平均偏离程度的度量，\(s\)越小，方程越好 1234567891011121314151617ModelMeasure &lt;- function(fit.value, y) &#123; # 决定系数 r.square &lt;- 1 - sum((y - fit.value)^2) / sum((y - mean(y))^2) # 残差标准误，之前使用summary函数查看模型结果时，已经给出该值 # 这里也再计算一遍 s &lt;- sqrt(sum((y - fit.value)^2) / (length(y) - 2)) return(c(r.square = r.square, s = s))&#125;# 创建空list，存放相应模型的决定系数和残差标准误model.measure.values &lt;- list()for (i in 1:length(fit.values)) &#123; if (!is.null(fit.values[[i]])) &#123; model.measure.values[[i]] &lt;- ModelMeasure(fit.values[[i]], ratio) &#125; &#125;# 查看相应模型的决定系数和残差标准误model.measure.values 输出结果： 12345678910111213[[1]]r.square s 0.988762 0.007621 [[2]]NULL[[3]]r.square s 0.95896 0.01456 [[4]]NULL[[5]]r.square s 0.95634 0.01502 将输出整理为表格形式： 模型编号 1 3 5 \(R^2\) 0.988762 0.95896 0.95634 \(s\) 0.007621 0.01456 0.01502 可以看到这里计算的残差标准误与使用summary函数查看模型结果时的残差标准误是一致的。通过表格可以看到，在决定系数\(R^2\)一行中模型1的值最大，最接近于1；在残差标准误\(s\)一行中，模型1的值最小。所以无论不论使用决定系数，还是使用残差标准误，模型1都是在这3个模型中拟合最好的。 预测 选择模型1回归模型对365天内的留存率进行预测，并指出留存率开始低于1%和0.5%时的天数。 123456789# 使用模型1作为回归模型best.model &lt;- model1# 进行预测predicted &lt;- predict(best.model, data.frame(day = seq(1, 365)))# 保留4位有效数字predicted &lt;- round(predicted, 4)# 找出留存率低于1%和0.5%的天数(threshold1 &lt;- max(which(predicted &gt;= 0.01)))(threshold2 &lt;- max(which(predicted &gt;= 0.005))) 输出结果： 12[1] 112[1] 356 输出结果表示留存率在112之后开始低于1%，在356日之后开始低于0.5%。 绘制回归曲线在1～365日内的图形 1234567plot(1:365, predicted, type = "l", xlim = c(1, 375))threshold1.value &lt;- predicted[threshold1]threshold2.value &lt;- predicted[threshold2]points(threshold1, threshold1.value, bg = "yellow", cex = 1.2, pch = 21)points(threshold2, threshold2.value, bg = "yellow", cex = 1.2, pch = 21)text(threshold1, threshold1.value, pos = 3, labels = sprintf("(%d, %.4f)", threshold1, threshold1.value))text(threshold2, threshold2.value, pos = 3, labels = sprintf("(%d, %.4f)", threshold2, threshold2.value))]]></content>
      <categories>
        <category>游戏数据分析</category>
      </categories>
      <tags>
        <tag>R语言</tag>
        <tag>非线性回归</tag>
      </tags>
  </entry>
</search>
